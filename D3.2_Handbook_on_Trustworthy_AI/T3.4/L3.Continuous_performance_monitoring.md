# Continuous Performance Monitoring

## In brief

## More in Detail

Monitoring the live functioning of a produtionalised ML/AI model or
system is an emergent topic that is gaining increasing attention as more
and more methods are being deployed in industrial, commercial and public
sectors. As any other piece of software, any tool based on AI/ML needs
to be maintained over time, for fixing bugs and ensuring quality. ML
models and systems require specific strategies that take into account
their nature of learning from data.

Idealistically, the behaviour of ML models trained on sample
well-curated data is expected to generalise on new, unseen data in the
post-deployment phase. Nonetheless, this happens rarely in practice, and
a model's performance assessed live is often different from the
performance evaluated offline during development. Furthermore, it is
well-known that the performance of an AI model or system degrades over
time.

Several phenomena have been identified as drivers of this decay. The
input data fed into the ML model may contain unexpected patterns not
present in the training datasets. Moreover, the characteristics of data
may change over time, causing that the relationships at the core of the
ML methods do not stand valid any more.

This phenomenon, termed *concept* or *model drift* {cite}`Tsymbal_2004`,
can lead the model to make wrong predictions. Additionally, if the
nature (or distribution) of the input data become vastly different with
respect to those used for training, the performance can even drop below
acceptance. This phenomenon is known as *covariate shift*
{cite}`sugi_2012`. Performance degradation can also result from the impact
that the same deployed ML model may have on the decision process that it
supports. The ML model may influence other elements involved in the
decision or induce an overall change in the phenomenon that is being
modelled, which was not taken into account during training.

Overall, after its deployment, an ML method can come across several
difficulties and changes that the level of efforts and skills needed in
its maintenance could be an order of magnitude higher than that needed
in model building.

Given these concerns, several strategies and best practices have been
investigated to monitor the behaviour of ML methods after deployment,
also in relation to any consequence the methods can have. The first work
published in 2015 described the various challenges that ML methods raise
after deployment in relation to data dependencies, model complexity,
reproducibility, testing, and changes in the external world
{cite}`Sculley_2015`. After that, several methods have been presented in
the literature, focusing specifically on data {cite}`Polyzotis_2017`, on
the role of humans in ML deployment {cite}`Flaounas_2017`, on testing
strategies {cite}`Beck_2016`, or the definition of a general framework to
track ML methods in their live functioning (e.g., pipelines, datasets,
execution configurations, code and human actions) {cite}`Sridhar_2018`.

Overall, the best practices, promoted also from industrial actors
{cite}`Accenture_2017,SaS_2019`, include a continuous monitoring of the
ML system to assess its quality and "vitality". Various types of metrics
are suggested in this respect, focusing mainly on performance
evaluation. The idea is to detect changes in the behaviour and then act
via re-training or implementing an active learning approach (when
reinforcement learning is adopted), so as to rectify any wrong conduct.
It should be noted that model maintenance can be seen as nurturing the
model, as it can take advantage of the new knowledge coming from the
real-setting scenario, thus it can produce an improvement of the
original version released.

Monitoring and maintenance can be performed in a *proactive* or
*reactive* fashion. Proactive monitoring works to identify the input
samples that deviate significantly from the patterns seen in the
training phase and to analyse them more in detail to understand any
drift. The reactive approach entails detecting a wrong output and
identifying its causes, so as to understand how the method can be
rectified.

The Continuous Delivery {cite}`Wolff_2017` and DevOps {cite}`Loukides_2012`
approaches have been also proposed to better manage the risks of
releasing changes to Machine Learning applications and, then, do them in
a safe and reliable way.



## Bibliography

```{bibliography}
:style: unsrt
:filter: docname in docnames
```

---
  
This entry was written by Sara Colantonio.
