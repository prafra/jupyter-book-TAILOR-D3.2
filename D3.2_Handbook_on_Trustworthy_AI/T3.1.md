# Explainable AI

<!--- This is a comment -->

## In Brief

**Explainable AI** (often shortened to **XAI**) is one of the ethical dimensions that is studied in the <a href="https://tailor-network.eu/" target=_blank>TAILOR project</a>.
The origin of XAI dates back to the entering into force of the General Data Protection Regulation (GDPR).
The GDPR {cite}`gdpr`, in its Recital 71, also mentions the right to explanation, as a suitable safeguard to ensure a fair and transparent processing in respect of data subjects. It is defined as the right “to obtain an explanation of the decision reached after [profiling]”.
According to NIST report {cite}`nist`, an explanation is the evidence, support, or reasoning related to a system’s output or process, where the output of a system differs by task and the process refers to the procedures, design, and system workflow which underlie the system.

## Abstract

While other aspect of ethics and trustworthiness, such as {doc}`./T3.5`, are not novel concepts, and a lot of scientific literature has been explored on these topics, the study of explainability is a new challenge.
In this part we will cover the main elements that define the explanation of AI systems. We will try to briefly survey the main guidelines related to explainability.
Then, we summarize a taxonomy that can be used to classify explanations. 
We will define the possible  {doc}`./T3.1/XAI_dimensions` of explanations (e.g., we can discriminate between {doc}`./T3.1/model_specific`).
We will describe the requirements to provide good explanations, some of the problems related to the Explainability topic. 
Finally, we will provide some examples of possible solutions we can adopt to provide explanations describing the reasoning behind a ML/AI model.

## Motivation and Background 

So far, the usage of black boxes in AI and ML processes implied the possibility of inadvertently making wrong decisions due to a systematic bias in training data collection. Several practical examples have been provided, highlighting the “bias in, bias out” concept. One of the most famous examples of this concept regards a classification task: the algorithm goal was to distinguish between photos of Wolves and Eskimo Dogs (huskies) {cite}`ribeiro`. Here, the training phase of the process was done with 20 images, hand selected such that all pictures of wolves had snow in the background, while pictures of huskies did not. This choice was intentional because it was part of a social experiment. In any case, on a collection of additional 60 images, the classifier predicts “Wolf” if there is snow (or light background at the bottom), and “Husky” otherwise, regardless of animal color, position, pose, etc (see an example in Fig. {numref}`{number} <husky>`).

```{figure} ./T3.1_husky.png
---
name: husky
width: 600px
align: center
---
Raw data and explanation of a bad model's prediction in the "Husky vs Wolf" task {cite}`ribeiro`.
```

However, one of the most worrisome cases was discovered and published by ProPublica, an independent, nonprofit newsroom that produces investigative journalism with moral force. In {cite}`propublica`, the authors showed how software can actually be racist. In a nutshell, the authors analyzed a tool called COMPAS (which stands for Correctional Offender Management Profiling for Alternative Sanctions). COMPAS tries to predict, among other indexes, the recidivism of defendants, who are ranked low, medium or high risk. It was used in many US states (such as New York and Wisconsin), to suggest to judges an appropriate probation or treatment plan for individuals being sentenced. Indeed, the tool was quite accurate (around 70 percent overall with 16,000 probationers), but ProPublica journalists found that black defendants were far more likely than white defendants to be incorrectly judged to be at a higher risk of recidivism, while white defendants were more likely than black defendants to be incorrectly flagged as low risk.

From the above examples, it appears evident that explanation technologies can help companies for creating safer, more trustable products, and better managing any possible liability they may have.

## Open the Black-Box Problem

The *Open the Black Box Problems* for understanding how a black box works can be summarized in the taxonomy proposed in {cite}`guidotti_survey` and reported in Fig. {numref}`{number} <T3.1taxonomy>`. The Open the Black Box Problems can be separated from one side as the problem of explaining how the decision system returned certain outcomes (*Black Box Explanation*) and on the other side as the problem of directly designing a transparent classifier that solves the same classification problem (*Transparent Box Design*). Moreover, the Black Box Explanation problem can be further divided among *Model Explanation* when the explanation involves the whole logic of the obscure classifier, *Outcome Explanation* when the target is to understand the reasons for the decisions on a given object, and *Model Inspection* when the target to understand how internally the black box behaves changing the input.

```{figure} ./T3.1_taxonomy.jpg
---
name: T3.1taxonomy
width: 600px
align: center
---
A possible taxonomy about solutions to the Open the Black-Box problem {cite}`guidotti_survey`.
```


On a different dimension, a lot of effort has been put in defining what are the possible  {doc}`./T3.1/XAI_dimensions` of explanations (e.g., we can discriminate between {doc}`./T3.1/model_specific`), the requirements to provide good explanations (see {ref}`guidelines`), how to [evaluate explanations](./T3.1/evaluating_explanations.md) and to understand the {doc}`./T3.1/feature_importance`. Then, it is important to note that a variety of different kinds of explanation can be provided, such as {doc}`./T3.1/saliency_maps`, [Factual and Counterfactual](./T3.1/counterfactuals.md), exemplars and counter-exemplars, [Rules List and Rules Sets](./T3.1/rules.md).

## Guidelines

Given the relatively novelty of the topic, a lot of guidelines have been development in the recent years.

However, the most authoritative guideline is the <a href="https://wayback.archive-it.org/12090/20201227221227/https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai" target=_blank>High-Level Expert Group on Artificial Intelligence - Ethics Guidelines for Trustworthy AI</a>. Here, the explainability topic is included in the broader {doc}`./T3.1/transparency`. According to this guideline, explainability concerns the ability to explain both the technical processes of an AI system and the related human decisions (e.g. application areas of a system). Following the GDPR interpretation, in {cite}`hleg` it is stated that whenever an AI system has a significant impact on people’s lives, it should be possible to demand a suitable explanation of the AI system’s decision-making process. Such explanation should be timely and adapted to the expertise of the stakeholder concerned (e.g. layperson, regulator or researcher). In addition, explanations of the degree to which an AI system influences and shapes the organizational decision-making process, design choices of the system, and the rationale for deploying it, should be available (hence ensuring business model transparency).

Another distinguished authority that has been worked on ethical guidance is **the Alan Turing Institute**, the UK’s national institute for data science and artificial intelligence, where David Leslie {cite}`leslie` summarized the risks due to the lack of transparency or the absence of a valid explanation, and he advocate the use of {doc}`./T3.1/counterfactuals` for contrasting unfair decisions. Together with the Information Commissioner’s Office (ICO), which is responsible for overseeing data protection in the UK, it has been developed more recent and complete guidance {cite}`ico`. Here, six steps are recommended to develop a system:
1. Select priority explanations by considering the domain, use case and impact on the individual.
2. Collect and pre-process data in an explanation-aware manner, stressing the fact that the way in which data is collected and pre-process may affect the quality of the explanation.
3. Build systems to ensure to being able to extract relevant information for a range of explanation types.
4. Translate the rationale of your system’s results into useable and easily understandable reasons, e.g., transforming the model’s logic from quantitative rationale into intuitive reasons or using everyday language that can be understood by non-technical stakeholders.
5. Prepare implementers to deploy the AI system, through appropriate training.
6. Consider how to build and present the explanation, particularly keeping in mind the context and contextual factors (domain, impact, data, urgency, audience) to deliver appropriate information to the individual.

Nevertheless, the attention on this theme is not relegated to the European border. Indeed, as an example of US effort in dealing with Explainability and Ethics, **NIST, the National Institute of Standards and Technology of Maryland**, developed some guidelines and a white paper {cite}`nist` was published after a first draft[^nist_draft] was published in 2020, a variety of comments[^nist_comments] were collected, and a workshop[^nist_workshop] involving different stakeholders was held. The white paper {cite}`nist` analyzes the multidisciplinary nature of explainable AI and acknowledge the existence of different users who requires different kinds of explanations, stating that one-size-
fits-all explanations do not exist. The fundamental properties of explanations contained in the report are: 
* *meaningfulness*, i.e., explanations must be understandable to the intended consumer(s). This means that there is the need to consider the intended audience, and some characteristics they can have, such as prior knowledge or the overall psychological differences between people. Moreover, explanation’s purpose is relevant too. Indeed, different scenarios and needs impacts on what is important and useful
in a given context. This implies understanding the audience’s needs, level of expertise, and relevancy to the question or query .
* *accuracy*, i.e., explanations correctly reflects a system’s process for generating its output. Explanation accuracy is a distinct concept from decision accuracy. Explanation accuracy needs to account for the level of detail in the explanation. This second principle might be in contrast with the previous one: detailed explanation may accurately reflect the system’s processing, but sacrifice how useful and accessible it is to certain audiences, while  a brief, simple explanation may be highly understandable but would not fully characterize the system. 
* *knowledge limits*, i.e., characterizing the fact that a system only operates under conditions for which it was designed and when it reaches sufficient confidence in its output. This practice safeguards answers so that a judgment is not provided when it may be inappropriate to do so. This principle can increase trust in a system by preventing misleading, dangerous, or unjust outputs.


## Software Frameworks Supporting Dimension

Within the European Research Council (ERC) <a href="https://xai-project.eu/" target=_blank>XAI project</a> and the European Union's Horizon 2020 <a href="http://project.sobigdata.eu/" target=_blank> SoBigData++ project</a>, we are developing an infrastructure for sharing experimental datasets and explanation algorithms with the research community, creating a common ground for researchers working on explanation of black boxes from different domains. 
All resources, provided they are not prohibited by specific legal/ethical constraints, will be collected and described in a <a href="https://sobigdata.d4science.org/catalogue-sobigdata" target=_blank>findable catalogue</a>.
A dedicated virtual research environment will be activated, so that a variety of relevant resources, such as data, methods, experimental workflows, platforms and literature, will be managed through the SoBigData++ e-infrastructure services and made available to the research community through a variety of regulated access policies.
We will provide a link to the libraries and framework as soon as they will fully published.


## Main Keywords[^todo]
- Ante-hoc Explanation vs Post-hoc Explanations
- Counterfactual Explanations
- Evaluating Explanations (measures, human expert studies)
- Features Importance
- Global vs Local Explanations
- Interpretability
- Transparency
-  {doc}`./T3.1/model_specific`: We distinguish between model-specific or model-agnostic explanation method depending on whether the technique adopted to retrieve the explanation acts on a particular model adopted by an AI system, or can be used on any type of AI. 
- Rules List and Rules Set
- Saliency Maps
- Single Tree Approximation

[^nist_draft]: https://doi.org/10.6028/NIST.IR.8312-draft
[^nist_comments]: https://www.nist.gov/artificial-intelligence/comments-received-four-principles-explainable-artificial-intelligence-nistir
[^nist_workshop]: https://www.nist.gov/system/files/documents/2021/09/24/XAI_Workshop_Summary_Final_20210922.pdf
[^todo]: Note from Francesca: all the short definitions must be reported also here

## Bibliography

<!-- :style: unsrtalpha -->

```{bibliography}
:style: unsrt
:filter: docname in docnames
```

---

This entry was readapted from *Pratesi, Trasarti, Giannotti. Ethics in Smart Information Systems. Policy Press (currently under review)* and from *Guidotti, Monreale, Ruggieri, Turini, Giannotti, Pedreschi. A survey of methods for explaining black box models. ACM Computing Surveys, Volume 51 Issue 5 (2019)* by Francesca Pratesi.


<!---
{footcite}`propublica`
```{footbibliography}
```
for the biblography

[^note]
[^note]: This is a note
-->


