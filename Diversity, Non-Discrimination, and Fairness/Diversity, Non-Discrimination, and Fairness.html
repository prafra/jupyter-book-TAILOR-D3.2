
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Fairness, Equity, and Justice by Design –&gt; Diversity, Non-Discrimination, and Fairness &#8212; The TAILOR Handbook of Trustworthy AI</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tailor.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="http://tailor.isti.cnr.it/handbookTAI/index.html/Diversity, Non-Discrimination, and Fairness/Diversity, Non-Discrimination, and Fairness.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Auditing AI" href="auditing.html" />
    <link rel="prev" title="Data Poisoning" href="../Technical%20Robustness%20and%20Safety/data_poisoning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/TAILOR-logo-coloured.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">The TAILOR Handbook of Trustworthy AI</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../TAILOR.html">
                    The TAILOR Handbook of Trustworthy AI
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../authors.html">
   Complete List of Contributors
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Human%20Agency%20and%20Oversight/Human%20Agency%20and%20Oversight.html">
   Human Agency and Oversight
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Human%20Agency%20and%20Oversight/Meaningful_human_control.html">
     Meaningful human control
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Transparency/Transparency.html">
   Transparency
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Transparency/XAI_kinds.html">
     Kinds of Explanations
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transparency/feature_importance.html">
       Feature Importance
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transparency/saliency_maps.html">
       Saliency Maps
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transparency/single_tree.html">
       Single Tree Approximation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Transparency/XAI_dimensions.html">
     Dimensions of Explanations
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
    <label for="toctree-checkbox-4">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transparency/blackbox_transparent.html">
       Black Box Explanation vs Explanation by Design
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transparency/model_specific.html">
       Model-Specific vs Model-Agnostic Explainers
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Transparency/global_local.html">
       Global vs Local Explanations
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Technical%20Robustness%20and%20Safety/Technical%20Robustness%20and%20Safety.html">
   Technical Robustness and Safety
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Technical%20Robustness%20and%20Safety/alignment.html">
     Alignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Technical%20Robustness%20and%20Safety/robustness.html">
     Robustness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Technical%20Robustness%20and%20Safety/reliability.html">
     Reliability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Technical%20Robustness%20and%20Safety/evaluation.html">
     Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Technical%20Robustness%20and%20Safety/negative_side_effects.html">
     Negative side effects
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Technical%20Robustness%20and%20Safety/distributional_shift.html">
     Distributional shift
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Technical%20Robustness%20and%20Safety/security.html">
     Security
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Technical%20Robustness%20and%20Safety/adversarial_attack.html">
     Adversarial Attack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Technical%20Robustness%20and%20Safety/data_poisoning.html">
     Data Poisoning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="current reference internal" href="#">
   Diversity, Non-Discrimination, and Fairness
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="auditing.html">
     Auditing AI
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bias.html">
     Bias
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="equity.html">
     Discrimination &amp; Equity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fairness.html">
     Fairness notions and metrics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fair_ML.html">
     Fair Machine Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="discrimination.html">
     Grounds of Discrimination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="justice.html">
     Justice
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="segregation.html">
     Segregation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Accountability/Accountability_and_Reproducibility.html">
   Accountability
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Accountability/L2.Accountability.html">
     Accountability
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
    <label for="toctree-checkbox-8">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Accountability/L3.Wicked_problems.html">
       Wicked problems
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Accountability/L3.The_frame_problem.html">
       The Frame Problem
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Accountability/L2.Reproducibility.html">
     Reproducibility
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Accountability/L2.Traceability.html">
     Traceability
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
    <label for="toctree-checkbox-9">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Accountability/L3.Provenance_tracking.html">
       Provenance Tracking
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Accountability/L3.Continuous_performance_monitoring.html">
       Continuous Performance Monitoring
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Privacy%20and%20Data%20Governance/Privacy%20and%20Data%20Governance.html">
   Respect for Privacy
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Privacy%20and%20Data%20Governance/L1.anonymization.html">
     Data Anonymization (and Pseudonymization)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Privacy%20and%20Data%20Governance/L2.pseudonymization.html">
       Pseudonymization
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Privacy%20and%20Data%20Governance/L1.privacy_model.html">
     Privacy Models
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
    <label for="toctree-checkbox-12">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../Privacy%20and%20Data%20Governance/L2.differential_privacy.html">
       Differential Privacy
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
      <label for="toctree-checkbox-13">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../Privacy%20and%20Data%20Governance/L3.epsilon_DP.html">
         <span class="math notranslate nohighlight">
          \(\epsilon\)
         </span>
         -Differential Privacy
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Privacy%20and%20Data%20Governance/L3.epsilon_delta_DP.html">
         (
         <span class="math notranslate nohighlight">
          \(\epsilon\)
         </span>
         ,
         <span class="math notranslate nohighlight">
          \(\delta\)
         </span>
         )-Differential Privacy
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Privacy%20and%20Data%20Governance/L2.perturbation_mechanisms.html">
         Achieving Differential Privacy
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Privacy%20and%20Data%20Governance/L2.k_anonymity.html">
       k-anonymity
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Privacy%20and%20Data%20Governance/L1.attacks.html">
     Attacks on anonymization schemes
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
    <label for="toctree-checkbox-14">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Privacy%20and%20Data%20Governance/L2.reidentification.html">
       Re-identification Attack
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Societal%20and%20Environmental%20Wellbeing/Societal%20and%20Environmental%20Wellbeing.html">
   Sustainability
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Societal%20and%20Environmental%20Wellbeing/greenAI.html">
     Green AI
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
    <label for="toctree-checkbox-16">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Societal%20and%20Environmental%20Wellbeing/power_aware.html">
       Power-aware Computing
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Societal%20and%20Environmental%20Wellbeing/cloud_computing.html">
     Cloud Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Societal%20and%20Environmental%20Wellbeing/edge_computing.html">
     Edge Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Societal%20and%20Environmental%20Wellbeing/data_centre.html">
     Data Centre
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Societal%20and%20Environmental%20Wellbeing/cradle_to_cradle.html">
     Cradle-to-cradle Design
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Societal%20and%20Environmental%20Wellbeing/resource_prediction.html">
     Resource Prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Societal%20and%20Environmental%20Wellbeing/resource_allocation.html">
     Resource Allocation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../TAILOR_project.html">
   About TAILOR
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/prafra/jupyter-book-TAILOR-D3.2"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/prafra/jupyter-book-TAILOR-D3.2/issues/new?title=Issue%20on%20page%20%2FDiversity, Non-Discrimination, and Fairness/Diversity, Non-Discrimination, and Fairness.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/Diversity, Non-Discrimination, and Fairness/Diversity, Non-Discrimination, and Fairness.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#in-brief">
   In brief
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#abstract">
   Abstract
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivations-and-background">
   Motivations and background
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#standards-and-guidelines">
   Standards and guidelines
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#software-frameworks-supporting-dimension">
   Software frameworks supporting dimension
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#main-keywords">
   Main keywords
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliography">
   Bibliography
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Fairness, Equity, and Justice by Design –> Diversity, Non-Discrimination, and Fairness</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#in-brief">
   In brief
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#abstract">
   Abstract
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivations-and-background">
   Motivations and background
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#standards-and-guidelines">
   Standards and guidelines
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#software-frameworks-supporting-dimension">
   Software frameworks supporting dimension
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#main-keywords">
   Main keywords
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliography">
   Bibliography
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="fairness-equity-and-justice-by-design-diversity-non-discrimination-and-fairness">
<h1>Fairness, Equity, and Justice by Design –&gt; Diversity, Non-Discrimination, and Fairness<a class="headerlink" href="#fairness-equity-and-justice-by-design-diversity-non-discrimination-and-fairness" title="Permalink to this headline">#</a></h1>
<div class="section" id="in-brief">
<h2>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">#</a></h2>
<p>The term fairness is defined as the quality or state of being fair; or a
lack of favoritism towards one side. However, fairness can mean
different concepts to different peoples, different contexts, and
different disciplines <span id="id1">[<a class="reference internal" href="fairness.html#id252">1</a>]</span>. An unfair Artificial
Intelligence (AI) model produces results that are biased towards
particular individuals or groups. The most relevant case of bias is
discrimination against protected-by-law social groups. Equity requires
that people are treated according to their needs, which does not mean
all people are treated equally <span id="id2">[<a class="reference internal" href="#id117">2</a>]</span>. Justice is the
“fair and equitable treatment of all individuals under the law”
<span id="id3">[<a class="reference internal" href="justice.html#id171">1</a>]</span>.</p>
</div>
<div class="section" id="abstract">
<h2>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">#</a></h2>
<p>We first provide motivations and background on fairness, equity and
justice in AI. This consists of warnings and legal obligations about
potential harms of unscrutinized AI tools, especially in socially
sensitive decision making. A taxonomy of fair-AI algorithms is then
presented, based on the step of the AI development process in which
fairness is checked/controlled for. Next, we summarize the guidelines
and draft standards for fair-AI, and the software frameworks supporting
the dimension. Finally, the main keywords of the dimension are
extensively detailed.</p>
</div>
<div class="section" id="motivations-and-background">
<h2>Motivations and background <a class="footnote-reference brackets" href="#readapt" id="id4">1</a><a class="headerlink" href="#motivations-and-background" title="Permalink to this headline">#</a></h2>
<p>Increasingly sophisticated algorithms from AI and Machine Learning (ML)
support knowledge discovery from big data of human activity. They enable
the extraction of patterns and profiles of human behavior which are able
to make extremely accurate predictions. Decisions are then being partly
or fully delegated to such algorithms for a wide range of socially
sensitive tasks: personnel selection and wages, credit scoring, criminal
justice, assisted diagnosis in medicine, personalization in schooling,
sentiment analysis in texts and images, people monitoring through facial
recognition, news recommendation, community bulding in social networks,
dynamic pricing of services and products.</p>
<p>The benefits of algorithmic-based decision making cannot be neglected,
e.g., procedural regularity – same procedure applied to each data
subject. However, automated decisions based on profiling or social
sorting may be biased <span id="id5">[<a class="reference internal" href="#id121">4</a>]</span> for
several reasons. Historical data may contain human (cognitive) bias and
discriminatory practices that are endemic, to which the algorithms
assign the status of general rules. Also, the usage of AI/ML models
reinforces such practices because data about model’s decisions become
inputs in subsequent model construction (feedback loops). Algorithms may
wrongly interpret spurious correlations in data as causation, making
predictions based on ungrounded reasons. Moreover, algorithms pursue the
optimization of quality metrics, such as accuracy of predictions, that
favor precision over the majority of people against small groups.
Finally, the technical process of designing and deploying algorithms is
not yet mature and standardized. Rather, it is full of small and big
decisions (sometimes, trial and error steps) that may hide bias, such as
selecting non-representative data, performing overspecialization of the
models, ignoring socio-technical impacts, or using models in deployment
contexts they are not tested for. These risks are exacerbated by the
fact that the AI/ML models are complex for human understanding, or not
even intelligible, sometimes they are based on randomness or
time-dependent non-reproducible conditions <span id="id6">[<a class="reference internal" href="#id151">5</a>]</span>.</p>
<p>Legal restrictions on automated decision-making are provided by the EU
General Data Protection Regulation, which states (<a href="https://gdpr-info.eu/art-22-gdpr/" target=_blank>Article 22</a>) “the right
not to be subject to a decision based solely on automated processing”.
Moreover, (<a href="https://gdpr-info.eu/recitals/no-71/" target=_blank>Recital 71</a>) “in order to ensure fair and transparent
processing in respect of the data subject […] the controller should
use appropriate mathematical or statistical procedures […] to
prevent, inter alia, discriminatory effects on natural persons”.</p>
<p>Fair algorithms are designed with the purpose of preventing biased
decisions in algorithmic decision making. Quantitative definitions have
been introduced in philosophy, economics, and machine learning in the
last 50 years
<span id="id7">[<a class="reference internal" href="#id116">6</a>, <a class="reference internal" href="#id115">7</a>, <a class="reference internal" href="auditing.html#id140">1</a>]</span>,
with more than 20 different definitions of fairness appeared thus far in
the computer science literature
<span id="id8">[<a class="reference internal" href="bias.html#id112">1</a>, <a class="reference internal" href="#id181">10</a>]</span>.
Four non-mutually exclusive strategies can be devised for
fairness-by-design of AI/ML models.</p>
<p><em>Pre-processing approaches.</em> They consists of a controlled sanitization
of the data used to train an AI/ML model with respect to specific
biases. Pre-processing approaches allow for obtaining less biased data,
which can be used for training AI/ML models. An advantage of
pre-processing approaches is that they are independent of the AI/ML
model and algorithm at hand.</p>
<p><em>In-processing approaches.</em> The second strategy is to modify the AI/ML
algorithm, by incorporating fairness criteria in model construction,
such as regularizing the optimization objective with a fairness measure.
There is a fast growing adoption of in-processing approaches in many
AI/ML problems other than in the original setting of classification,
including ranking, clustering, community detection, influence
maximization, distribution/allocation of goods, and models on
non-structured data such as natural language texts and images. An area
somehow in the middle between pre-processing and in-processing
approaches is fair representation learning, where the model inferred
from data is not used directly for decision making, but rather as
intermediate knowledge.</p>
<p><em>Post-processing approaches.</em> This strategy consists of post-processing
an AI/ML model once it has been computed, so to identify and remove
unfair decision paths. This can be achieved also by involving human
experts in the exploration and interpretation of the model or of the
model’s decisions. Post-processing approaches consist of altering the
model’s internals, for instance by correcting the confidence of
classification rules, or the probabilities of Bayesian models.
Post-processing becomes necessary for tasks for which there is no
in-processing approach explicitly designed for the fairness requirement
at hand.</p>
<p><em>Prediction-time approaches.</em> The last strategy assumes no change in the
construction of AI/ML models, but rather correcting their predictions at
run-time. Proposed approaches include promoting, demoting or rejecting
predictions close to the decision boundary, differentiating the decision
boundary itself over different social groups, or wrapping a fair
classifier on top of a black-box base classifier. Such approaches may be
applied to legacy software, including non-AI/ML algorithms, that cannot
be replaced by in-processing approaches or changed by post-processing
approaches.</p>
</div>
<div class="section" id="standards-and-guidelines">
<h2>Standards and guidelines<a class="headerlink" href="#standards-and-guidelines" title="Permalink to this headline">#</a></h2>
<p>Several initiatives have started to audit, standardize and certify
algorithmic fairness, such as the <a class="reference external" href="https://ico.org.uk/about-the-ico/ico-and-stakeholder-consultations/ico-consultation-on-the-draft-ai-auditing-framework-guidance-for-organisations">ICO Draft on AI Auditing
Framework</a>,
the draft <a class="reference external" href="https://standards.ieee.org/project/7003.html">IEEE P7003™ Standard on Algorithmic Bias
Considerations</a>, the <a class="reference external" href="https://standards.ieee.org/industry-connections/ecpais.html">IEEE
Ethics Certification Program for Autonomous and Intelligent
Systems</a>,
and the <a class="reference external" href="https://www.iso.org/standard/77607.html">ISO/IEC TR 24027:2021 Bias in AI systems and AI aided decision
making</a> (see also the entry on
<a class="reference internal" href="auditing.html"><span class="doc">Auditing AI</span></a>). Regarding the issue of equality data collection, the European Union
High Level Group on Non-discrimination, Equality and Diversity has set
up “<a class="reference external" href="https://ec.europa.eu/info/sites/default/files/en-guidelines-improving-collection-and-use-of-equality-data.pdf">Guidelines on improving the collection and use of equality
data</a>”,
and the European Union Agency for Fundamental Rights (FRA) maintains <a href="https://fra.europa.eu/en/promising-practices-list" target=_blank>a
list</a> of promising practices for equality data collection.</p>
<p>Very few scientific works attempt at investigating the practical
applicability of fairness in AI
<span id="id9">[<a class="reference internal" href="#id124">11</a>, <a class="reference internal" href="#id123">12</a>]</span>.
This issue is challenging, and likely to require domain-specific
approaches <span id="id10">[<a class="reference internal" href="#id122">13</a>]</span>. On the educational side,
however, there are hundreds of university courses on technology ethics
<span id="id11">[<a class="reference internal" href="#id125">14</a>]</span>, many of which cover fairness in AI.</p>
</div>
<div class="section" id="software-frameworks-supporting-dimension">
<h2>Software frameworks supporting dimension<a class="headerlink" href="#software-frameworks-supporting-dimension" title="Permalink to this headline">#</a></h2>
<p>The landscape of software libraries and tools is very large. Existing
proposals cover almost every step of the data-friven AI development
process (data collection, data processing, model development, model
deployment, model monitoring), every type of AI models (classification,
regression, clustering, ranking, community detection, influence
maximization, distribution/allocation of goods), and every type of data
(tabular, text, images, videos). Reviews and critical discussions of
gaps for a few fairness toolkits can be found in
<span id="id12">[<a class="reference internal" href="fair_ML.html#id268">1</a>, <a class="reference internal" href="#id126">16</a>]</span>.</p>
</div>
<div class="section" id="main-keywords">
<h2>Main keywords<a class="headerlink" href="#main-keywords" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="auditing.html"><span class="doc">Auditing AI</span></a>: <strong>Auditing AI</strong> aims to identify and address possible risks and impacts while ensuring robust and trustworthy (see <a class="reference internal" href="../Accountability/L2.Accountability.html"><span class="doc">Accountability</span></a>).</p></li>
<li><p><a class="reference internal" href="bias.html"><span class="doc">Bias</span></a>: <strong>Bias</strong> refers to an inclination towards or against a particular individual, group, or sub-groups. AI models may inherit biases from training data or introduce new forms of bias.</p></li>
<li><p><a class="reference internal" href="equity.html"><span class="doc">Discrimination &amp; Equity</span></a>: Forms of bias that count as discrimination against social groups or individuals should be avoided, both from legal and ethical perspectives. Discrimination can be direct or indirect, intentional or unintentional.</p></li>
<li><p><a class="reference internal" href="fairness.html"><span class="doc">Fairness notions and metrics</span></a>: The term <strong>fairness</strong> is defined as the quality or state of being fair; or a lack of favoritism towards one side. The notions of fairness, and quantitative measures of them (fairness metrics), can be distinguished based on the focus on individuals, groups and sub-groups.</p></li>
<li><p><a class="reference internal" href="fair_ML.html"><span class="doc">Fair Machine Learning</span></a>: <strong>Fair Machine Learning</strong> models take into account the issues of bias and fairness. Approaches can be categorized as pre-processig, which transform the input data, as in-processing, which modify the learning algorithm, and post-processing, which alter models’ internals or their decisions.</p></li>
<li><p><a class="reference internal" href="discrimination.html"><span class="doc">Grounds of Discrimination</span></a>: International and national laws prohibit <strong>discriminating on some explicitly defined grounds</strong>, such as race, sex, religion, etc. They can be considered in isolation, or interacting, giving rise to multiple discrimination and intersectional discrimination.</p></li>
<li><p><a class="reference internal" href="justice.html"><span class="doc">Justice</span></a>: <strong>Justice</strong> encompasses three different perspectives: (1) <em>fairness</em> understood as the fair treatment of people, (2) <em>rightness</em> as the quality of being fair or reasonable, and (3) a legal system, the scheme or system of law. Justice can be distinguished between <em>substantive</em> and <em>procedural</em>.</p></li>
<li><p><a class="reference internal" href="segregation.html"><span class="doc">Segregation</span></a>: <strong>Social segregation</strong> refers to the separation of groups on the grounds of personal or cultural traits. Separation can be physical (e.g., in schools or neighborhoods) or virtual (e.g., in social networks).</p></li>
</ul>
</div>
<div class="section" id="bibliography">
<h2>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">#</a></h2>
<p id="id13"><dl class="citation">
<dt class="label" id="id225"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Deirdre K Mulligan, Joshua A Kroll, Nitin Kohli, and Richmond Y Wong. This thing called fairness: disciplinary confusion realizing a value in technology. <em>Proceedings of the ACM on Human-Computer Interaction</em>, 3(CSCW):1–36, 2019.</p>
</dd>
<dt class="label" id="id117"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Martha Minow. Equality vs. Equity. <em>American Journal of Law and Equality</em>, 1:167–193, 2021.</p>
</dd>
<dt class="label" id="id144"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Jeffrey Lehman, Shirelle Phelps, and others. <em>West's encyclopedia of American law</em>. Thomson/Gale, 2004.</p>
</dd>
<dt class="label" id="id121"><span class="brackets"><a class="fn-backref" href="#id5">4</a></span></dt>
<dd><p>Eirini Ntoutsi, Pavlos Fafalios, Ujwal Gadiraju, Vasileios Iosifidis, Wolfgang Nejdl, Maria-Esther Vidal, Salvatore Ruggieri, Franco Turini, Symeon Papadopoulos, Emmanouil Krasanakis, Ioannis Kompatsiaris, Katharina Kinder-Kurlanda, Claudia Wagner, Fariba Karimi, Miriam Fernández, Harith Alani, Bettina Berendt, Tina Kruegel, Christian Heinze, Klaus Broelemann, Gjergji Kasneci, Thanassis Tiropanis, and Steffen Staab. Bias in data-driven artificial intelligence systems - an introductory survey. <em>WIREs Data Mining Knowl. Discov.</em>, 2020.</p>
</dd>
<dt class="label" id="id151"><span class="brackets"><a class="fn-backref" href="#id6">5</a></span></dt>
<dd><p>Joshua A. Kroll, Joanna Huey, Solon Barocas, Edward W. Felten, Joel R. Reidenberg, David G. Robinson, and Harlan Yu. Accountable algorithms. <em>U. of Penn. Law Review</em>, 165:633–705, 2017.</p>
</dd>
<dt class="label" id="id116"><span class="brackets"><a class="fn-backref" href="#id7">6</a></span></dt>
<dd><p>Ben Hutchinson and Margaret Mitchell. 50 years of test (un)fairness: lessons for machine learning. In <em>FAT</em>, 49–58. ACM, 2019.</p>
</dd>
<dt class="label" id="id115"><span class="brackets"><a class="fn-backref" href="#id7">7</a></span></dt>
<dd><p>Reuben Binns. Fairness in machine learning: lessons from political philosophy. In <em>FAT</em>, volume 81 of Proceedings of Machine Learning Research, 149–159. PMLR, 2018.</p>
</dd>
<dt class="label" id="id127"><span class="brackets"><a class="fn-backref" href="#id7">8</a></span></dt>
<dd><p>Andrea Romei and Salvatore Ruggieri. A multidisciplinary survey on discrimination analysis. <em>Knowl. Eng. Rev.</em>, 29(5):582–638, 2014.</p>
</dd>
<dt class="label" id="id113"><span class="brackets"><a class="fn-backref" href="#id8">9</a></span></dt>
<dd><p>Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. <em>ACM Comput. Surv.</em>, 54(6):115:1–115:35, 2021.</p>
</dd>
<dt class="label" id="id181"><span class="brackets"><a class="fn-backref" href="#id8">10</a></span></dt>
<dd><p>Indre Zliobaite. Measuring discrimination in algorithmic decision making. <em>Data Min. Knowl. Discov.</em>, 31(4):1060–1089, 2017.</p>
</dd>
<dt class="label" id="id124"><span class="brackets"><a class="fn-backref" href="#id9">11</a></span></dt>
<dd><p>Karima Makhlouf, Sami Zhioua, and Catuscia Palamidessi. On the applicability of machine learning fairness notions. <em>SIGKDD Explor.</em>, 23(1):14–23, 2021.</p>
</dd>
<dt class="label" id="id123"><span class="brackets"><a class="fn-backref" href="#id9">12</a></span></dt>
<dd><p>Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Allison Woodruff, Christine Luu, Pierre Kreitmann, Jonathan Bischof, and Ed H. Chi. Putting fairness principles into practice: challenges, metrics, and improvements. In <em>AIES</em>, 453–459. ACM, 2019.</p>
</dd>
<dt class="label" id="id122"><span class="brackets"><a class="fn-backref" href="#id10">13</a></span></dt>
<dd><p>Michelle Seng Ah Lee and Luciano Floridi. Algorithmic fairness in mortgage lending: from absolute conditions to relational trade-offs. <em>Minds Mach.</em>, 31(1):165–191, 2021.</p>
</dd>
<dt class="label" id="id125"><span class="brackets"><a class="fn-backref" href="#id11">14</a></span></dt>
<dd><p>Casey Fiesler, Natalie Garrett, and Nathan Beard. What do we teach when we teach tech ethics?: A syllabi analysis. In <em>SIGCSE</em>, 289–295. ACM, 2020.</p>
</dd>
<dt class="label" id="id264"><span class="brackets"><a class="fn-backref" href="#id12">15</a></span></dt>
<dd><p>Michelle Seng Ah Lee and Jatinder Singh. The landscape and gaps in open source fairness toolkits. In <em>CHI</em>, 699:1–699:13. ACM, 2021.</p>
</dd>
<dt class="label" id="id126"><span class="brackets"><a class="fn-backref" href="#id12">16</a></span></dt>
<dd><p>Brianna Richardson and Juan E. Gilbert. A framework for fairness: A systematic review of existing fair AI solutions. <em>CoRR</em>, 2021.</p>
</dd>
<dt class="label" id="id150"><span class="brackets"><a class="fn-backref" href="#id449">17</a></span></dt>
<dd><p>Salvatore Ruggieri. Algorithmic fairness. In <em>Elgar Encyclopedia of Law and Data Science</em>. Edward Elgar Publishing Limited, 2022.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Salvatore Ruggieri.</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="readapt"><span class="brackets"><a class="fn-backref" href="#id4">1</a></span></dt>
<dd><p>This Section was readapted from <span id="id449">[<a class="reference internal" href="#id150">17</a>]</span>.</p>
</dd>
</dl>
</div>
<div class="toctree-wrapper compound">
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Diversity, Non-Discrimination, and Fairness"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../Technical%20Robustness%20and%20Safety/data_poisoning.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Data Poisoning</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="auditing.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Auditing AI</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By TAILOR WP3 members; see <a href="/handbookTAI/authors.html" target="_blank">here</a> for the complete list of contributors.<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>