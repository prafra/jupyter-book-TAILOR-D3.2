# Alignment

*Synonyms*: (Mis)directed behaviour, (Un)intended behaviour

The goal of AI **alignment** is to ensure that AI systems are aligned with human intentions and values. This first requires determining the normative question of what values or principles we have and what humans really want, collectively or individually, and second, the technical question of how to imbue AI systems with these values and goals.

You can find futher information about Alignment [here](../../Technical_Robustness_and_Safety/alignment.md)
