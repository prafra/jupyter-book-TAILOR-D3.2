# Differential Privacy

## In brief

**Differential privacy** implies that adding or deleting a single record does not significantly affect the result of any analysis.

## More in detail

### The Family of Differential Privacy Models
Differential privacy is a prominent family of privacy-preserving data
publishing models (see {doc}`./L1.privacy_model`). It comprehends
privacy as the ability to set a limit on the impact of any single
individual on the outputs of the function that produces the
information to publish (computation, e.g., of a set of statistics, of
a machine learning model, of generated synthetic data). In other
words, a differentially private function promises to each individual
that its outputs will be more or less the same whether the
individual's data is input by the function or not. Differential
privacy models all share this common intuitive goal but they differ in
the way they formalize it - for example, on the quantification of the
impact of an individual or on the tolerance to possible failures of
the guarantees (though improbable). They usually exhibit properties
that have been identified as key requirements to privacy models. <!--(see {doc}`./L2.privmod_properties` for details).


### Achieving Differential Privacy
Designing a function that satisfies differential privacy often boils
down to carefully combining basic perturbation mechanisms (such as,
e.g., the Laplace mechanism) and to demonstrating formally either that
data is only accessed through a differentially private function
(leveraging the safety under post-processing and the
self-composability properties), or that the output distribution of the
function complies with the targeted differential privacy model
(through, e.g., randomness alignments). We refer the interested reader
to {doc}`./L2.perturbation_mechanisms` for more information.


### An expanding universe
The seminal differential privacy models were proposed in the
mid-2000's and include $\epsilon$-differential privacy (see
[$\epsilon$-Differential Privacy](./L3.epsilon_DP)) or $(\epsilon,
\delta)$-differential privacy (see <!--{doc}`./L3.epsilon_delta_DP` -
--> [($\epsilon$,$\delta$)-Differential
Privacy](./L3.epsilon_delta_DP.md)). The number of differential
privacy models has grown fastly over the years (more than 200
extensions or variants have been reported in a 2020 survey
paper). Differential privacy is often considered in the academia as a
*de facto* standard for privacy-preserving data publishing and has
earned the original authors the prestigious GÃ¶del Prize
in 2017. Famous organizations (e.g., the US Census Bureau) and
companies (e.g., Google, Apple, LinkedIn, Microsoft) have launched
ambitious real-life applications of differential privacy.


## Bibliography
The seminal differential privacy models were introduced in
{cite:p}`Dwork2006DifferentialP`, {cite:p}`Dwork2006CalibratingNT`,
and {cite:p}`Dwork2006OurDO`. Differential privacy is thoroughly
introduced in {cite:p}`Dwork2014TheAF` and numerous variants and
extensions are surveyed in {cite:p}`Desfontaines2020SoKDP`. The book
{cite:p}`Near2021DifferentialPF` surveys differential privacy
techniques related to database queries.

```{bibliography}
:style: unsrt
:filter: docname in docnames
```

> This entry was written by Tristan Allard.
