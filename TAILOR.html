
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>The TAILOR Handbook of Trustworthy AI</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/tailor.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="http://tailor.isti.cnr.it/handbookTAI/index.html/TAILOR.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-2 bd-sidebar site-navigation show single-page" id="site-navigation">
    
</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            
            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/prafra/jupyter-book-TAILOR-D3.2"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Repository di origine"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/prafra/jupyter-book-TAILOR-D3.2/issues/new?title=Issue%20on%20page%20%2FTAILOR.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Apri un problema"><i class="fas fa-lightbulb"></i>questione aperta</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Modalità schermo intero"
        title="Modalità schermo intero"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contenuti
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="TAILOR.html#document-main/Ethical_Legal_Framework/Ethical_Legal_Framework">
   The Ethical and Legal Framework
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-main/Ethical_Legal_Framework/HLEG">
     Ethics Guidelines for Trustworthy AI by High-Level Expert Group on Artificial Intelligence
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-main/Ethical_Legal_Framework/AI_ACT">
     The EU AI Act
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-main/Ethical_Legal_Framework/Prohibited_AI">
       Prohibited AI Practices
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-main/Ethical_Legal_Framework/High_Risk_AI">
       High Risk AI Systems
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="TAILOR.html#document-main/Trustworthy_AI">
   Trustworthy AI
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-Human_Agency_and_Oversight/Human_Agency_and_Oversight">
     Human Agency and Oversight
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Human_Agency_and_Oversight/Meaningful_human_control">
       Meaningful human control
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Human_Agency_and_Oversight/Causal_responsibility">
       Causal Responsibility
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-Transparency/Transparency">
     Transparency
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Transparency/XAI_dimensions">
       Dimensions of Explanations
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Transparency/blackbox_transparent">
         Black Box Explanation vs Explanation by Design
        </a>
       </li>
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Transparency/model_specific">
         Model-Specific vs Model-Agnostic Explainers
        </a>
       </li>
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Transparency/global_local">
         Global vs Local Explanations
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Transparency/XAI">
       Explainable AI
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Transparency/XAI_kinds">
         Kinds of Explanations
        </a>
        <ul class="nav section-nav flex-column">
         <li class="toctree-l5 toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="TAILOR.html#document-Transparency/counterfactual">
           Counterfactuals
          </a>
         </li>
         <li class="toctree-l5 toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="TAILOR.html#document-Transparency/feature_importance">
           Feature Importance
          </a>
         </li>
         <li class="toctree-l5 toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="TAILOR.html#document-Transparency/lore">
           Local Rule-based Explanation
          </a>
         </li>
         <li class="toctree-l5 toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="TAILOR.html#document-Transparency/prototypes">
           Prototypes
          </a>
         </li>
         <li class="toctree-l5 toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="TAILOR.html#document-Transparency/rules">
           Rules List and Rules Set
          </a>
         </li>
         <li class="toctree-l5 toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="TAILOR.html#document-Transparency/saliency_maps">
           Saliency Maps
          </a>
         </li>
         <li class="toctree-l5 toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="TAILOR.html#document-Transparency/single_tree">
           Single Tree Approximation
          </a>
         </li>
        </ul>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-Technical_Robustness_and_Safety/Technical_Robustness_and_Safety">
     Technical Robustness and Safety
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Technical_Robustness_and_Safety/alignment">
       Alignment
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Technical_Robustness_and_Safety/robustness">
       Robustness
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Technical_Robustness_and_Safety/reliability">
       Reliability
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Technical_Robustness_and_Safety/evaluation">
       Evaluation
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Technical_Robustness_and_Safety/negative_side_effects">
       Negative side effects
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Technical_Robustness_and_Safety/distributional_shift">
       Distributional shift
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Technical_Robustness_and_Safety/security">
       Security
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Technical_Robustness_and_Safety/adversarial_attack">
       Adversarial Attack
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Technical_Robustness_and_Safety/data_poisoning">
       Data Poisoning
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Technical_Robustness_and_Safety/uncertainty">
       Uncertainty
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/Diversity_Non-Discrimination_and_Fairness">
     Diversity, Non-Discrimination, and Fairness
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/auditing">
       Auditing AI
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/bias">
       Bias
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/bias_factors">
       Bias Conducive Factors
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/bias_lmm">
       Bias and Fairness in LLMs
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/equity">
       Discrimination &amp; Equity
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/fairness">
       Fairness notions and metrics
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/fair_ML">
       Fair Machine Learning
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/discrimination">
       Grounds of Discrimination
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/intersectionality">
       Intersectionality
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/justice">
       Justice
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/segregation">
       Segregation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-Accountability/Accountability_and_Reproducibility">
     Accountability
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Accountability/L2.Accountability">
       Accountability
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Accountability/L3.Wicked_problems">
         Wicked problems
        </a>
       </li>
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Accountability/L3.The_frame_problem">
         The Frame Problem
        </a>
       </li>
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Accountability/L3.Problem_of_many_hands">
         The Problem of Many Hands
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Accountability/L2.Reproducibility">
       Reproducibility
      </a>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Accountability/L2.Traceability">
       Traceability
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Accountability/L3.Provenance_tracking">
         Provenance Tracking
        </a>
       </li>
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Accountability/L3.Continuous_performance_monitoring">
         Continuous Performance Monitoring
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-Privacy_and_Data_Governance/Privacy_and_Data_Governance">
     Privacy and Data Governance
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Privacy_and_Data_Governance/L1.anonymization">
       Data Anonymization (and Pseudonymization)
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.pseudonymization">
         Pseudonymization
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Privacy_and_Data_Governance/L1.privacy_model">
       Privacy Models
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.randomization">
         Randomization Methods
        </a>
       </li>
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.differential_privacy">
         Differential Privacy
        </a>
        <ul class="nav section-nav flex-column">
         <li class="toctree-l5 toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="TAILOR.html#document-Privacy_and_Data_Governance/L3.epsilon_DP">
           <span class="math notranslate nohighlight">
            \(\epsilon\)
           </span>
           -Differential Privacy
          </a>
         </li>
         <li class="toctree-l5 toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="TAILOR.html#document-Privacy_and_Data_Governance/L3.epsilon_delta_DP">
           (
           <span class="math notranslate nohighlight">
            \(\epsilon\)
           </span>
           ,
           <span class="math notranslate nohighlight">
            \(\delta\)
           </span>
           )-Differential Privacy
          </a>
         </li>
         <li class="toctree-l5 toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.perturbation_mechanisms">
           Achieving Differential Privacy
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.indistinguishability">
         Anonymity by Indistinguishability
        </a>
        <ul class="nav section-nav flex-column">
         <li class="toctree-l5 toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.k_anonymity">
           k-anonymity
          </a>
         </li>
         <li class="toctree-l5 toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.l_diversity">
           l-diversity
          </a>
         </li>
         <li class="toctree-l5 toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.t_closeness">
           t-closeness
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.federated">
         Federated Learning
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Privacy_and_Data_Governance/L1.attacks">
       Attacks on anonymization schemes
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.reidentification">
         Re-identification Attack
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/Societal_and_Environmental_Wellbeing">
     Societal and Environmental Wellbeing
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/sustenaible_AI">
       Sustainable AI
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/greenAI">
         Green AI
        </a>
        <ul class="nav section-nav flex-column">
         <li class="toctree-l5 toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/power_aware">
           Power-aware Computing
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/cloud_computing">
         Cloud Computing
        </a>
       </li>
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/edge_computing">
         Edge Computing
        </a>
       </li>
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/data_centre">
         Data Centre
        </a>
       </li>
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/cradle_to_cradle">
         Cradle-to-cradle Design
        </a>
       </li>
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/resource_prediction">
         Resource Prediction
        </a>
       </li>
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/resource_allocation">
         Resource Allocation
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/social_impact">
       Social Impact of AI Systems
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/human_interaction">
         AI human interaction
        </a>
        <ul class="nav section-nav flex-column">
         <li class="toctree-l5 toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/self-identification">
           Self-identification of AI
          </a>
         </li>
         <li class="toctree-l5 toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/emotional_impact">
           Emotional Impact
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/workforce_impact">
         AI Impact on the Workforce
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/society_and_democracy">
       Society and Democracy
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/social_scoring">
         AI for social scoring
        </a>
       </li>
       <li class="toctree-l4 toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/propaganda">
         AI for propaganda
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="TAILOR.html#document-TAILOR_project">
   The TAILOR project
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="TAILOR.html#document-authors">
   Complete List of Contributors
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="TAILOR.html#document-main/AnalyticalIndex">
   Index
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/2CC2">
     2CC2
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Accountability/Accountability">
     Accountability
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/propaganda">
     AI for propaganda
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/social_scoring">
     AI for social scoring
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/human_interaction">
     AI human interaction
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/workforce_impact">
     AI Impact on the Workforce
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Alignment">
     Alignment
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Adversarial attack">
     Adversarial Attack
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Adversarial example">
     Adversarial Example
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Adversarial input">
     Adversarial Input
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Indistinguishability">
     Anonymity by Indistinguishability
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Ante-hoc Explanation">
     Ante-hoc Explanation
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Assessment">
     Assessment
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Attacks Anonym">
     Attacks on Anonymization Schema
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Attacks on Pseudonymised Data">
     Attacks on Pseudonymised Data
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/Auditing">
     Auditing
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/Bias">
     Bias
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/bias_factors">
     Bias Conducive Factors
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/bias_lmm">
     Bias and Fairness in LLMs
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Black-box Explanations">
     Black-box Explanation
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Brittleness">
     Brittleness
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/C2C">
     C2C
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Human_Agency_and_Oversight/Causal_responsibility">
     Causal Responsibility
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Cloud Computing">
     Cloud Computing
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Accountability/Continuous monitoring">
     Continuous Performance Monitoring
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Counterexemplar">
     Counterexemplars
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Counterfactual">
     Counterfactuals
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/cradle 2 cradle">
     Cradle 2 cradle
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Cradle">
     Cradle-to-cradle Design
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Data Anonymization">
     Data Anonymization
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Data Center">
     Data Center
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Data Poisoning">
     Data Poisoning
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Dependability">
     Dependability
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Differential Privacy models">
     Differential Privacy Models
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/emotional_impact">
     Emotional Impact
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/epsilon_delta-differential_privacy">
     (
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
     ,
     <span class="math notranslate nohighlight">
      \(\delta\)
     </span>
     )-Differential Privacy
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Epsilon-differential_privacy">
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
     -Differential Privacy
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Epsilon-indist">
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
     -Indistinguishability
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Data Shift">
     Distributional Shift
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Dimensions of Explanations">
     Dimensions of Explanations
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/Discrimination">
     Grounds of Discrimination
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Distributional Shift">
     Distributional Shift
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Direct">
     Direct Behaviour
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Edge Computing">
     Edge Computing
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Energy Aware">
     Energy-aware Computing
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Energy Efficient">
     Energy-efficient Computing
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Evaluation">
     Evaluation
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/Equity">
     Equity
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Exemplars">
     Exemplars
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Explainable AI">
     Explainable AI
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Explanation by Design">
     Explanation by Design
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/Fair Machine Learning">
     Fair Machine Learning
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/Fairness">
     Fairness
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Features Importance">
     Feature Importance
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Federated">
     Federated Learning
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Accountability/Frame">
     The Frame Problem
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Fog Computing">
     Fog Computing
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Generalizable XAI">
     Model Agnostic
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Global Explanations">
     Global Explanations
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Green AI">
     Green AI
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Green Computing">
     Green Computing
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Green IT">
     Green IT
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/ICT sustainability">
     ICT sustainability
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Intended">
     Intended Behaviour
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/intersectionality">
     Intersectionality
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/Justice">
     Justice
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/K-Anonymity">
     K-anonymity
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/L_diversity">
     l-diversity
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Linking Attack">
     Linking Attack
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Local Explanations">
     Local Explanations
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Lore">
     Local Rule-based Explanation
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Human_Agency_and_Oversight/Meaningful_human_control">
     Meaningful Human Control
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Measurement">
     Measurement
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Mesh Computing">
     Mesh Computing
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Misdirect">
     Misdirect Behaviour
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Model-Agnostic">
     Model Agnostic
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Model-Specific">
     Model Specific
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Negative Side Effects">
     Negative Side Effects
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Not Generalizable XAI">
     Model Specific
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Perturbation">
     Achiving Differential Privacy
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Post-hoc Explanations">
     Post-hoc Explanation
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Power Aware">
     Power-aware Computing
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Privacy model">
     Privacy models
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Accountability/Problem_of_many_hands">
     Problem of Many Hands
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Prototypes">
     Prototypes
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Accountability/Provenance">
     Provenance Tracking
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Pseudonymised Data">
     Pseudonymization
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Randomization">
     Randomization Methods
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Re-identification Attack">
     Re-identification Attack
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Regenerative Design">
     Regenerative Design
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Reliability">
     Reliability
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Accountability/Repeatability">
     Repeatability
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Accountability/Replicability">
     Replicability
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Accountability/Reproducibility">
     Reproducibility
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Resource Allocation">
     Resource Allocation
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Resource Prediction">
     Resource Prediction
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Resource Scheduling">
     Resource Scheduling
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Robustness">
     Robustness
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Rules">
     Rules List and Rules Set
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Saliency Maps">
     Saliency Maps
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Security">
     Security
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/Segregation">
     Segregation
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/self-identification">
     Self-identification of AI
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Single Tree Approximation">
     Single Tree Approxiamation
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/social_impact">
     Social Impact of AI Systems
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/society_and_democracy">
     Society and Democracy
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Privacy_and_Data_Governance/T_closeness">
     t-closeness
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Testing">
     Testing
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Accountability/Traceability">
     Traceability
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/Transparency">
     Transparency
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Unintended">
     Unintended Behaviour
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Transparency/XAI">
     XAI
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Accountability/Wicked">
     Wicked Problems
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Workload Forecast">
     Workload Forecast
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Workload Prediction">
     Workload Prediction
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="the-tailor-handbook-of-trustworthy-ai">
<h1>The TAILOR Handbook of Trustworthy AI<a class="headerlink" href="#the-tailor-handbook-of-trustworthy-ai" title="Permalink to this headline">¶</a></h1>
<p><strong>An encyclopedia of the major scientific and technical terms related to Trustworthy Artificial Intelligence</strong></p>
<!--**TAILOR: Foundations of Trustworthy AI – Integrating Reasoning, Learning and Optimization**

Add an introduction to the first page.
Add an Executive summary to the first page. This should be stand-alone, so that we can use it in formal reporting, for websites, social media etc.
-->
<p>According the <a class="reference internal" href="TAILOR.html#document-main/Ethical_Legal_Framework/HLEG"><span class="doc">Ethics Guidelines for Trustworthy AI by High-Level Expert Group on Artificial Intelligence</span></a>, <strong>Trustworthy Artificial Intelligence (Trustworthy AI)</strong> has three components, which should be met throughout the system’s entire life cycle. Indeed, it should be:</p>
<ol class="simple">
<li><p><em>lawful</em>, complying with all applicable laws and regulations;</p></li>
<li><p><em>ethical</em>, ensuring adherence to ethical principles and values;</p></li>
<li><p><em>robust</em>, both from a technical and social perspective since, even with good intentions, AI systems can cause unintentional harm.</p></li>
</ol>
<div class="section" id="executive-abstract">
<h2>Executive Abstract<a class="headerlink" href="#executive-abstract" title="Permalink to this headline">¶</a></h2>
<p>The main goal of the <em>Handbook of Trustworthy AI</em> (HTAI) is to provide to non-experts, researchers and students, an overview of the problem related to the developing of ethical and trustworthy AI systems. In particular, the HTAI aims at providing an overview of the main dimensions of trustworthiness, starting with a understandable explanation of the dimension itself, and then presenting the characterization of the problems (starting with a brief summary and the explanation of the importance of the dimension, presenting a taxonomy and some guidelines, if they are available and consolidated), summarizing what are the major challenges and solutions in the field, and concluding with what are the latest research developments.</p>
<p>Each entry will be correlated with a bibliography, allowing the reader to go more in depth with a specific topic if interested knowing more about it.</p>
<p>All the entries have a list of authors that have directly contributed to the writing of the HTAI (some of them are already external to the TAILOR consortium), while the complete list of contributors can be found <a class="reference internal" href="TAILOR.html#document-authors"><span class="doc std std-doc">here</span></a>.</p>
</div>
<div class="section" id="about-the-tailor-handbook">
<h2>About the TAILOR Handbook<a class="headerlink" href="#about-the-tailor-handbook" title="Permalink to this headline">¶</a></h2>
<p>The HTAI assumes an encyclopedia-like structure and is presented in the form of a publicly accessible WIKI. To do so, the Jupiter Book framework has been used. In the long term, the Handbook is meant to become a point of reference for resources (key concepts, tools, documentation, tutorials, teaching material, etc.) related to Trustworthy AI.</p>
<p>Here, you can find the <a class="reference internal" href="TAILOR.html#document-main/Ethical_Legal_Framework/Ethical_Legal_Framework"><span class="doc">The Ethical and Legal Framework</span></a> we are referring to, to have the context of the current European context.</p>
<!-- {doc}`../T3.4/L1.Accountability_and_Reproducibility`. -->
<p>The organization of the chapters is based on the seven principles described in the Ethics Guidelines for Trustworthy AI of the High-Level Expert Group on Artificial Intelligence <span id="id1">[<a class="reference internal" href="#id22">1</a>]</span>. In particular, in this Encyclopedia, you can find definitions related to:</p>
<ul class="simple">
<li><p><a class="reference internal" href="TAILOR.html#document-Human_Agency_and_Oversight/Human_Agency_and_Oversight"><span class="doc std std-doc">Human Agency and Oversight</span></a>. This section of the Encyclopedia defines Human agency, i.e., the necessity of knowledge and tools to comprehend and interact properly with AI systems, and Human oversight in different phases of AI processes.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/Technical_Robustness_and_Safety"><span class="doc std std-doc">Technical Robustness and Safety</span></a>. In this section of the Encyclopedia, we analyze the challenges in developing AI systems that are safe, reliable, and robust; we also provide a way to evaluate these aspects in practice, and we promote the dynamic evaluation in managing risk during the normal use of AI systems.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/Privacy_and_Data_Governance"><span class="doc std std-doc">Privacy and Data Governance</span></a>. This section will focus mostly on the Respect for Privacy side. Here, we provide an overview of the main attacks that can threaten individual privacy, explain the difference between pseudonymization and actual anonymization, and describe the main family of privacy models.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Transparency/Transparency"><span class="doc std std-doc">Transparency</span></a>. In this part of the Handbook, we will first provide the distinction between a transparent model and an explainable one (indeed, in the ML context, a commonly study problem is related to the Explainable AI). In this section, we provide an overview of the main properties that an explanation should have and of the several methods to provide multi-modal explanations; moreover, our focus will also be on overcoming the need to explain the opaque model and, instead, move toward the use of transparent models.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Accountability/Accountability_and_Reproducibility"><span class="doc std std-doc">Accountability</span></a>. Here, we analyze the two souls of this topic, the two interrelated concepts of Accountability and Reproducibility: the former term is more related to responsibility, blameworthiness, liability, and preventing misuse, while the latter term is more related to measures, quality standards, and procedures to model the development of learning methods for AI.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/Diversity_Non-Discrimination_and_Fairness"><span class="doc std std-doc">Diversity, Non-Discrimination, and Fairness</span></a>. In this chapter, we will start recalling what the grounds of discrimination are, how we can define a bias or segregation; then, we will make a step in determining what fair machine learning could be, what are the metrics we can adopt to measure (un)fairness, and, more generally, how we could move towards Justice by Design systems.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/Societal_and_Environmental_Wellbeing"><span class="doc std std-doc">Societal and Environmental Wellbeing</span></a>. The last chapter of the Handbook is focus of one of the newest challenge that our society is facing. In particular, our focus is on environmental wellbeing (aka sustainability) and on providing solutions to optimize both the resources used in AI systems and the computation itself.</p></li>
</ul>
<p>As one can see, we mapped one of the seven requirements for Trustworthy AI, as defined by the High-Level Expert Group on AI <span id="id2">[<a class="reference internal" href="#id22">1</a>]</span>, in a chapter of this Handbook.
However, there are some differences in emphasis, as the <a href="https://tailor-network.eu" target="_blank">TAILOR project</a>
is more focused on specific aspects, e.g., more on privacy than on data governance, more on the sustainability of AI than on societal wellbeing, and more on <a class="reference internal" href="TAILOR.html#document-Transparency/blackbox_transparent"><span class="doc std std-doc">post-hoc explanations</span></a> than on <a class="reference internal" href="TAILOR.html#document-Transparency/Transparency"><span class="doc std std-doc">transparency</span></a>.</p>
<p>Finally, we report a final chapter, where you can find an <a class="reference internal" href="TAILOR.html#document-main/AnalyticalIndex"><span class="doc std std-doc">Index</span></a> that lists all entries in alphabetical order. In each term, you can find a reference to a short definition of the entry and where it is used within the Handbook, with a link to go more in-depth with the definition. Potential synonyms have their own entries in this index.</p>
</div>
<div class="section" id="related-work">
<h2>Related work<a class="headerlink" href="#related-work" title="Permalink to this headline">¶</a></h2>
<p>With respect to other encyclopedias or handbooks, the TAILOR HTAI is characterized by:</p>
<ul class="simple">
<li><p>A choice of terms directly related to the experience of the <span class="xref myst">TAILOR</span> consortium; i.e., all the entries were chosen by the TAILOR partners in a bottom-up way and inserted in the Handbook if they are relevant to the research pursued in the project development.</p></li>
<li><p>The fact that, given the living-document nature of the Handbook and the attitude of oneness of the TAILOR network of excellence, we are ready to correct, integrate, and update information, and receptive to (even external) suggestions.</p></li>
<li><p>The wiki structure will permit to take advantage of hyperlinks to jump directly to correlated terms.</p></li>
<li><p>Unlike other encyclopedias (such as <span id="id3">[<a class="reference internal" href="#id20">2</a>, <a class="reference internal" href="#id21">3</a>]</span>), it is freely accessible.</p></li>
<li><p>We are not focused on general and omni-comprehensive machine learning or artificial intelligence resources (for this, other work <span id="id4">[<a class="reference internal" href="#id19">4</a>]</span> can be more exhaustive), or very specific ethical aspects on very specific topics (e.g., privacy in mobile data <span id="id5">[<a class="reference internal" href="#id18">5</a>]</span>), but, starting from the work done in the High-Level Expert Group report <span id="id6">[<a class="reference internal" href="#id22">1</a>]</span>, which is one of the leading work in the EU, we specifically focus on the operative aspects of trustworthy AI.</p></li>
</ul>
<!--The plan will be to integrate it into the TAILOR web page and to make a Wikipedia entry (by v2 of the handbook). A final paper book is also planned by then.-->
</div>
<div class="section" id="bibliography">
<h2>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h2>
<p id="id7"><dl class="citation">
<dt class="label" id="id22"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>,<a href="#id6">3</a>)</span></dt>
<dd><p>High-Level Expert Group on Artificial Intelligence. Ethics guidelines for trustworthy ai. URL: <a class="reference external" href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai">https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</a> (visited on 2024-04-23).</p>
</dd>
<dt class="label" id="id20"><span class="brackets"><a class="fn-backref" href="#id3">2</a></span></dt>
<dd><p>Giovanni Comandé, editor. <em>Elgar Encyclopedia of Law and Data Science</em>. Edward Elgar Publishing, 2022. ISBN 978-1.83910-458-9. URL: <a class="reference external" href="https://www.e-elgar.com/shop/gbp/elgar-encyclopedia-of-law-and-data-science-9781839104589.html">https://www.e-elgar.com/shop/gbp/elgar-encyclopedia-of-law-and-data-science-9781839104589.html</a>.</p>
</dd>
<dt class="label" id="id21"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Juan Ramón Rabuñal Dopico, Julian Dorado, and Alejandro Pazos, editors. <em>Encyclopedia of Artificial Intelligence (3 Volumes)</em>. IGI Global, 2008. ISBN 9781599048499. <a class="reference external" href="https://doi.org/10.4018/978-1-59904-849-9">doi:10.4018/978-1-59904-849-9</a>.</p>
</dd>
<dt class="label" id="id19"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Claude Sammut and Geoffrey I. Webb, editors. <em>Encyclopedia of Machine Learning and Data Mining</em>. Springer, 2017. ISBN 978-1-4899-7685-7. URL: <a class="reference external" href="https://doi.org/10.1007/978-1-4899-7687-1">https://doi.org/10.1007/978-1-4899-7687-1</a>, <a class="reference external" href="https://doi.org/10.1007/978-1-4899-7687-1">doi:10.1007/978-1-4899-7687-1</a>.</p>
</dd>
<dt class="label" id="id18"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>Aris Gkoulalas-Divanis and Claudio Bettini, editors. <em>Handbook of Mobile Data Privacy</em>. Springer, 2018. ISBN 978-3-319-98160-4. URL: <a class="reference external" href="https://doi.org/10.1007/978-3-319-98161-1">https://doi.org/10.1007/978-3-319-98161-1</a>, <a class="reference external" href="https://doi.org/10.1007/978-3-319-98161-1">doi:10.1007/978-3-319-98161-1</a>.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Francesca Pratesi and Umberto Straccia.</p>
</div></blockquote>
<blockquote>
<div><p>This research was partially supported by TAILOR, a project funded by EU Horizon 2020 research and innovation programme under GA No 952215</p>
</div></blockquote>
</div>
<div class="toctree-wrapper compound">
<span id="document-main/Ethical_Legal_Framework/Ethical_Legal_Framework"></span><div class="tex2jax_ignore mathjax_ignore section" id="the-ethical-and-legal-framework">
<h2>The Ethical and Legal Framework<a class="headerlink" href="#the-ethical-and-legal-framework" title="Permalink to this headline">¶</a></h2>
<!-- TODO 

point the link
[risk-based approach](./Ethical_Legal_Framework/AI_ACT.md}
to the correct section of the entry


-->
<p>In these pages, we focus on the European framework only, mostly relying on two main sources, which are described below and in the linked pages.</p>
<p>The first document, at least chronologically speaking, we refer to is the <a class="reference internal" href="TAILOR.html#document-main/Ethical_Legal_Framework/HLEG"><span class="doc std std-doc">Ethical Guidelines for Trustworthy AI</span></a> <span id="id1">[<a class="reference internal" href="TAILOR.html#id22">1</a>]</span>, which, as the name itself suggests is not a law or a legal obligation. Nevertheless, it is commonly recognized as the most relevant document in the field of Trustworthy AI.
Here, as we already mentioned, there are listed a definition of Trustworthy AI, the foundation of Trustworthy AI, the seven key requirements that AI systems should implement and meet throughout their entire life cycle, and a concrete assessment list to operationalize the requirements.</p>
<p>Then, the other fundamental source is the world’s first comprehensive law on Artificial Intelligence (AI): the <a class="reference internal" href="TAILOR.html#document-main/Ethical_Legal_Framework/AI_ACT"><span class="doc std std-doc">EU AI Act</span></a> <span id="id2">[<a class="reference internal" href="TAILOR.html#id23">1</a>]</span>. The text provides a classification of AI systems using a <a class="reference internal" href="TAILOR.html#document-main/Ethical_Legal_Framework/AI_ACT"><span class="doc std std-doc">risk-based approach</span></a>; four levels of risk were identified, and different obligations are listed for the different categories of AI systems to be compliant with this law.</p>
<p id="id3"><dl class="citation">
<dt class="label" id="id18"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>High-Level Expert Group on Artificial Intelligence. Ethics guidelines for trustworthy ai. URL: <a class="reference external" href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai">https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</a> (visited on 2024-04-23).</p>
</dd>
<dt class="label" id="id20"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Artificial Intelligence Act, European Parliament legislative resolution of 13 March 2024 on the proposal for a regulation of the European Parliament and of the Council on laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union Legislative Acts (COM(2021)0206 – C9-0146/2021 – 2021/0106(COD)), P9_TA(2024)0138. 2024. URL: <a class="reference external" href="https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.pdf">https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.pdf</a> (visited on 2024-04-23).</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Francesca Pratesi and Umberto Straccia.</p>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-main/Ethical_Legal_Framework/HLEG"></span><div class="tex2jax_ignore mathjax_ignore section" id="ethics-guidelines-for-trustworthy-ai-by-high-level-expert-group-on-artificial-intelligence">
<h3>Ethics Guidelines for Trustworthy AI by High-Level Expert Group on Artificial Intelligence<a class="headerlink" href="#ethics-guidelines-for-trustworthy-ai-by-high-level-expert-group-on-artificial-intelligence" title="Permalink to this headline">¶</a></h3>
<!--

point the link
[many different interpretations of fairness](../../Diversity_Non-Discrimination_and_Fairness/Diversity_Non-Discrimination_and_Fairness), possibly at the point "Quantitative definitions have been introduced in philosophy, economics, and machine learning in the last 50 years"}

+
\link{``black box''}\footnote{XAI, possibly to 'the Open the Black-Box problem' section}

-->
<p>On 8 April 2019, the High-Level Expert Group on AI presented the <em>‘’Ethics Guidelines for Trustworthy Artificial Intelligence’’</em> <span id="id1">[<a class="reference internal" href="TAILOR.html#id22">1</a>]</span>, an influential document that set the stage for Trustworthy AI.</p>
<div class="section" id="in-brief">
<h4>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h4>
<p>The guidance in this document is provided in three chapters, from the most abstract in Chapter I to the most concrete in Chapter III:</p>
<ul class="simple">
<li><p><em>Chapter I – Foundations of Trustworthy AI</em>: sets out the foundations of Trustworthy AI by laying out its fundamental-rights based approach. It identifies and describes the ethical principles that must be adhered to in order to ensure ethical and robust AI.</p></li>
<li><p><em>Chapter II – Realising Trustworthy AI</em>: translates these ethical principles into seven key requirements that AI systems should implement and meet throughout their entire life cycle. In addition, it offers both technical and non-technical methods that can be used for their implementation.</p></li>
<li><p><em>Chapter III – Assessing Trustworthy</em>: sets out a concrete and non-exhaustive Trustworthy AI assessment list to operationalise the requirements of Chapter II, offering AI practitioners practical guidance. This assessment should be tailored to the particular system’s application.</p></li>
</ul>
<p>According to these guidelines, <strong>Trustworthy AI has three components</strong>, which should be met throughout the system’s entire life cycle. Indeed, it should be:</p>
<ol class="simple">
<li><p><strong>lawful</strong>, complying with all applicable laws and regulations;</p></li>
<li><p><strong>ethical</strong>, ensuring adherence to ethical principles and values;</p></li>
<li><p><strong>robust</strong>, both from a technical and social perspective since, even with good intentions, AI systems can cause unintentional harm.</p></li>
</ol>
<p>Each component in itself is necessary but not sufficient for the achievement of Trustworthy AI.  Ideally, all three components work in harmony and overlap in their operation. If, in practice, tensions arise between these components, society should endeavor to align them.</p>
</div>
<div class="section" id="more-in-detail">
<h4>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h4>
<p>In Figure <a class="reference internal" href="#fig-hleg"><span class="std std-numref">1</span></a>, we reported the main outcomes of the Guidelines.
First, the three pillars of Trustworthy AI are introduced. Then, in Chapter I the four ethical principles are described.
Thirdly, the seven key requirements are listed in Chapter II.
Finally, Chapter III offers a more practical way to assess Trustworthy AI.
We will briefly examine all these concepts in the following.</p>
<div class="figure align-center" id="fig-hleg">
<a class="reference internal image-reference" href="_images/hleg.png"><img alt="_images/hleg.png" src="_images/hleg.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 1 </span><span class="caption-text">The Guidelines as a framework for Trustworthy AI <span id="id2">[<a class="reference internal" href="TAILOR.html#id21">1</a>]</span>.</span><a class="headerlink" href="#fig-hleg" title="Permalink to this image">¶</a></p>
</div>
<div class="section" id="the-three-pillars">
<h5>The three pillars<a class="headerlink" href="#the-three-pillars" title="Permalink to this headline">¶</a></h5>
<p>As one can see, the three pillars (described in the document’s introduction) of Trustworthy AI are that an AI system must be lawful, ethical, and robust.
We will provide some additional details on what is intended for each of these three components.</p>
<p><strong>Lawful AI.</strong>
AI systems do not operate in a lawless world. A number of legally binding rules at European, national, and international levels already apply or are relevant to the development, deployment, and use of AI systems today.</p>
<!-- copied from TAI entry -->
<p>Legal sources include, but are not limited to: EU primary law (the <a href="https://european-union.europa.eu/principles-countries-history/principles-and-values/founding-agreements_en" target=_blank>Treaties of the European Union</a> and its <a href="https://www.europarl.europa.eu/charter/pdf/text_en.pdf" target=_blank>Charter of Fundamental Rights</a>), EU secondary law (such as the <a href="https://gdpr-info.eu/" target=_blank>General Data Protection Regulation (GDPR)</a> <span id="id3">[<a class="reference internal" href="TAILOR.html#id8">1</a>]</span>, the <a href="https://single-market-economy.ec.europa.eu/single-market/goods/free-movement-sectors/liability-defective-products_en" target=_blank>Product Liability Directive</a>, the <a href="https://digital-strategy.ec.europa.eu/en/policies/non-personal-data" target=_blank>Regulation on the Free Flow of Non-Personal Data</a>, <a href="https://commission.europa.eu/aid-development-cooperation-fundamental-rights/your-rights-eu/know-your-rights/equality/non-discrimination_en" target=_blank>anti-discrimination Directives</a>, consumer law and <a href="https://osha.europa.eu/en/safety-and-health-legislation/european-directives" target=_blank>Safety and Health at Work Directives</a>, the <a href="https://www.un.org/en/about-us/universal-declaration-of-human-rights" target=_blank>UN Human Rights treaties</a> and the Council of Europe conventions (such as the <a href="https://www.coe.int/en/web/human-rights-convention/the-convention-in-1950" target=_blank>European Convention on Human Rights</a>), and numerous EU Member State laws. Besides horizontally applicable rules, various domain-specific rules exist that apply to particular AI applications (such as, for instance, the Medical Device Regulation in the healthcare sector).</p>
<!-- end paragraph -->
<p>The law provides both positive and negative obligations, which means that it should not only be interpreted with reference to what cannot be done but also concerning what should be done and what may be done. The law not only prohibits certain actions but also enables others. In this regard, it can be noted that the EU Charter contains articles on the ‘freedom to conduct a business’ and the ‘freedom of the arts and sciences’, alongside articles addressing areas that we are more familiar with when looking to ensure AI’s trustworthiness, such as data protection and non-discrimination.</p>
<p>The Guidelines do not explicitly address the first component of Trustworthy AI (lawful AI) since they proceed on the assumption that all legal rights and obligations that apply to the processes and activities involved in developing, deploying, and using AI systems remain mandatory and must be duly observed.</p>
<p><strong>Ethical AI.</strong>
Achieving Trustworthy AI requires not only compliance with the law, which is only one of its three components. Laws are not always up to speed with technological developments, can at times be out of step with ethical norms or may simply not be well suited to addressing certain issues. For AI systems to be trustworthy, they should hence also be ethical, ensuring alignment with ethical norms.</p>
<p><strong>Robust AI.</strong>
Even if an ethical purpose is ensured, individuals and society must also be confident that AI systems will not cause any unintentional harm. Such systems should perform in a safe, secure, and reliable manner. Moreover, safeguards should be foreseen to prevent any unintended adverse impacts. It is, therefore, important to ensure that AI systems are robust. This is needed both from a technical perspective (ensuring the system’s technical robustness as appropriate in a given context, such as the application domain or life cycle phase) and from a social perspective (in due consideration of the context and environment in which the system operates).
Ethical and robust AI are hence closely intertwined and complement each other. The principles put forward in Chapter I, and the requirements derived from these principles in Chapter II, address both components.</p>
</div>
<div class="section" id="the-four-ethical-principles">
<h5>The four ethical principles<a class="headerlink" href="#the-four-ethical-principles" title="Permalink to this headline">¶</a></h5>
<p>AI systems should improve individual and collective wellbeing. The four ethical principles, rooted in fundamental rights, must be respected in order to ensure that AI systems are developed, deployed and used in a trustworthy manner. They are specified as ethical imperatives, such that AI practitioners should always strive to adhere to them.</p>
<p>These are the principles of:</p>
<ol class="simple">
<li><p><strong>Respect for human autonomy</strong>;</p></li>
<li><p><strong>Prevention of harm</strong>;</p></li>
<li><p><strong>Fairness</strong>; and</p></li>
<li><p><strong>Explicability</strong>.</p></li>
</ol>
<p>Many of these are, to a large extent, already reflected in existing legal requirements for which mandatory compliance is required and hence also fall within the scope of lawful AI (for example, in the <a href="https://gdpr-info.eu/" target=_blank>GDPR</a> or in the <a href="https://commission.europa.eu/law/law-topic/consumer-protection-law_en" target=_blank>EU consumer protection regulations</a>), which is Trustworthy AI’s first component.</p>
<p>In the following, we are going to shortly detail those four principles.</p>
<p><strong>The principle of respect for human autonomy.</strong>
Humans interacting with AI systems must be able to maintain full and effective self-determination over themselves and partake in the democratic process. AI systems should not unjustifiably subordinate, coerce, deceive, manipulate, condition or herd humans. Instead, they should be designed to augment, complement, and empower human cognitive, social, and cultural skills. The allocation of functions between humans and AI systems should follow human-centric design principles and leave meaningful opportunities for human choice. This means securing human oversight over work processes in AI systems.</p>
<p><strong>The principle of prevention of harm.</strong>
AI systems should neither cause nor exacerbate harm or otherwise adversely affect human beings, where harms are intended to be both individual or collective, and can include intangible harm to social, cultural and political environment. This entails the protection of human dignity as well as mental and physical integrity. AI systems and the environments in which they operate must be safe and secure. They must be technically robust and it should be ensured that they are not open to malicious use. Vulnerable persons should receive greater attention and be included in the development, deployment, and use of AI systems. Particular attention must also be paid to situations where AI systems can cause or exacerbate adverse impacts due to asymmetries of power or information, such as between employers and employees, businesses and consumers, or governments and citizens. Preventing harm also entails consideration of the natural environment and all living beings.</p>
<p><strong>The principle of fairness.</strong>
The development, deployment, and use of AI systems must be fair. There are, of course, <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/Diversity_Non-Discrimination_and_Fairness"><span class="doc std std-doc">many different interpretations of fairness</span></a>, but according to the Guidelines, this dimension implies a commitment to ensuring equal and just distribution of both benefits and costs and ensuring that individuals and groups are free from unfair bias, discrimination, and stigmatization Equal opportunity in terms of access to education, goods, services and technology should also be fostered. Additionally, fairness implies that AI practitioners should respect the principle of proportionality between means and ends, and consider carefully how to balance competing interests and objectives. <!--The procedural dimension of fairness entails the ability to contest and seek effective redress against decisions made by AI systems and by the humans operating them. In order to do so, the entity accountable for the decision must be identifiable, and the decision-making processes should be explicable.--></p>
<p><strong>The principle of explicability.</strong>
Explicability is crucial for building and maintaining users’ trust in AI systems. This means that processes need to be transparent, the capabilities and purpose of AI systems openly communicated, and decisions – to the extent possible – explainable to those directly and indirectly affected. Without such information, a decision cannot be duly contested. An explanation as to why a model has generated a particular output or decision (and what combination of input factors contributed to that) is not always possible. These cases are referred to as <a class="reference internal" href="TAILOR.html#document-Transparency/blackbox_transparent"><span class="doc std std-doc">‘’black box’’</span></a> algorithms and require special attention. In those circumstances, other explicability measures (e.g., <span class="xref myst">traceability</span>, <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/auditing"><span class="doc std std-doc">auditability</span></a>, and transparent communication on system capabilities) may be required, provided that the system as a whole respects fundamental rights. <!--The degree to which explicability is needed is highly dependent on the context and the severity of the consequences if that output is erroneous or otherwise inaccurate.--></p>
</div>
<div class="section" id="the-seven-key-requirements">
<h5>The seven key requirements<a class="headerlink" href="#the-seven-key-requirements" title="Permalink to this headline">¶</a></h5>
<p>The principles outlined before must be translated into concrete requirements to achieve Trustworthy AI. These requirements are applicable to different stakeholders partaking in AI systems’ life cycle: developers, deployers, and end-users, as well as the broader society. By developers, we refer to those who research, design and/or develop AI systems. By deployers, we refer to public or private organizations that use AI systems within their business processes and offer products and services to others. End-users are those engaging with the AI system, directly or indirectly. Finally, the broader society encompasses all others that are directly or indirectly affected by AI systems.</p>
<p>Different groups of stakeholders have different roles to play in ensuring that the requirements are met:</p>
<ul class="simple">
<li><p>developers should implement and apply the requirements to design and development processes;</p></li>
<li><p>deployers should ensure that the systems they use and the products and services they offer meet the requirements;</p></li>
<li><p>end-users and the broader society should be informed about these requirements and able to request that they are upheld.</p></li>
</ul>
<p>The Guidelines identified a list of seven requirements (see also Figure <a class="reference internal" href="#fig-tairequiremens"><span class="std std-numref">2</span></a>), which will be analyzed more in deep in the respective chapters of this Handbook.</p>
<ul class="simple">
<li><p><a class="reference internal" href="TAILOR.html#document-Human_Agency_and_Oversight/Human_Agency_and_Oversight"><span class="doc std std-doc">Human agency and oversight</span></a>. Including fundamental rights, human agency, and human oversight.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/Technical_Robustness_and_Safety"><span class="doc std std-doc">Technical robustness and safety</span></a>. Including resilience to attack and security, fall back plan and general safety, accuracy, reliability and reproducibility.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/Privacy_and_Data_Governance"><span class="doc std std-doc">Privacy and data governance</span></a>. Including respect for privacy, quality and integrity of data, and access to data.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Transparency/Transparency"><span class="doc std std-doc">Transparency</span></a>. Including traceability, explainability and communication.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/Diversity_Non-Discrimination_and_Fairness"><span class="doc std std-doc">Diversity, non-discrimination and fairness</span></a> Including the avoidance of unfair bias, accessibility and universal design, and stakeholder participation.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/Societal_and_Environmental_Wellbeing"><span class="doc std std-doc">Societal and environmental wellbeing</span></a>. Including sustainability and environmental friendliness, social impact, society and democracy.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Accountability/Accountability_and_Reproducibility"><span class="doc std std-doc">Accountability</span></a>. Including auditability, minimization and reporting of negative impact, trade-offs, and redress.</p></li>
</ul>
<div class="figure align-center" id="fig-tairequiremens">
<a class="reference internal image-reference" href="_images/TAIrequiremens.png"><img alt="_images/TAIrequiremens.png" src="_images/TAIrequiremens.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">Interrelationship of the seven requirements: all are of equal importance, support each other, and should be implemented and evaluated throughout the AI system’s lifecycle <span id="id4">[<a class="reference internal" href="TAILOR.html#id21">1</a>]</span>.</span><a class="headerlink" href="#fig-tairequiremens" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="operationalise-the-key-requirements">
<h5>Operationalise the key requirements<a class="headerlink" href="#operationalise-the-key-requirements" title="Permalink to this headline">¶</a></h5>
<p>Finally, Chapter III offers a more practical way to assess Trustworthy AI. In particular, the Guidelines proposed a (non-exhaustive) Trustworthy AI assessment list, based on the key requirements listed before. This list particularly applies to AI systems that directly interact with users, and is primarily addressed to developers and deployers of AI systems (whether self-developed or acquired from third parties). It is further developed in the <a href="https://futurium.ec.europa.eu/en/european-ai-alliance/pages/welcome-altai-portal" target=_blank>Assessment List for Trustworthy Artificial Intelligence (ALTAI) tool</a>.
In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI through a governance structure embracing both operational and management levels.</p>
<p id="id5"><dl class="citation">
<dt class="label" id="id20"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>High-Level Expert Group on Artificial Intelligence. Ethics guidelines for trustworthy ai. URL: <a class="reference external" href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai">https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</a> (visited on 2024-04-23).</p>
</dd>
<dt class="label" id="id10"><span class="brackets"><a class="fn-backref" href="#id3">2</a></span></dt>
<dd><p>European Parliament &amp; Council. General data protection regulation. 2016. L119, 4/5/2016, p. 1–88.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was re-adapted from the Ethics guidelines for Trustworthy AI of the High-Level Expert Group on Artificial Intelligence by Francesca Pratesi and Umberto Straccia.</p>
</div></blockquote>
</div>
</div>
</div>
<span id="document-main/Ethical_Legal_Framework/AI_ACT"></span><div class="tex2jax_ignore mathjax_ignore section" id="the-eu-ai-act">
<h3>The EU AI Act<a class="headerlink" href="#the-eu-ai-act" title="Permalink to this headline">¶</a></h3>
<div class="section" id="in-brief">
<h4>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h4>
<p>The Artificial Intelligence Act (or AI Act, AI ACT, AIA) is a European Regulation aiming at regulating artificial intelligence (AI) to ensure better conditions for the development and use of this innovative technology.</p>
</div>
<div class="section" id="more-in-detail">
<h4>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h4>
<p>On March 13, 2024, the European Parliament adopted the <em>Artificial Intelligence Act (AI Act)</em> <span id="id1">[<a class="reference internal" href="#id23">1</a>]</span>. It is considered to be the world’s first comprehensive legal framework for AI <span id="id2">[<a class="reference internal" href="#id25">2</a>]</span>. It provides for EU-wide rules on data quality, transparency, human oversight and accountability.
The first version of the text proposed in 2021 was significantly amended to take into account the technological progress in the field (for example, the development of general-purpose AI) and the instances of institutions, advocates, and associations.
One of the most significant changes follows the calls of several organizations and scholars <span id="id3">[<a class="reference internal" href="#id26">3</a>, <a class="reference internal" href="#id27">4</a>]</span> and introduces the obligation for some deployers - bodies governed by public law, or private operators providing public services, or operators deploying high-risk systems that evaluate the creditworthiness of natural persons, establish their credit score, or use AI for risk assessment and pricing in the case of life and health insurance - to carry out a <em>Fundamental Rights Impact Assessment (“FRIA”)</em>. The FRIA is required when the aforementioned deployers put into use certain high-risk systems, as listed in Annex III of the EU AI Act <span id="id4">[<a class="reference internal" href="#id23">1</a>]</span>, with the exception of the systems used for critical infrastructures. Article 27 identifies the cases in which the FRIA is mandatory and the content of the assessment, but it does not provide indications on parameters and criteria for the implementation of adequate measurement paths.</p>
<p>It is worth noting the scope of the AI Act, defined in <a href="https://artificialintelligenceact.eu/article/2/" target=_blank>Article 2 of the AI Act</a>:</p>
<div class="admonition-scope-of-the-ai-act admonition">
<p class="admonition-title">Scope of the AI Act</p>
<!--<a href="https://artificialintelligenceact.eu/article/2/" target=_blank>Article 2 of the AI Act</a> defines the scope of the Regulation:-->
<ol class="simple">
<li><p>This Regulation applies to:</p></li>
</ol>
<p>(a) providers placing on the market or putting into service AI systems or placing on the market general-purpose AI models in the Union, irrespective of whether those providers are established or located within the Union or in a third country;</p>
<p>(b) deployers of AI systems that have their place of establishment or are located within the Union;</p>
<p>(c) providers and deployers of AI systems that have their place of establishment or are located in a third country, where the output produced by the AI system is used in the Union;</p>
<p>(d) importers and distributors of AI systems;</p>
<p>(e) product manufacturers placing on the market or putting into service an AI system together with their product and under their own name or trademark;</p>
<p>(f) authorised representatives of providers, which are not established in the Union;</p>
<p>(g) affected persons that are located in the Union.</p>
<ol class="simple">
<li><p>For AI systems classified as high-risk AI systems in accordance with Article 6(1) related to products covered by the Union harmonisation legislation listed in Section B of Annex I, only Article 6(1), Articles 102 to 109 and Article 112 apply. Article 57 applies only in so far as the requirements for high-risk AI systems under this Regulation have been integrated in that Union harmonisation legislation.</p></li>
<li><p>This Regulation does not apply to areas outside the scope of Union law, and shall not, in any event, affect the competences of the Member States concerning national security, regardless of the type of entity entrusted by the Member States with carrying out tasks in relation to those competences. This Regulation does not apply to AI systems where and in so far they are placed on the market, put into service, or used with or without modification exclusively for military, defence or national security purposes, regardless of the type of entity carrying out those activities. This Regulation does not apply to AI systems which are not placed on the market or put into service in the Union, where the output is used in the Union exclusively for military, defence or national security purposes, regardless of the type of entity carrying out those activities.</p></li>
<li><p>This Regulation applies neither to public authorities in a third country nor to international organisations falling within the scope of this Regulation pursuant to paragraph 1, where those authorities or organisations use AI systems in the framework of international cooperation or agreements for law enforcement and judicial cooperation with the Union or with one or more Member States, provided that such a third country or international organisation provides adequate safeguards with respect to the protection of fundamental rights and freedoms of individuals.</p></li>
<li><p>This Regulation shall not affect the application of the provisions on the liability of providers of intermediary services as set out in Chapter II of Regulation (EU) 2022/2065.</p></li>
<li><p>This Regulation does not apply to AI systems or AI models, including their output, specifically developed and put into service for the sole purpose of scientific research and development.</p></li>
<li><p>Union law on the protection of personal data, privacy and the confidentiality of communications applies to personal data processed in connection with the rights and obligations laid down in this Regulation. This Regulation shall not affect Regulation
(EU) 2016/679 or (EU) 2018/1725, or Directive 2002/58/EC or (EU) 2016/680, without prejudice to Article 10(5) and Article 59 of this Regulation.</p></li>
<li><p>This Regulation does not apply to any research, testing or development activity regarding AI systems or AI models prior to their being placed on the market or put into service. Such activities shall be conducted in accordance with applicable Union law. Testing in real world conditions shall not be covered by that exclusion.</p></li>
<li><p>This Regulation is without prejudice to the rules laid down by other Union legal acts related to consumer protection and product safety.</p></li>
<li><p>This Regulation does not apply to obligations of deployers who are natural persons using AI systems in the course of a purely personal non-professional activity.</p></li>
<li><p>This Regulation does not preclude the Union or Member States from maintaining or introducing laws, regulations or administrative provisions which are more favourable to workers in terms of protecting their rights in respect of the use of AI systems by employers, or from encouraging or allowing the application of collective agreements which are more favourable to workers.</p></li>
<li><p>This Regulation does not apply to AI systems released under free and open-source licences, unless they are placed on the market or put into service as high-risk AI systems or as an AI system that falls under Article 5 or 50.</p></li>
</ol>
</div>
<div class="section" id="a-risk-based-approach">
<h5>A risk-based approach<a class="headerlink" href="#a-risk-based-approach" title="Permalink to this headline">¶</a></h5>
<p>However, the final text maintains the risk-based approach proposed by the Commission in 2021, with different rules for AI systems depending on the level of risk they pose. The technologies belonging to the high-risk category (i.e., the one that is mainly in the scope of the EU AI Act) are not written in the law itself but are listed in specific annexes to make the general law more durable: indeed, the general principles should be the most durable, while the individual technology can be added to the appropriate annex if needed.</p>
<div class="figure align-center" id="fig-aiact-risk-pyramid">
<a class="reference internal image-reference" href="_images/aiact_risk_pyramid.png"><img alt="_images/aiact_risk_pyramid.png" src="_images/aiact_risk_pyramid.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text">The pyramid of AI systems based on their risk</span><a class="headerlink" href="#fig-aiact-risk-pyramid" title="Permalink to this image">¶</a></p>
</div>
<p>A graphical representation of this classification is given in Figure <a class="reference internal" href="#fig-aiact-risk-pyramid"><span class="std std-numref">3</span></a>, while the four categories are summarized in the following <span id="id5">[<a class="reference internal" href="#id28">5</a>]</span>:</p>
<ul class="simple">
<li><p><a class="reference internal" href="TAILOR.html#document-main/Ethical_Legal_Framework/Prohibited_AI"><span class="doc std std-doc">Unacceptable Risk</span></a>. The technologies that create an “unacceptable risk” are completely banned.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-main/Ethical_Legal_Framework/High_Risk_AI"><span class="doc std std-doc">High-Risk</span></a>. The “high-risk” category is heavily regulated, with significant requirements for the systems and obligations for the providers, importers, distributors, and deployers of these technologies.</p></li>
<li><p><strong>Limited risk</strong>. Limited risk refers to the risks associated with a lack of transparency in AI usage. The EU AI Act introduces specific transparency obligations to ensure that humans are informed when necessary, fostering trust. For instance, when using AI systems such as chatbots, humans should be made aware that they are interacting with a machine so they can take an informed decision to continue or step back. Providers will also have to ensure that AI-generated content is identifiable. Besides, AI-generated text published with the purpose to inform the public on matters of public interest must be labelled as artificially generated. This also applies to audio and video content constituting deep fakes.</p></li>
<li><p><strong>Minimal or no risk</strong>. The EU AI Act allows the free use of minimal-risk AI. This includes applications such as AI-enabled video games or spam filters. The vast majority of AI systems currently used in the EU fall into this category.</p></li>
</ul>
</div>
<div class="section" id="the-structure-of-the-eu-ai-act">
<h5>The structure of the EU AI ACT<a class="headerlink" href="#the-structure-of-the-eu-ai-act" title="Permalink to this headline">¶</a></h5>
<p>The EU AI Act is composed of 180 Recitals, 113 Articles organized in 13 Chapters, and 13 Annexes.
Here, the main structure of the AI Act is provided, chapter by chapter, to help in its consultancy.</p>
<ul class="simple">
<li><p><em>Chapter I - General Provisions</em>. Four articles where the preliminary information (such as the scope of the Regulation) and the basic definitions (such as “AI system”, “risk”, “deployer”, “provider”, “making available on the market”, and “putting into service”) are given.</p></li>
<li><p><em>Chapter II - Prohibited AI Practices</em>. One article listing the AI practices that shall be prohibited. We reported this list <a class="reference internal" href="TAILOR.html#document-main/Ethical_Legal_Framework/Prohibited_AI"><span class="doc std std-doc">here</span></a>.</p></li>
<li><p><em>Chapter III - High-risk AI Systems</em>. The main part of the Regulation, it is composed of forty-four articles divided into five Sections:</p>
<ul>
<li><p>Classification of AI systems as high-risk (Articles 6 and 7).</p></li>
<li><p>Requirements for high-risk AI systems (Articles 8-15).</p></li>
<li><p>Obligations of providers and deployers of high-risk AI systems and other parties (Articles 16-27).</p></li>
<li><p>Notifying authorities and notified bodies (Articles 28-39).</p></li>
<li><p>Standards, conformity assessment, certificates, and registration (Articles 40-49).
We examine more in depth the high-risk systems <a class="reference internal" href="TAILOR.html#document-main/Ethical_Legal_Framework/High_Risk_AI"><span class="doc std std-doc">here</span></a>.</p></li>
</ul>
</li>
<li><p><em>Chapter IV - Transparency Obligations for Providers and Deployers of Certain AI Systems</em>. This Chapter, composed of one article only, lists all the obligations that providers and deployers of limited risk AI systems have. Indeed, such AI systems must be designed and developed in such a way that the natural persons concerned are informed that they are interacting with an AI system (e.g., chatbots, or a clear disclosure that a content has been artificially generated).</p></li>
<li><p><em>Chapter V - General-Purpose AI Models</em>. This Chapter, composed of 6 Articles, regulates General-purpose AI models, including large generative AI models, which might also be integrated into other AI systems.  A key point of this Chapter is the obligation of transparency to understand such models better and ensure that their providers respect copyright law when training their models. In addition, some of these models could pose systemic risks, because they are widely used or very capable (for now, the threshold is fixed to have a training measured in floating point operations -or FLOPS- greater than **<span class="math notranslate nohighlight">\(10^{25}\)</span>). **Providers of such models must assess and mitigate risks, report serious incidents, conduct state-of-the-art tests and model evaluations, ensure cybersecurity, and provide information on the energy consumption of their models.</p></li>
<li><p><em>Chapter VI - Measures in Support of Innovation</em>. This Chapter is composed of seven Articles and introduces the “regulatory sandboxes””, i.e., a mechanism established to foster innovation in AI, experimenting and testing in a controlled environment new products and services under a regulator’s supervision.</p></li>
<li><p><em>Chapter VII - Governance</em>. This Chapter is composed of seven Articles, dealing with the establishment of mechanisms that can help in facilitating the application of the EU AI Act, such as the European Artificial Intelligence Board Advisory forum and the Scientific panel of independent experts. Moreover, this Chapter advocates the designation of national competent authorities and single points of contact.</p></li>
<li><p><em>Chapter VIII - Eu Database For High-Risk AI Systems</em>. This Chapter is composed of one article, where the creation of a database for high-risk AI systems listed in Annex III, controlled by the EU Commission itself, where adequate technical and administrative support should be made available to providers, prospective providers and deployers. This database shall comply with the applicable accessibility requirements, e.g., being machine-readable and (unless the provider has decided to open the information to the public) accessible only to market surveillance authorities and the Commission.</p></li>
<li><p><em>Chapter IX - Post-Market Monitoring, Information Sharing and Market Surveillance</em>. This Chapter, composed of twenty-three Articles, regulates the monitoring and surveillance after an AI system is made available in the market. The Chapter mainly deals with high-risk systems and general-purpose AI models. It covers various topic such as the reporting of serious incidents, the supervision of testing in real world conditions by market surveillance authorities, the powers of national authorities protecting fundamental rights, and the possible remedies (e.g., the right to lodge a complaint with a market surveillance authority and the right to an <a class="reference internal" href="TAILOR.html#document-Transparency/Transparency"><span class="doc std std-doc">explanation of individual decision-making</span></a>.</p></li>
</ul>
<!-- right to explaination (Art 86)-->
<ul class="simple">
<li><p><em>Chapter X - Codes of Conduct and Guidelines</em>. This Chapter, composed of two Articles, describes the contents of codes of conduct for voluntary application of specific requirements, and the commitment of the Commission to publish suitable guidelines on the implementation of the EU AI Act.</p></li>
<li><p><em>Chapter XI - Delegation of Power and Committee Procedure</em>. This Chapter, composed of two Articles, illustrates the possibility of the exercise of the delegation, and advocates that the Commission shall be assisted by a specific committee.</p></li>
<li><p><em>Chapter XII - Penalties</em>. This Chapter, composed of three Articles, describes the fines and penalties, clarifying that the penalties shall be effective, dissuasive, and proportionate (especially for SMEs, they should take into account their economic viability).</p></li>
<li><p><em>Chapter XIII - Final Provisions</em>. This Chapter, composed of twelve Articles, mostly listing amendments to previous Regulations but also explaining what happens to (general-purpose) AI systems already placed on the market or put into service.</p></li>
</ul>
<blockquote>
<div><p>This entry was re-adapted from the Artificial Intelligence Act by Francesca Pratesi and Umberto Straccia.</p>
</div></blockquote>
<p id="id6"><dl class="citation">
<dt class="label" id="id23"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id4">2</a>)</span></dt>
<dd><p>Artificial Intelligence Act, European Parliament legislative resolution of 13 March 2024 on the proposal for a regulation of the European Parliament and of the Council on laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union Legislative Acts (COM(2021)0206 – C9-0146/2021 – 2021/0106(COD)), P9_TA(2024)0138. 2024. URL: <a class="reference external" href="https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.pdf">https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.pdf</a> (visited on 2024-04-23).</p>
</dd>
<dt class="label" id="id25"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>EU AI Act: first regulation on artificial intelligence. 2024. URL: <a class="reference external" href="https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence">https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence</a> (visited on 2024-04-23).</p>
</dd>
<dt class="label" id="id26"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Urgent appeal to approve a solid Fundamental Rights Impact Assessment in the EU Artificial Intelligence Act. 2023. URL: <a class="reference external" href="https://brusselsprivacyhub.com/2023/09/12/brussels-privacy-hub-and-other-academic-institutions-ask-to-approve-a-fundamental-rights-impact-assessment-in-the-eu-artificial-intelligence-act/">https://brusselsprivacyhub.com/2023/09/12/brussels-privacy-hub-and-other-academic-institutions-ask-to-approve-a-fundamental-rights-impact-assessment-in-the-eu-artificial-intelligence-act/</a> (visited on 2024-04-23).</p>
</dd>
<dt class="label" id="id27"><span class="brackets"><a class="fn-backref" href="#id3">4</a></span></dt>
<dd><p>The AI Act Must Protect the Rule of Law. 2023. URL: <a class="reference external" href="https://dq4n3btxmr8c9.cloudfront.net/files/iytbh9/AI_and_RoL_Open_Letter_final_27092023.pdf">https://dq4n3btxmr8c9.cloudfront.net/files/iytbh9/AI_and_RoL_Open_Letter_final_27092023.pdf</a> (visited on 2024-04-23).</p>
</dd>
<dt class="label" id="id28"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>Shaping Europe’s digital future. 2024. URL: <a class="reference external" href="https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai">https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai</a> (visited on 2024-04-23).</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was re-adapted from the Ethics guidelines for Trustworthy AI of the High-Level Expert Group on Artificial Intelligence by Francesca Pratesi and Umberto Straccia.</p>
</div></blockquote>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-main/Ethical_Legal_Framework/Prohibited_AI"></span><div class="tex2jax_ignore mathjax_ignore section" id="prohibited-ai-practices">
<h4>Prohibited AI Practices<a class="headerlink" href="#prohibited-ai-practices" title="Permalink to this headline">¶</a></h4>
<!--

Add cross-references

- (par:real-time)=*"real-time" remote biometric identification systems in publicly accessible spaces* for the purposes of law enforcement, unless and in so far as such use is strictly necessary for one of the following objectives:

(see point {ref}`par:real-time`)
 -->
<p>Article 5 of the EU AI Act <span id="id1">[<a class="reference internal" href="TAILOR.html#id23">1</a>]</span> lists all the <em>AI systems that are not allowed</em> in Europe:</p>
<ul class="simple">
<li><p><em>subliminal techniques or purposefully manipulative or deceptive techniques</em>, with the objective or the effect of materially distorting the behavior of a person or a group of persons by appreciably impairing their ability to make an informed decision;</p></li>
<li><p>systems that <em>exploit any of the vulnerabilities</em> of a natural person or a specific group of persons due to their age, disability, or a specific social or economic situation, with the objective or the effect of materially distorting the behavior of that person(s) in a manner that (likely) causes significant harm;</p></li>
<li><p>systems for the <em>evaluation or classification of natural persons based on their social behavior</em> or known, inferred or predicted personal or personality characteristics, with the social score leading to either or both of the following:</p>
<ul>
<li><p>detrimental or unfavorable treatment in social contexts that are unrelated to the contexts in which the data was originally generated or collected;</p></li>
<li><p>detrimental or unfavourable treatment that is unjustified or disproportionate to their social behaviour or its gravity;</p></li>
</ul>
</li>
<li><p>systems for making risk assessments of natural persons in order to <em>assess or predict the risk of a natural person committing a criminal offence</em>, based solely on the profiling of a natural person or on assessing their personality traits and characteristics, unless these systems are used only to support the human assessment;</p></li>
<li><p>systems that create or expand <em>facial recognition databases</em> through the untargeted scraping of facial images from the internet or CCTV footage;</p></li>
<li><p>systems to <em>infer emotions</em> of a natural person in the areas of workplace and education institutions, unless these systems are used for medical or safety reasons;</p></li>
<li><p><em>biometric categorisation systems</em> that categorize individually natural persons based on their biometric data to deduce or infer their race, political opinions, trade union membership, religious or philosophical beliefs, sex life or sexual orientation, except lawfully acquired biometric datasets in the area of law enforcement;</p></li>
<li><p><em>“real-time” remote biometric identification systems in publicly accessible spaces</em> for the purposes of law enforcement, unless and in so far as such use is strictly necessary for one of the following objectives:</p>
<ul>
<li><p>the targeted search for specific victims of abduction, trafficking in human beings or sexual exploitation of human beings, as well as the search for missing persons;</p></li>
<li><p>the prevention of a specific, substantial, and imminent threat to the life or physical safety of natural persons or a genuine and present or genuine and foreseeable threat of a terrorist attack;</p></li>
<li><p>the localization or identification of a person suspected of having committed a criminal offence, for the purpose of conducting a criminal investigation or prosecution or executing a criminal penalty for offences referred to in Annex II of the EU AI Act and punishable in the Member State concerned by a custodial sentence or a detention order for a maximum period of at least four years.</p></li>
</ul>
</li>
</ul>
<p>In particular, the use of real-time biometric identification systems is investigated in Article 5 of the EU AI Act, where it is also specified that a real-time biometric identification system:</p>
<ul class="simple">
<li><p>shall be deployed for the purposes set out in that point only to confirm the identity of the specifically targeted individual, and it shall take into account the following elements:</p>
<ul>
<li><p>the nature of the situation giving rise to the possible use, in particular the seriousness, probability, and scale of the harm that would be caused if the system were not used;</p></li>
<li><p>the consequences of the use of the system for the rights and freedoms of all persons concerned, in particular the seriousness, the probability, and the scale of those consequences.</p></li>
</ul>
</li>
<li><p>shall comply with necessary and proportionate safeguards and conditions in relation to the use in accordance with the national law authorizing the use thereof, in particular as regards the temporal, geographic, and personal limitations.</p></li>
</ul>
<p>As a general norm, the use of such systems shall be authorized only if the law enforcement authority
has completed a fundamental rights impact assessment. The EU AI Act envisages some derogation in justified cases of urgency, provided that such authorization is requested within 24 hours. The authorization shall be requested to a competent judicial authority or an independent administrative authority whose decision is binding.</p>
<p>In any case, the use of the “real-time” remote biometric identification systems must be <em>necessary for, and proportionate to, achieving one of the objectives specified above</em> (see last point of the first list in this entry) and remains limited to what is strictly necessary concerning the period of time as well as the geographic and personal scope.</p>
<blockquote>
<div><p>This entry was re-adapted from the Artificial Intelligence Act by Francesca Pratesi and Umberto Straccia.</p>
</div></blockquote>
</div>
<span id="document-main/Ethical_Legal_Framework/High_Risk_AI"></span><div class="tex2jax_ignore mathjax_ignore section" id="high-risk-ai-systems">
<h4>High Risk AI Systems<a class="headerlink" href="#high-risk-ai-systems" title="Permalink to this headline">¶</a></h4>
<p>For an AI system to be classified into the “high-risk” category, there are two different criteria to be considered:</p>
<ul class="simple">
<li><p>an AI system is considered “high-risk” if is intended to be used as a safety component of a product or is itself a product covered by the Union harmonization legislation listed in Annex II of the EU AI Act <span id="id1">[<a class="reference internal" href="TAILOR.html#id23">1</a>]</span> (which includes, for example, the Directive on the safety of toys, the Regulations on medical devices, the Regulation on personal protective equipment, …), and the product is required to undergo a third-party conformity assessment pursuant to the legislation listed in Annex II;</p></li>
<li><p>even if the first criterion does not apply, an AI system is considered ``high-risk’’ if it is included in the systems referred to in Annex III of the EU AI Act <span id="id2">[<a class="reference internal" href="TAILOR.html#id23">1</a>]</span>. This criterion does not apply to systems that do not pose significant risk of harm to the health, safety, or fundamental rights of natural persons according to the criteria listed in Article 6 of the EU AI Act <span id="id3">[<a class="reference internal" href="TAILOR.html#id23">1</a>]</span>, unless the system performs profiling of natural persons.</p></li>
</ul>
<p>Thus, it is important to look into the Annex III of the EU AI Act <span id="id4">[<a class="reference internal" href="TAILOR.html#id23">1</a>]</span>, where high-risk systems are listed. In particular, high-risk systems are systems used in the following areas:</p>
<!--%High-risk systems listed in Annex III of the AI Act are some systems used in the following areas:-->
<ul class="simple">
<li><p><em>Biometrics</em>, for example, remote biometric identification systems and emotion recognition AI.</p></li>
<li><p><em>Critical infrastructure</em>, for example, AI systems used as safety component of a water supply infrastructure or for road traffic.</p></li>
<li><p><em>Education and vocational training</em>, for example, systems used to determine access to an educational institution, proctoring systems, AI used to evaluate learning outcomes.</p></li>
<li><p><em>Employment, workers management and access to self-employment</em>, for example, systems used for recruitment or to make decisions related to promotions.</p></li>
<li><p><em>Access to and enjoyment of essential private services and essential public services and benefits</em>, for example, systems used to evaluate the eligibility of natural persons for healthcare services and systems used to establish a person’s credit score.</p></li>
<li><p><em>Law enforcement</em>, for example, polygraphs and systems used by authorities to evaluate the reliability of evidence during an investigation.</p></li>
<li><p><em>Migration, asylum, and border control management</em>, for example, systems used by authorities to assess the risk of irregular migration posed by a natural person who intends to enter into the territory of a Member State.</p></li>
<li><p><em>Administration of justice and democratic processes</em>, for example, systems used to assist a judicial authority in applying the law to a concrete set of facts, systems used to influence the outcome of an election.</p></li>
</ul>
<blockquote>
<div><p>This entry was re-adapted from the Artificial Intelligence Act by Francesca Pratesi and Umberto Straccia.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
</div>
<span id="document-main/Trustworthy_AI"></span><div class="tex2jax_ignore mathjax_ignore section" id="trustworthy-ai">
<h2>Trustworthy AI<a class="headerlink" href="#trustworthy-ai" title="Permalink to this headline">¶</a></h2>
<!-- TODO 

point the link
[risk-based approach](./Ethical_Legal_Framework/AI_ACT.md}
to the correct section of the entry


-->
<p>In the last years, several definition on what Trustworthy AI is are given by the scientific communities. Each definition can vary on the dimension where they focus more, but there is a general consensus in Europe to use as official definition the one given by the High Level Expert Group on AI in its Ethical Guidelines for Trustworthy AI <span id="id1">[<a class="reference internal" href="TAILOR.html#id22">1</a>]</span>.</p>
<p>Indeed, according to this document, Trustworthy AI has three components, which should be met throughout the system’s entire life cycle. Indeed, it should be:</p>
<ol class="simple">
<li><p><strong>lawful</strong>, complying with all applicable laws and regulations;</p></li>
<li><p><strong>ethical</strong>, ensuring adherence to ethical principles and values;</p></li>
<li><p><strong>robust</strong>, both from a technical and social perspective since, even with good intentions, AI systems can cause unintentional harm.</p></li>
</ol>
<p>Each component in itself is necessary but not sufficient for the achievement of Trustworthy AI.  <!--Ideally, all three components work in harmony and overlap in their operation. If, in practice, tensions arise between these components, society should endeavour to align them.--><br>
We will provide some additional details on what it is intended for each of these three components.</p>
<div class="section" id="lawful-ai">
<h3>Lawful AI<a class="headerlink" href="#lawful-ai" title="Permalink to this headline">¶</a></h3>
<p>AI systems do not operate in a lawless world. A number of legally binding rules at European, national, and international levels already apply or are relevant to the development, deployment, and use of AI systems today.
Legal sources include, but are not limited to: EU primary law (the <a href="https://european-union.europa.eu/principles-countries-history/principles-and-values/founding-agreements_en" target=_blank>Treaties of the European Union</a> and its <a href="https://www.europarl.europa.eu/charter/pdf/text_en.pdf" target=_blank>Charter of Fundamental Rights</a>), EU secondary law (such as the <a href="https://gdpr-info.eu/" target=_blank>General Data Protection Regulation (GDPR)</a> <span id="id2">[<a class="reference internal" href="TAILOR.html#id8">1</a>]</span>, the <a href="https://single-market-economy.ec.europa.eu/single-market/goods/free-movement-sectors/liability-defective-products_en" target=_blank>Product Liability Directive</a>, the <a href="https://digital-strategy.ec.europa.eu/en/policies/non-personal-data" target=_blank>Regulation on the Free Flow of Non-Personal Data</a>, <a href="https://commission.europa.eu/aid-development-cooperation-fundamental-rights/your-rights-eu/know-your-rights/equality/non-discrimination_en" target=_blank>anti-discrimination Directives</a>, consumer law and <a href="https://osha.europa.eu/en/safety-and-health-legislation/european-directives" target=_blank>Safety and Health at Work Directives</a>, the <a href="https://www.un.org/en/about-us/universal-declaration-of-human-rights" target=_blank>UN Human Rights treaties</a> and the Council of Europe conventions (such as the <a href="https://www.coe.int/en/web/human-rights-convention/the-convention-in-1950" target=_blank>European Convention on Human Rights</a>), and numerous EU Member State laws. Besides horizontally applicable rules, various domain-specific rules exist that apply to particular AI applications (such as, for instance, the Medical Device Regulation in the healthcare sector).</p>
<p>The law provides both positive and negative obligations, which means that it should not only be interpreted with reference to what cannot be done, but also with reference to what should be done and what may be done. The law not only prohibits certain actions but also enables others. In this regard, it can be noted that the EU Charter contains articles on the ‘’freedom to conduct a business’’ and the ‘’freedom of the arts and sciences’’, alongside articles addressing areas that we are more familiar with when looking to ensure AI’s trustworthiness, such as for instance data protection and non-discrimination.</p>
</div>
<div class="section" id="ethical-ai">
<h3>Ethical AI<a class="headerlink" href="#ethical-ai" title="Permalink to this headline">¶</a></h3>
<p>Achieving Trustworthy AI requires not only compliance with the law, which is but one of its three components. Laws are not always up to speed with technological developments, can at times be out of step with ethical norms or may simply not be well suited to addressing certain issues. For AI systems to be trustworthy, they should hence also be ethical, ensuring alignment with ethical norms.
Regarding ethics, some philosophical currents (such as Floridi’s work <span id="id3">[<a class="reference internal" href="#id25">4</a>]</span>) have been discriminating between hard and soft ethics.</p>
<p><strong>Hard ethics</strong> is what we usually have in mind when discussing values, rights, duties, and responsibilities— or, more broadly, what is morally right or wrong, and what ought or ought not to be done—in the course of formulating new regulations or challenging existing ones. In short, hard ethics is what makes or shapes the law. %Thus, in the best scenario, lobbying in favour of some good legislation or to improve that which already exists can be a case of hard ethics. For example, hard ethics helped to dismantle apartheid in South Africa and supported the approval of legislation in Iceland that requires public and private businesses to prove that they offer equal pay to employees, irrespective of their gender (the gender pay gap continues to be a scandal in most countries).</p>
<p><strong>Soft ethics</strong> covers the same normative ground as hard ethics, but it does so by considering what ought and ought not to be done over and above the existing regulation, not against it, or despite its scope, or to change it, or to by-pass it (e.g. in terms of self-regulation). In other words, soft ethics is post-compliance ethics: in this case, ‘’ought implies may’’.</p>
</div>
<div class="section" id="robust-ai">
<h3>Robust AI<a class="headerlink" href="#robust-ai" title="Permalink to this headline">¶</a></h3>
<p>Even if an ethical purpose is ensured, individuals and society must also be confident that AI systems will not cause any unintentional harm. Such systems should perform in a safe, secure and reliable manner, and safeguards should be foreseen to prevent any unintended adverse impacts. It is therefore important to ensure that AI systems are robust. This is needed both from a technical perspective (ensuring the system’s technical robustness as appropriate in a given context, such as the application domain or life cycle phase), and from a social perspective (in due consideration of the context and environment in which the system operates).</p>
<!--https://www.cohubicol.com/blog/robust-ai-and-robust-law-part-i-robust-ai/-->
</div>
<div class="section" id="the-european-legal-framework">
<h3>The European Legal Framework<a class="headerlink" href="#the-european-legal-framework" title="Permalink to this headline">¶</a></h3>
<p>In these pages, we focus on the European framework only, mostly relying on two main sources, which are described below and in the linked pages.</p>
<p>The first document, at least chronologically speaking, we refer to is the <span class="xref myst">Ethical Guidelines for Trustworthy AI</span> <span id="id4">[<a class="reference internal" href="TAILOR.html#id22">1</a>]</span>, which, as the name itself suggests is not a law or a legal obligation. Nevertheless, it is commonly recognized as the most relevant document in the field of Trustworthy AI.
Here, as we already mentioned, there are listed a definition of Trustworthy AI, the foundation of Trustworthy AI, the seven key requirements that AI systems should implement and meet throughout their entire life cycle, and a concrete assessment list to operationalize the requirements.</p>
<p>Then, the other fundamental source is the world’s first comprehensive law on Artificial Intelligence (AI): the <span class="xref myst">EU AI Act</span> <span id="id5">[<a class="reference internal" href="TAILOR.html#id23">1</a>]</span>. The text provides a classification of AI systems using a <a class="reference internal" href="TAILOR.html#document-main/Ethical_Legal_Framework/AI_ACT"><span class="doc std std-doc">risk-based approach</span></a>; four levels of risk were identified, and different obligations are listed for the different categories of AI systems to be compliant with this law.</p>
</div>
<div class="section" id="the-tailor-handbook-of-trustworthy-ai">
<h3>The TAILOR Handbook of Trustworthy AI<a class="headerlink" href="#the-tailor-handbook-of-trustworthy-ai" title="Permalink to this headline">¶</a></h3>
<p>In order to provide a better understanding on Trustworthy AI, within the <a class="reference internal" href="TAILOR.html#document-TAILOR_project"><span class="doc std std-doc">TAILOR project</span></a> we have been developing an <a class="reference internal" href="TAILOR.html#document-TAILOR"><span class="doc std std-doc">Handbook on Trustworthy AI</span></a>. Here, you can find an overview of all the ethical dimensions highlighted in the <span class="xref myst">Ethical Guidelines for Trustworthy AI</span> <span id="id6">[<a class="reference internal" href="TAILOR.html#id22">1</a>]</span> and a deeper analysis of concepts that are related to and are going to contribute to each dimension.</p>
</div>
<div class="section" id="bibliography">
<h3>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h3>
<p id="id7"><dl class="citation">
<dt class="label" id="id22"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id4">2</a>,<a href="#id6">3</a>)</span></dt>
<dd><p>High-Level Expert Group on Artificial Intelligence. Ethics guidelines for trustworthy ai. URL: <a class="reference external" href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai">https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</a> (visited on 2024-04-23).</p>
</dd>
<dt class="label" id="id24"><span class="brackets"><a class="fn-backref" href="#id5">2</a></span></dt>
<dd><p>Artificial Intelligence Act, European Parliament legislative resolution of 13 March 2024 on the proposal for a regulation of the European Parliament and of the Council on laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union Legislative Acts (COM(2021)0206 – C9-0146/2021 – 2021/0106(COD)), P9_TA(2024)0138. 2024. URL: <a class="reference external" href="https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.pdf">https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.pdf</a> (visited on 2024-04-23).</p>
</dd>
<dt class="label" id="id12"><span class="brackets"><a class="fn-backref" href="#id2">3</a></span></dt>
<dd><p>European Parliament &amp; Council. General data protection regulation. 2016. L119, 4/5/2016, p. 1–88.</p>
</dd>
<dt class="label" id="id25"><span class="brackets"><a class="fn-backref" href="#id3">4</a></span></dt>
<dd><p>Luciano Floridi. Soft ethics and the governance of the digital. <em>Philosophy and Technology</em>, 2018. <a class="reference external" href="https://doi.org/https://doi.org/10.1007/s13347-018-0303-9">doi:https://doi.org/10.1007/s13347-018-0303-9</a>.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was re-adapted from the Ethics guidelines for Trustworthy AI of the High-Level Expert Group on Artificial Intelligence by Francesca Pratesi and Umberto Straccia.</p>
</div></blockquote>
</div>
<div class="toctree-wrapper compound">
<span id="document-Human_Agency_and_Oversight/Human_Agency_and_Oversight"></span><div class="tex2jax_ignore mathjax_ignore section" id="human-agency-and-oversight">
<h3>Human Agency and Oversight<a class="headerlink" href="#human-agency-and-oversight" title="Permalink to this headline">¶</a></h3>
<div class="section" id="in-brief">
<h4>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h4>
<p>AI systems should support human autonomy and decision-making, as prescribed by the principle of respect for human autonomy. This requires that AI systems should both act as enablers to a democratic, flourishing and equitable society by supporting the user’s agency and foster fundamental rights, and allow for human oversight <span id="id1">[<a class="reference internal" href="TAILOR.html#id21">1</a>]</span>.</p>
</div>
<div class="section" id="abstract">
<h4>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">¶</a></h4>
<p>We first provide motivations and background on Human Agency and Human oversight, explaining the various shades of this ethical dimension, such as the *<em>”human-in-the-loop”</em> paradigm. Next, we summarize the guidelines and draft standards for human agency, human control, and human oversight, and the software frameworks supporting the dimension.</p>
</div>
<div class="section" id="motivations-and-background">
<h4>Motivations and background<a class="headerlink" href="#motivations-and-background" title="Permalink to this headline">¶</a></h4>
<p>AI Actors should take necessary measures to preserve the autonomy and free will of human in the process of decision making, their right to choose, as well as preserve human intellectual abilities in general as an intrinsic value and a system forming factor of modern civilization. Also, AI Actors should forecast possible negative consequences for the development of human cognitive abilities at the earliest stages of AI systems creation and refrain from the development of AI systems that purposefully cause such consequences <span id="id2">[<a class="reference internal" href="#id26">1</a>]</span>.</p>
<p>According to the Ethical Guidelines of the High Level Expert Group on AI <span id="id3">[<a class="reference internal" href="TAILOR.html#id21">1</a>]</span>, there are three different aspects to be considered when we talk about this ethical dimension. We will analyze them one by one.</p>
<p><strong>Fundamental Rights.</strong> Like many technologies, AI systems can enable and hamper fundamental rights. They might benefit people for instance by helping them track their personal data, or by increasing the accessibility to education, hence supporting their right to education. However, given the reach, capacity, and opacity of many AI systems, they can also negatively affect fundamental rights. In situations where such risks exist, a fundamental rights impact assessment should be undertaken. This should be done prior to the system’s development and include an evaluation of whether those risks can be reduced or justified as necessary in a democratic society in order to respect the rights and freedoms of others. Design approaches such as value sensitive and participatory design can support bridging the gap between societal context and perspective from stakeholders who are impact by a given technology with technical design requirements <span id="id4">[<a class="reference internal" href="#id38">2</a>]</span>. Moreover, mechanisms should be put into place to receive external feedback regarding AI systems that potentially infringe on fundamental rights.</p>
<p><strong>Human agency.</strong> Users should be able to make informed autonomous decisions regarding AI systems. They should be given the knowledge and tools to comprehend and interact with AI systems to a satisfactory degree and, where possible, be enabled to reasonably self-assess or challenge the system. AI systems should support individuals in making better, more informed choices in accordance with their goals. These requirements have been studied in different forms under the concepts of <a class="reference internal" href="TAILOR.html#document-Human_Agency_and_Oversight/Meaningful_human_control"><span class="doc">Meaningful human control</span></a> over AI systems <span id="id5">[<a class="reference internal" href="#id37">3</a>]</span>, contestable AI \cite{alfrink2022contestable}, and reflection machines <span id="id6">[<a class="reference internal" href="#id35">4</a>]</span>.
AI systems can sometimes be deployed to shape and influence human behaviour through mechanisms that may be difficult to detect, since they may harness sub-conscious processes, including various forms of unfair manipulation, deception, herding and conditioning, all of which may threaten individual autonomy. Besides technical aspects, institutional and societal considerations related to the context and stakeholders interacting with the AI system must be accounted for in order to get the full picture of potential effects of AI systems on human autonomy <span id="id7">[<a class="reference internal" href="#id34">5</a>]</span>. To achieve trustworthy AI, the overall principle of user autonomy must be central to the system’s functionality. Key to this is the right not to be subject to a decision based solely on automated processing when this produces legal effects on users or similarly significantly affects them.</p>
<p><strong>Human oversight.</strong> Human oversight helps ensuring that an AI system does not undermine human autonomy or causes other adverse effects. Oversight may be achieved through governance mechanisms such as a <em>“human-in-the-loop” (HITL)</em>, <em>“human-on-the-loop” (HOTL)</em>, or <em>“human-in-command” (HIC)</em> approach. HITL refers to the capability for human intervention in every decision cycle of the system, which in many cases is neither possible nor desirable.
HOTL refers to the capability for human intervention during the design cycle of the system and monitoring the system’s operation. HIC refers to the capability to oversee the overall activity of the AI system (including its broader economic, societal, legal and ethical impact) and the ability to decide when and how to use the system in any particular situation. This can include the decision not to use an AI system in a particular situation, to establish levels of human discretion during the use of the system, or to ensure the ability to override a decision made by a system.
Moreover, it must be ensured that public enforcers have the ability to exercise oversight in line with their mandate.
Oversight mechanisms can be required in varying degrees to support other safety and control measures, depending on the AI system’s application area and potential risk.
All other things being equal, the less oversight a human can exercise over an AI system, the more extensive testing and stricter governance is required.</p>
</div>
<div class="section" id="standards-and-guidelines">
<h4>Standards and guidelines<a class="headerlink" href="#standards-and-guidelines" title="Permalink to this headline">¶</a></h4>
<p>Many AI practitioners already have existing assessment tools and software development processes in place to ensure compliance also with non-legal standards.
In addidion, the European Commission has emphasised the importance of adopting Artificial Intelligence (AI) systems with a human-centric approach to ensure their safe deployment. This human-centric approach requires implementing AI systems safely and reliably to benefit humanity, with the aim of protecting human rights and dignity by keeping a “human-in-the-loop”.</p>
<p>One of the most famous tool for the self-assessment of AI systems is the <a href="https://futurium.ec.europa.eu/en/european-ai-alliance/pages/welcome-altai-portal" target=_blank>Assessment List for Trustworthy Artificial Intelligence (ALTAI) tool</a>, where 19 questions are posed regarding the human agency and the human oversight, dealing with the effect of AI systems that are aimed at guiding, influencing or supporting humans in decision making processes, with the effect on human perception and expectation when confronted with AI systems that “act” like humans, and with the effect of AI systems on human affection, trust and (in)dependence.
In the document, it explicitly stated that the assessment list should not necessarily be carried out as a stand-alone exercise, but can be incorporated into such existing practices.</p>
<p>Other important standards are the ISO norms, in particular the <a href="https://www.iso.org/standard/83012.html" target=_blank>ISO/IEC TS 8200</a>, which norms the controllability of automated artificial intelligence systems, and the <a href="https://www.iso.org/standard/86902.html" target=_blank>ISO/IEC AWI 42105</a>, which will be specifically published for providing guidance on human control and monitoring of AI systems (i.e., human oversight).</p>
<p>Last but not least, the <a href="https://artificialintelligenceact.com/title-iii/chapter-2/article-14/" target=_blank>Article 14 of the AI Act</a> is related to the human oversight, with the aim at preventing or minimising the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse.
However, an unforeseen consequence pointed out in some critical articles <span id="id8">[<a class="reference internal" href="#id39">6</a>]</span> is that <a href="https://artificialintelligenceact.com/title-iii/chapter-2/article-14/" target=_blank>Article 14 (1) of the AI Act</a> could create a legal loophole to justify the shifting of responsibilities and accountabilities from one party (users of AI systems working as human overseers) to another (designers of AI systems). This ambiguity could create legal challenges as human overseers could argue that <a href="https://artificialintelligenceact.com/title-iii/chapter-2/article-14/" target=_blank>Article 14 of the AI ACT</a> is not intended to regulate them. Moreover, authors of \cite{holistic} highlight how Article 14 of the EU AI Act provides little detail on the human overseers’ responsibilities. This lack of clear guidelines on the responsibility of human overseers or what constitutes meaningful human oversight under the proposal arguably undermines a human-centric approach.</p>
</div>
<div class="section" id="software-frameworks-supporting-dimension">
<h4>Software frameworks supporting dimension<a class="headerlink" href="#software-frameworks-supporting-dimension" title="Permalink to this headline">¶</a></h4>
<p>To the best of our knowledge, there are no software framework supporting human autonomy and oversight.
However, beside the aforementioned guidelines, there are other guidelines and recommendations, both from the academia and the private sector, such as the ones provided by:</p>
<ul class="simple">
<li><p><a href="https://digitalfuturesociety.com/app/uploads/2022/11/Towards_meaningful_oversight_of_automated_decision_making_systems.pdf" target=_blank>Digital Future Society</a>.
In this report, authors first put the human oversight dimension in context, defining different typologies of human-algorithm interaction, then reported some case studies to explain better the problem, and finally listed some policy recommendations, like defining the minimum human involvement, being aware of automation context-dependency, and adequately training developers and operators.</p></li>
<li><p><a href="https://www.ibm.com/blog/best-practices-for-augmenting-human-intelligence-with-ai/" target=_blank>IBM</a>. In this report, author reported the major standards on
human oversight, a great variety of case studies in which AI systems can be applied and the ethical risks (e.g., overtrust) that can occur, and a list of best practices to be applied to mitigate such risks.</p></li>
<li><p><a href="https://datacraft-paris.github.io/trustworthyai/human.html" target=_blank>Danone, Datacraft</a>, where authors defined some guidelines depending on the phase (specifically, conception or use) of the AI system lifecyle.</p></li>
</ul>
</div>
<div class="section" id="main-keywords">
<h4>Main Keywords<a class="headerlink" href="#main-keywords" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><a class="reference internal" href="TAILOR.html#document-Human_Agency_and_Oversight/Meaningful_human_control"><span class="doc">Meaningful human control</span></a>: <strong>Meaningful human control</strong> is the notion that aims to generalize the traditional concept of operational control over technological artifacts to artificial intelligent systems. It implies that artificial systems should not make morally consequential decisions on their own, without appropriate control from responsible humans.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Human_Agency_and_Oversight/Causal_responsibility"><span class="doc">Causal Responsibility</span></a>: <strong>Causal responsibility</strong> is the notion of responsibility that is concerned with actual causation <span id="id9">[<a class="reference internal" href="#id46">7</a>, <a class="reference internal" href="#id47">8</a>, <a class="reference internal" href="#id48">9</a>]</span>.</p></li>
</ul>
</div>
<div class="section" id="bibliography">
<h4>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h4>
<p id="id10"><dl class="citation">
<dt class="label" id="id26"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>International Research Center for AI Ethics and n Chinese Academy of Sciences Governance, Institute of Automatio. Linking artificial intelligence principles (laip). URL: <a class="reference external" href="https://www.linking-ai-principles.org/term/174">https://www.linking-ai-principles.org/term/174</a> (visited on 2024-04-23).</p>
</dd>
<dt class="label" id="id38"><span class="brackets"><a class="fn-backref" href="#id4">2</a></span></dt>
<dd><p>Evgeni Aizenberg and Jeroen Van Den Hoven. Designing for human rights in ai. <em>Big Data &amp; Society</em>, 7(2):2053951720949566, 2020.</p>
</dd>
<dt class="label" id="id37"><span class="brackets"><a class="fn-backref" href="#id5">3</a></span></dt>
<dd><p>Luciano Cavalcante Siebert, Maria Luce Lupetti, Evgeni Aizenberg, Niek Beckers, Arkady Zgonnikov, Herman Veluwenkamp, David Abbink, Elisa Giaccardi, Geert-Jan Houben, Catholijn M Jonker, and others. Meaningful human control: actionable properties for ai system development. <em>AI and Ethics</em>, 3(1):241–255, 2023.</p>
</dd>
<dt class="label" id="id35"><span class="brackets"><a class="fn-backref" href="#id6">4</a></span></dt>
<dd><p>NAJ Cornelissen, RJM van Eerdt, HK Schraffenberger, and Willem FG Haselager. Reflection machines: increasing meaningful human control over decision support systems. <em>Ethics and Information Technology</em>, 24(2):19, 2022.</p>
</dd>
<dt class="label" id="id34"><span class="brackets"><a class="fn-backref" href="#id7">5</a></span></dt>
<dd><p>Arto Laitinen and Otto Sahlgren. Ai systems and respect for human autonomy. <em>Frontiers in artificial intelligence</em>, 4:151, 2021.</p>
</dd>
<dt class="label" id="id39"><span class="brackets"><a class="fn-backref" href="#id8">6</a></span></dt>
<dd><p>Holistic AI. Key issues: human oversight. 2024. Last Accessed: 2024-05-20. URL: <a class="reference external" href="https://www.euaiact.com/key-issue/4">https://www.euaiact.com/key-issue/4</a>.</p>
</dd>
<dt class="label" id="id46"><span class="brackets"><a class="fn-backref" href="#id9">7</a></span></dt>
<dd><p>Nicole A. Vincent. A Structured taxonomy of responsibility concepts. In Nicole A. Vincent, Ibo van de Poel, and Jeroen van den Hoven, editors, <em>Moral Responsibility</em>, Library of Ethics and Applied Philosophy, pages 15–35. Springer, Springer Nature, United States, 2011. <a class="reference external" href="https://doi.org/10.1007/978-94-007-1878-4_2">doi:10.1007/978-94-007-1878-4_2</a>.</p>
</dd>
<dt class="label" id="id47"><span class="brackets"><a class="fn-backref" href="#id9">8</a></span></dt>
<dd><p>Hana Chockler and Joseph Y. Halpern. Responsibility and Blame: A Structural-Model Approach. <em>jair</em>, 22:93–115, October 2004. <a class="reference external" href="https://doi.org/10.1613/jair.1391">doi:10.1613/jair.1391</a>.</p>
</dd>
<dt class="label" id="id48"><span class="brackets"><a class="fn-backref" href="#id9">9</a></span></dt>
<dd><p>Florian Engl. A Theory of Causal Responsibility Attribution. <em>SSRN Journal</em>, 2018. <a class="reference external" href="https://doi.org/10.2139/ssrn.2932769">doi:10.2139/ssrn.2932769</a>.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was re-adapted from the Ethics guidelines for Trustworthy AI of the High-Level Expert Group on Artificial Intelligence by Francesca Pratesi and Luciano C Siebert.</p>
</div></blockquote>
</div>
<div class="toctree-wrapper compound">
<span id="document-Human_Agency_and_Oversight/Meaningful_human_control"></span><div class="tex2jax_ignore mathjax_ignore section" id="meaningful-human-control">
<h4>Meaningful human control<a class="headerlink" href="#meaningful-human-control" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p><strong>Meaningful human control</strong> is the notion that aims to generalize the traditional concept of operational control over technological artifacts to artificial intelligent systems. It implies that artificial systems should not make morally consequential decisions on their own, without appropriate control from responsible humans.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>The notion of meaningful human control has its origins in the discussions on lethal autonomous weapon systems (LAWS), specifically in regards to life-or-death decisions that such systems could in principle make. Avoiding ethical issues related to autonomous decision making by artificial agents requires that humans, and only humans, have control of and are accountable for the use of lethal force <span id="id1">[<a class="reference internal" href="#id2703">1</a>]</span>. The concrete implications of this requirement are still debated, with proposals range from calls for a full ban of LAWS  <span id="id2">[<a class="reference internal" href="#id2704">2</a>]</span> to suggestions on governance, implementation, and use of such systems that can contribute to meaningful human control  (e.g.  <span id="id3">[<a class="reference internal" href="#id2705">3</a>]</span>,  <span id="id4">[<a class="reference internal" href="#id2706">4</a>]</span>).
While ethical issues associated with the lack of human control are perhaps most apparent for autonomous weapon systems, they extend far beyond the military domain, to a wider class of human-AI systems that make decisions with moral implications. At the time of writing, researchers have approached meaningful human control in the contexts of automated driving systems  <span id="id5">[<a class="reference internal" href="#id2707">5</a>]</span>  <span id="id6">[<a class="reference internal" href="#id2708">6</a>]</span>, medical decision support systems <span id="id7">[<a class="reference internal" href="#id2709">7</a>]</span>, unmanned aerial vehicles <span id="id8">[<a class="reference internal" href="#id2710">8</a>]</span>, among other domains. Many of these domain-specific operationalizations rely on a philosophical account of meaningful human control proposed by <span id="id9">[<a class="reference internal" href="#id2711">9</a>]</span>. This account builds on the concept of “guidance control” <span id="id10">[<a class="reference internal" href="#id2712">10</a>]</span> and provides two necessary conditions for meaningful human control. The tracking condition requires that the decision-making system tracks and responds to all human reasons (i.e., values, norms, intentions) relevant in given circumstances. The tracing condition requires that any action/decision of the human-AI system should be traceable to at least one human within the system who has proper moral understanding of the situation and the effects of the system in that situation.</p>
<p>Tracking and tracing, as well as several alternative domain-specific accounts, provide conceptual frameworks for meaningful human control. Making these concepts less vague and more relatable to design and engineering practice is however very challenging <span id="id11">[<a class="reference internal" href="#id2713">11</a>]</span>. In <span id="id12">[<a class="reference internal" href="#id2714">12</a>]</span> an attempt to close this gap between theory and practice is made by proposing four actionable properties that can be addressed throughout the system’s lifecycle:</p>
<ul class="simple">
<li><p>Property 1: A system in which humans and AI algorithms interact should have an explicitly defined domain of morally loaded situations within which the system ought to operate.</p></li>
<li><p>Property 2: Humans and AI agents within the system should have appropriate and mutually compatible representations.</p></li>
<li><p>Property 3: Responsibility attributed to a human should be commensurate with that human’s ability and authority to control the system.</p></li>
<li><p>Property 4: There should be explicit links between the actions of the AI agents and actions of humans who are aware of their moral responsibility.</p></li>
</ul>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id13"><dl class="citation">
<dt class="label" id="id2703"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Article 36. Key areas for debate on autonomous weapons systems: memorandum for delegates at the convention on certain conventional weapons (ccw) meeting of experts on lethal autonomous weapons systems. Technical Report, Article 36, 2014. URL: <a class="reference external" href="https://www.article36.org/wp-content/uploads/2014/05/A36-CCW-May-2014.pdf">https://www.article36.org/wp-content/uploads/2014/05/A36-CCW-May-2014.pdf</a>.</p>
</dd>
<dt class="label" id="id2704"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Article 36. Killing by machine: key issues for understanding meaningful human control. Technical Report, Article 36, 2015. URL: <a class="reference external" href="https://www.stopkillerrobots.org/wp-content/uploads/2021/09/KILLING_BY_MACHINE_6.4.15.pdf">https://www.stopkillerrobots.org/wp-content/uploads/2021/09/KILLING_BY_MACHINE_6.4.15.pdf</a>.</p>
</dd>
<dt class="label" id="id2705"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Heather M Roff and Richard Moyes. Meaningful human control, artificial intelligence and autonomous weapons. In <em>Briefing Paper Prepared for the Informal Meeting of Experts on Lethal Au-Tonomous Weapons Systems, UN Convention on Certain Conventional Weapons</em>. 2016.</p>
</dd>
<dt class="label" id="id2706"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>M.C. Horowitz and P Scharre. Meaningful human control in weapon systems: a primer. Technical Report, Center for a New American Security, 2015.</p>
</dd>
<dt class="label" id="id2707"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>Daniël D Heikoop, Marjan Hagenzieker, Giulio Mecacci, Simeon Calvert, Filippo Santoni De Sio, and Bart van Arem. Human behaviour with automated driving systems: a quantitative framework for meaningful human control. <em>Theoretical issues in ergonomics science</em>, 20(6):711–730, 2019.</p>
</dd>
<dt class="label" id="id2708"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p>Simeon C Calvert, Bart van Arem, Daniël D Heikoop, Marjan Hagenzieker, Giulio Mecacci, and Filippo Santoni de Sio. Gaps in the control of automated vehicles on roads. <em>IEEE intelligent transportation systems magazine</em>, 13(4):146–153, 2020.</p>
</dd>
<dt class="label" id="id2709"><span class="brackets"><a class="fn-backref" href="#id7">7</a></span></dt>
<dd><p>Matthias Braun, Patrik Hummel, Susanne Beck, and Peter Dabrock. Primer on an ethics of ai-based decision support systems in the clinic. <em>Journal of medical ethics</em>, 47(12):e3–e3, 2021.</p>
</dd>
<dt class="label" id="id2710"><span class="brackets"><a class="fn-backref" href="#id8">8</a></span></dt>
<dd><p>Marc Steen, Jurriaan van Diggelen, Tjerk Timan, and Nanda van der Stap. Meaningful human control of drones: exploring human–machine teaming, informed by four different ethical perspectives. <em>AI and Ethics</em>, pages 1–13, 2022.</p>
</dd>
<dt class="label" id="id2711"><span class="brackets"><a class="fn-backref" href="#id9">9</a></span></dt>
<dd><p>Filippo Santoni de Sio and Jeroen Van den Hoven. Meaningful human control over autonomous systems: a philosophical account. <em>Frontiers in Robotics and AI</em>, pages 15, 2018.</p>
</dd>
<dt class="label" id="id2712"><span class="brackets"><a class="fn-backref" href="#id10">10</a></span></dt>
<dd><p>John Martin Fischer and Mark Ravizza. <em>Responsibility and control: A theory of moral responsibility</em>. Cambridge university press, 1998.</p>
</dd>
<dt class="label" id="id2713"><span class="brackets"><a class="fn-backref" href="#id11">11</a></span></dt>
<dd><p>Rebecca Crootof. A meaningful floor for meaningful human control. <em>Temp. Int'l &amp; Comp. LJ</em>, 30:53, 2016.</p>
</dd>
<dt class="label" id="id2714"><span class="brackets"><a class="fn-backref" href="#id12">12</a></span></dt>
<dd><p>Luciano Cavalcante Siebert, Maria Luce Lupetti, Evgeni Aizenberg, Niek Beckers, Arkady Zgonnikov, Herman Veluwenkamp, David Abbink, Elisa Giaccardi, Geert-Jan Houben, Catholijn M Jonker, and others. Meaningful human control: actionable properties for ai system development. <em>AI and Ethics</em>, pages 1–15, 2022.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Arkady Zgonnikov and Luciano C Siebert.</p>
</div></blockquote>
</div>
</div>
<span id="document-Human_Agency_and_Oversight/Causal_responsibility"></span><div class="tex2jax_ignore mathjax_ignore section" id="causal-responsibility">
<h4>Causal Responsibility<a class="headerlink" href="#causal-responsibility" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p>Causal responsibility is the notion of responsibility that is concerned with actual causation <span id="id1">[<a class="reference internal" href="TAILOR.html#id46">7</a>, <a class="reference internal" href="TAILOR.html#id47">8</a>, <a class="reference internal" href="TAILOR.html#id48">9</a>]</span>.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p><strong>Causal responsibility</strong> is a notion of responsibility that captures the causal influence an event, or an agent’s action or omission has on a particular outcome or state of affairs <span id="id2">[<a class="reference internal" href="TAILOR.html#id46">7</a>, <a class="reference internal" href="TAILOR.html#id47">8</a>, <a class="reference internal" href="TAILOR.html#id48">9</a>]</span>. In the context of human-AI systems, <em>human causal responsibility</em> captures the actual causal influence the human has on an outcome while interacting with the AI system.</p>
<p>Apart from causal responsibility, <span id="id3">[<a class="reference internal" href="TAILOR.html#id46">7</a>]</span> identifies five other notions of responsibility: capacity responsibility, role responsibility, outcome responsibility, virtue responsibility and legal liability. She goes on to explain how causal responsibility is a prerequisite for outcome responsibility which is in turn a prerequisite for legal liability. To ascribe praise, blame or moral responsibility to the actions of an agent, causal responsibility is a necessary condition <span id="id4">[<a class="reference internal" href="#id53">4</a>, <a class="reference internal" href="#id54">5</a>]</span>. When ascribing praise or blame to an an agent’s actions, in addition to causal influence, considerations about the intentions, epistemic conditions and roles of the agent are taken into account. Nonetheless, causal responsibility plays a crucial role in debates about responsibility and legal liability <span id="id5">[<a class="reference internal" href="#id55">6</a>, <a class="reference internal" href="#id56">7</a>]</span>.</p>
<p>Counterfactual reasoning which is predominantly associated with determining actual causality has also been popular in approaches for evaluating causal responsibility <span id="id6">[<a class="reference internal" href="TAILOR.html#id47">8</a>, <a class="reference internal" href="TAILOR.html#id48">9</a>, <a class="reference internal" href="#id57">8</a>, <a class="reference internal" href="#id58">9</a>, <a class="reference internal" href="#id59">10</a>]</span>. In counterfactual reasoning, we check whether an agent’s action is pivotal for the outcome, i.e., whether changing the agent’s action would prevent an outcome from happening. In <span id="id7">[<a class="reference internal" href="TAILOR.html#id47">8</a>]</span> was defined a graded metric for causal responsibility based on the number of changes that have to be made to make an agent’s action pivotal for an outcome, while in <span id="id8">[<a class="reference internal" href="#id60">11</a>]</span> was proposed a notion of causal responsibility in spatial settings based on how one agent restricts the feasible action space of another agent in a concurrent game setting. Related models of group responsibility have also been suggested which are primarily focused on the ability of groups of agents to cause or prevent an outcome or state of affairs <span id="id9">[<a class="reference internal" href="#id61">12</a>, <a class="reference internal" href="#id62">13</a>]</span>. Concerning human-AI interaction, Douer and Meyer have opted an information theoretic approach to propose a metric for human causal responsibility based on how human actions affect the distribution of the outcomes of the human-AI system <span id="id10">[]</span>.</p>
<p>Disentangling causal responsibility is tricky when it comes to complex human-AI systems <span id="id11">[<a class="reference internal" href="#id63">14</a>]</span>. Nonetheless, understanding the causal influences of different agents is crucial for identifying who should change their behaviour for better satisfying relevant human values and ethical principles. For systems to be under meaningful human control, we should be able to the trace the responsibility to the right human(s) and to ensure that the right humans have control over the outcomes <span id="id12">[<a class="reference internal" href="#id64">15</a>, <a class="reference internal" href="#id65">16</a>]</span>. Also teaching AI systems to reason about responsibility is crucial for making AI systems trustworthy <span id="id13">[<a class="reference internal" href="#id66">17</a>]</span>.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id14"><dl class="citation">
<dt class="label" id="id50"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>,<a href="#id3">3</a>)</span></dt>
<dd><p>Nicole A. Vincent. A Structured taxonomy of responsibility concepts. In Nicole A. Vincent, Ibo van de Poel, and Jeroen van den Hoven, editors, <em>Moral Responsibility</em>, Library of Ethics and Applied Philosophy, pages 15–35. Springer, Springer Nature, United States, 2011. <a class="reference external" href="https://doi.org/10.1007/978-94-007-1878-4_2">doi:10.1007/978-94-007-1878-4_2</a>.</p>
</dd>
<dt class="label" id="id51"><span class="brackets">2</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>,<a href="#id6">3</a>,<a href="#id7">4</a>)</span></dt>
<dd><p>Hana Chockler and Joseph Y. Halpern. Responsibility and Blame: A Structural-Model Approach. <em>jair</em>, 22:93–115, October 2004. <a class="reference external" href="https://doi.org/10.1613/jair.1391">doi:10.1613/jair.1391</a>.</p>
</dd>
<dt class="label" id="id52"><span class="brackets">3</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>,<a href="#id6">3</a>)</span></dt>
<dd><p>Florian Engl. A Theory of Causal Responsibility Attribution. <em>SSRN Journal</em>, 2018. <a class="reference external" href="https://doi.org/10.2139/ssrn.2932769">doi:10.2139/ssrn.2932769</a>.</p>
</dd>
<dt class="label" id="id53"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>M. Braham and M. Van Hees. An Anatomy of Moral Responsibility. <em>Mind</em>, 121(483):601–634, July 2012. <a class="reference external" href="https://doi.org/10.1093/mind/fzs081">doi:10.1093/mind/fzs081</a>.</p>
</dd>
<dt class="label" id="id54"><span class="brackets"><a class="fn-backref" href="#id4">5</a></span></dt>
<dd><p>Ibo R. van de Poel and Lambèr M.M. Royakkers. <em>Ethics, Technology, and Engineering : An Introduction</em>. Wiley-Blackwell, United States, 2011. ISBN 978-1-4443-3095-3.</p>
</dd>
<dt class="label" id="id55"><span class="brackets"><a class="fn-backref" href="#id5">6</a></span></dt>
<dd><p>H.L.A. Hart and John Gardner. <em>Punishment and Responsibility</em>. Oxford University Press, March 2008. ISBN 978-0-19-953477-7. <a class="reference external" href="https://doi.org/10.1093/acprof:oso/9780199534777.001.0001">doi:10.1093/acprof:oso/9780199534777.001.0001</a>.</p>
</dd>
<dt class="label" id="id56"><span class="brackets"><a class="fn-backref" href="#id5">7</a></span></dt>
<dd><p>H. L. A. Hart and Tony Honoré. <em>Causation in the Law</em>. Oxford University Press, May 1985. ISBN 978-0-19-825474-4. <a class="reference external" href="https://doi.org/10.1093/acprof:oso/9780198254744.001.0001">doi:10.1093/acprof:oso/9780198254744.001.0001</a>.</p>
</dd>
<dt class="label" id="id57"><span class="brackets"><a class="fn-backref" href="#id6">8</a></span></dt>
<dd><p>Hein Duijf. A Logical Study of Moral Responsibility. <em>Erkenn</em>, September 2023. <a class="reference external" href="https://doi.org/10.1007/s10670-023-00730-2">doi:10.1007/s10670-023-00730-2</a>.</p>
</dd>
<dt class="label" id="id58"><span class="brackets"><a class="fn-backref" href="#id6">9</a></span></dt>
<dd><p>Stelios Triantafyllou, Adish Singla, and Goran Radanovic. Actual Causality and Responsibility Attribution in Decentralized Partially Observable Markov Decision Processes. August 2022. <a class="reference external" href="https://arxiv.org/abs/2204.00302">arXiv:2204.00302</a>.</p>
</dd>
<dt class="label" id="id59"><span class="brackets"><a class="fn-backref" href="#id6">10</a></span></dt>
<dd><p>E. Lorini, D. Longin, and E. Mayor. A logical analysis of responsibility attribution: emotions, individuals and collectives. <em>Journal of Logic and Computation</em>, 24(6):1313–1339, December 2014. <a class="reference external" href="https://doi.org/10.1093/logcom/ext072">doi:10.1093/logcom/ext072</a>.</p>
</dd>
<dt class="label" id="id60"><span class="brackets"><a class="fn-backref" href="#id8">11</a></span></dt>
<dd><p>Ashwin George, Luciano Cavalcante Siebert, David Abbink, and Arkady Zgonnikov. Feasible Action-Space Reduction as a Metric of Causal Responsibility in Multi-Agent Spatial Interactions. 2023 (ECAI 2023 Accepted). <a class="reference external" href="https://arxiv.org/abs/2305.15003">arXiv:2305.15003</a>.</p>
</dd>
<dt class="label" id="id61"><span class="brackets"><a class="fn-backref" href="#id9">12</a></span></dt>
<dd><p>Vahid Yazdanpanah and Mehdi Dastani. Distant Group Responsibility in Multi-agent Systems. In Matteo Baldoni, Amit K. Chopra, Tran Cao Son, Katsutoshi Hirayama, and Paolo Torroni, editors, <em>PRIMA 2016: Princiles and Practice of Multi-Agent Systems</em>, volume 9862, pages 261–278. Springer International Publishing, Cham, 2016. <a class="reference external" href="https://doi.org/10.1007/978-3-319-44832-9_16">doi:10.1007/978-3-319-44832-9_16</a>.</p>
</dd>
<dt class="label" id="id62"><span class="brackets"><a class="fn-backref" href="#id9">13</a></span></dt>
<dd><p>Vahid Yazdanpanah, Sebastian Stein, Enrico H. Gerding, and Nicholas R. Jennings. Applying strategic reasoning for accountability ascription in multiagent teams. In Huáscar Espinoza, John A. McDermid, Xiaowei Huang, Mauricio Castillo-Effen, Xin Cynthia Chen, José Hernández-Orallo, Seán Ó hÉigeartaigh, Richard Mallah, and Gabriel Pedroza, editors, <em>Proceedings of the Workshop on Artificial Intelligence Safety 2021 Co-Located with the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI 2021), Virtual, August, 2021</em>, volume 2916 of CEUR Workshop Proceedings. CEUR-WS.org, 2021.</p>
</dd>
<dt class="label" id="id63"><span class="brackets"><a class="fn-backref" href="#id11">14</a></span></dt>
<dd><p>Virginia Dignum. <em>Responsible Artificial Intelligence: How to Develop and Use AI in a Responsible Way</em>. Artificial Intelligence: Foundations, Theory, and Algorithms. Springer International Publishing, Cham, 2019. ISBN 978-3-030-30370-9 978-3-030-30371-6. <a class="reference external" href="https://doi.org/10.1007/978-3-030-30371-6">doi:10.1007/978-3-030-30371-6</a>.</p>
</dd>
<dt class="label" id="id64"><span class="brackets"><a class="fn-backref" href="#id12">15</a></span></dt>
<dd><p>Filippo Santoni de Sio and Jeroen van den Hoven. Meaningful Human Control over Autonomous Systems: A Philosophical Account. <em>Front. Robot. AI</em>, 5:15, February 2018. <a class="reference external" href="https://doi.org/10.3389/frobt.2018.00015">doi:10.3389/frobt.2018.00015</a>.</p>
</dd>
<dt class="label" id="id65"><span class="brackets"><a class="fn-backref" href="#id12">16</a></span></dt>
<dd><p>Frank Flemisch, Matthias Heesen, Tobias Hesse, Johann Kelsch, Anna Schieben, and Johannes Beller. Towards a dynamic balance between humans and automation: authority, ability, responsibility and control in shared and cooperative control situations. <em>Cogn Tech Work</em>, 14(1):3–18, March 2012. <a class="reference external" href="https://doi.org/10.1007/s10111-011-0191-6">doi:10.1007/s10111-011-0191-6</a>.</p>
</dd>
<dt class="label" id="id66"><span class="brackets"><a class="fn-backref" href="#id13">17</a></span></dt>
<dd><p>Mehdi Dastani and Vahid Yazdanpanah. Responsibility of AI Systems. <em>AI &amp; Soc</em>, 38(2):843–852, April 2023. <a class="reference external" href="https://doi.org/10.1007/s00146-022-01481-4">doi:10.1007/s00146-022-01481-4</a>.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by George Ashwin.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
<span id="document-Transparency/Transparency"></span><div class="tex2jax_ignore mathjax_ignore section" id="transparency">
<h3>Transparency<a class="headerlink" href="#transparency" title="Permalink to this headline">¶</a></h3>
<!--- This is a comment -->
<!-- 

TODO: add ./T3.1_taxonomy.jpg to github!!! 
+ add pyramid.jpeg
+ add reference to Open the Black-Box Problem in introduction


-->
<div class="section" id="in-brief">
<h4>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h4>
<p>The <strong>transparency</strong> requirement is closely linked with the principle of explicability and encompasses transparency of elements
relevant to an AI system: the data, the system and the business models.</p>
<p>In Machine Learning domains, the problem of transparency is often called the <em>“Open the Black-Box Problem”</em> (see below).</p>
<!-- **Explainable AI** (often shortened to **XAI**) is one of the ethical dimensions that is studied in the <a href="https://tailor-network.eu/" target=_blank>TAILOR project</a>.
The origin of XAI dates back to the entering into force of the General Data Protection Regulation (GDPR).
The GDPR {cite}`gdpr`, in its <a href="https://gdpr-info.eu/recitals/no-71/" target=_blank>Recital 71</a>, also mentions the right to explanation, as a suitable safeguard to ensure fair and transparent processing in respect of data subjects. It is defined as the right "to obtain an explanation of the decision reached after profiling". 
According to NIST report {cite}`nist`, an explanation is the evidence, support, or reasoning related to a system’s output or process, where the output of a system differs by task, and the process refers to the procedures, design, and system workflow which underlie the system.
--> <!-- TODO: add link to profiling -->
</div>
<div class="section" id="abstract">
<h4>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">¶</a></h4>
<p>While other aspects of ethics and trustworthiness, such as <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/Privacy_and_Data_Governance"><span class="doc std std-doc">Respect for Privacy</span></a>, are not novel concepts, and a lot of scientific literature has been explored on these topics, the study of transpacency is a relatively new challenge.</p>
<p>In this part, we will cover the main elements that define the explanation of AI systems. First, we will try to survey briefly the main guidelines related to transparency.
Then, we summarize a taxonomy that can be used to classify possible explanations.
We will define the possible  <a class="reference internal" href="TAILOR.html#document-Transparency/XAI_dimensions"><span class="doc">Dimensions of Explanations</span></a> (e.g., we can discriminate between <a class="reference internal" href="TAILOR.html#document-Transparency/model_specific"><span class="doc">Model-Specific vs Model-Agnostic Explainers</span></a>).
Next, we will describe the requirements to provide good explanations and some of the problems related to the Transparency topic.
Finally, we will give some examples of possible solutions we can adopt to provide explanations describing the reasoning behind an ML/AI model.</p>
</div>
<div class="section" id="motivation-and-background">
<h4>Motivation and Background<a class="headerlink" href="#motivation-and-background" title="Permalink to this headline">¶</a></h4>
<p>So far, the usage of black boxes in AI and ML processes has implied the possibility of inadvertently making wrong decisions due to a systematic bias in training data collection. Several practical examples have been provided, highlighting the “bias in, bias out” concept. One of the most famous examples of this concept regards a classification task: the algorithm’s goal was to distinguish between photos of Wolves and Eskimo Dogs (huskies) <span id="id1">[<a class="reference internal" href="#id19">2</a>]</span>. Here, the training phase of the process was done with 20 images, hand-selected such that all pictures of wolves had snow in the background, while pictures of huskies did not. This choice was intentional because it was part of a social experiment. In any case, on a collection of additional 60 images, the classifier predicts “Wolf” if there is snow (or light background at the bottom), and “Husky” otherwise, regardless of animal color, position, pose, etc (see an example in Fig. <a class="reference internal" href="#husky"><span class="std std-numref">4</span></a>).</p>
<div class="figure align-center" id="husky">
<a class="reference internal image-reference" href="_images/husky3.png"><img alt="_images/husky3.png" src="_images/husky3.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 4 </span><span class="caption-text">Raw data and explanation of a bad model’s prediction in the “Husky vs Wolf” task <span id="id2">[<a class="reference internal" href="#id19">2</a>]</span>. On the left, there is the original image; on the right, there is the explanation of the classification.</span><a class="headerlink" href="#husky" title="Permalink to this image">¶</a></p>
</div>
<p>However, one of the most worrisome cases was discovered and published by ProPublica, an independent, nonprofit newsroom that produces investigative journalism with moral force. In <span id="id3">[<a class="reference internal" href="#id20">3</a>]</span>, the authors showed how software can actually be racist. In a nutshell, the authors analyzed a tool called COMPAS (which stands for Correctional Offender Management Profiling for Alternative Sanctions). COMPAS tries to predict, among other indexes, the recidivism of defendants, who are ranked low, medium, or high risk. It was used in many US states (such as New York and Wisconsin) to suggest to judges an appropriate probation or treatment plan for individuals being sentenced. Indeed, the tool was quite accurate (around 70 percent overall with 16,000 probationers), but ProPublica journalists found that black defendants were far more likely than white defendants to be incorrectly judged to be at a higher risk of recidivism, while white defendants were more likely than black defendants to be incorrectly flagged as low risk.</p>
<p>From the above examples, it appears evident that explanation technologies can help companies for creating safer, more trustable products, and better managing any possible liability they may have.</p>
<div class="section" id="the-three-elements-of-transparency">
<h5>The three elements of Transparency<a class="headerlink" href="#the-three-elements-of-transparency" title="Permalink to this headline">¶</a></h5>
<p>According to the <a href="https://wayback.archive-it.org/12090/20201227221227/https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai" target=_blank>High-Level Expert Group on Artificial Intelligence report</a>, the transparency dimension is related to three different but related aspects: traceability, explainability, and communication (as we report as follows) <span id="id4">[<a class="reference internal" href="#id21">1</a>]</span>.</p>
<!--
explainability concerns the ability to explain both the technical processes of an AI system and the related human decisions (e.g., application areas of a system). Following the GDPR interpretation, in -->
<p><em>Traceability.</em> The data sets and the processes that yield the AI system’s decision, including those of data gathering and data labelling as well as the algorithms used, should be documented to the best possible standard to allow for traceability and an increase in transparency. This also applies to the decisions made by the AI system. This enables
identification of the reasons why an AI-decision was erroneous which, in turn, could help prevent future mistakes.
Traceability facilitates <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/auditing"><span class="doc std std-doc">auditability</span></a> as well as <a class="reference internal" href="TAILOR.html#document-Transparency/XAI"><span class="doc std std-doc">explainability</span></a>.</p>
<p><em>Explainability.</em> Explainability concerns the ability to explain both the technical processes of an AI system and the related human decisions (e.g., application areas of a system). Technical explainability requires that the decisions made by an AI system can be understood and traced by human beings. Moreover, trade-offs might have to be made between enhancing a system’s explainability (which may reduce its accuracy) or increasing its accuracy (at the cost of explainability). Whenever an AI system has a significant impact on people’s lives, it should be possible to demand a suitable explanation of the AI system’s decision-making process. Such explanation should be timely and adapted to the expertise of the stakeholder concerned (e.g., layperson, regulator or researcher). In addition, explanations of the degree to which an AI system influences and shapes the organisational decision-making process, design choices of the system, and the rationale for deploying it, should be available (hence ensuring business model transparency).</p>
<p><em>Communication.</em> AI systems should not represent themselves as humans to users; humans have the right to be informed that they are interacting with an AI system (this transparency obbligation is guaranteed by the <a class="reference internal" href="TAILOR.html#document-main/Ethical_Legal_Framework/AI_ACT"><span class="doc std std-doc">AI Act</span></a>. This entails that AI systems must be identifiable as such. In addition, the option to decide against this interaction in favour of human interaction should be provided where needed to ensure compliance with fundamental rights. Beyond this, the AI system’s capabilities and limitations should be communicated to AI practitioners or end-users in a manner appropriate to the use case at hand. This could encompass communication of the AI system’s level of accuracy, as well as its limitations.</p>
<!-- **Explainable AI** (often shortened to **XAI**) is one of the ethical dimensions that is studied in the <a href="https://tailor-network.eu/" target=_blank>TAILOR project</a>.
The origin of XAI dates back to the entering into force of the General Data Protection Regulation (GDPR).
The GDPR {cite}`gdpr`, in its <a href="https://gdpr-info.eu/recitals/no-71/" target=_blank>Recital 71</a>, also mentions the right to explanation, as a suitable safeguard to ensure fair and transparent processing in respect of data subjects. It is defined as the right "to obtain an explanation of the decision reached after profiling". 
According to NIST report {cite}`nist`, an explanation is the evidence, support, or reasoning related to a system’s output or process, where the output of a system differs by task, and the process refers to the procedures, design, and system workflow which underlie the system.
--> <!-- TODO: add link to profiling -->
</div>
</div>
<div class="section" id="open-the-black-box-problem">
<h4>Open the Black-Box Problem<a class="headerlink" href="#open-the-black-box-problem" title="Permalink to this headline">¶</a></h4>
<p>The <em>Open the Black Box Problems</em> for understanding how a black box works can be summarized in the taxonomy proposed in <span id="id5">[<a class="reference internal" href="#id17">4</a>]</span> and reported in Fig. <a class="reference internal" href="#t3-1taxonomy31"><span class="std std-numref">5</span></a>. The Open the Black Box Problems can be separated from one side as the problem of explaining how the decision system returned certain outcomes (<em>Black Box Explanation</em>) and on the other side as the problem of directly designing a transparent classifier that solves the same classification problem (<em>Transparent Box Design</em>). Moreover, the Black Box Explanation problem can be further divided among <em>Model Explanation</em> when the explanation involves the whole logic of the obscure classifier, <em>Outcome Explanation</em> when the target is to understand the reasons for the decisions on a given object, and <em>Model Inspection</em> when the target to understand how internally the black box behaves changing the input.</p>
<div class="figure align-center" id="t3-1taxonomy31">
<a class="reference internal image-reference" href="_images/xai_taxonomy.png"><img alt="_images/xai_taxonomy.png" src="_images/xai_taxonomy.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text">A possible taxonomy about solutions to the Open the Black-Box problem <span id="id6">[<a class="reference internal" href="#id17">4</a>]</span>.</span><a class="headerlink" href="#t3-1taxonomy31" title="Permalink to this image">¶</a></p>
</div>
<!-- TODO: report the chosen paragraph from XAI -->
<p>On a different dimension, a lot of effort has been put into defining what are the possible techniques <a class="reference internal" href="TAILOR.html#document-Transparency/XAI_dimensions"><span class="doc">Dimensions of Explanations</span></a> (e.g., we can discriminate between <a class="reference internal" href="TAILOR.html#document-Transparency/model_specific"><span class="doc">Model-Specific vs Model-Agnostic Explainers</span></a>), the requirements to provide good explanations (see <!-- TODO: add anchor {ref}`guidelines` --> guidelines), how to <!-- TODO: add link [evaluate explanations](./evaluating_explanations.md) --> evaluate explanations, and to understand the <a class="reference internal" href="TAILOR.html#document-Transparency/feature_importance"><span class="doc">Feature Importance</span></a>. Then, it is important to note that a variety of different kinds of explanations can be provided, such as <a class="reference internal" href="TAILOR.html#document-Transparency/single_tree"><span class="doc">Single Tree Approximation</span></a>, <a class="reference internal" href="TAILOR.html#document-Transparency/feature_importance"><span class="doc">Feature Importance</span></a>, <a class="reference internal" href="TAILOR.html#document-Transparency/saliency_maps"><span class="doc">Saliency Maps</span></a>, <a class="reference internal" href="TAILOR.html#document-Transparency/counterfactual"><span class="doc std std-doc">Factual and Counterfactual</span></a>, <a class="reference internal" href="TAILOR.html#document-Transparency/prototypes"><span class="doc std std-doc">Exemplars and Counter-Exemplars</span></a>, and <a class="reference internal" href="TAILOR.html#document-Transparency/rules"><span class="doc std std-doc">Rules List and Rules Sets</span></a>.<!--, exemplars and counter-exemplars, .--></p>
</div>
<div class="section" id="guidelines">
<h4>Guidelines<a class="headerlink" href="#guidelines" title="Permalink to this headline">¶</a></h4>
<p>Given the relative novelty of the topic, a lot of guidelines have been developed in recent years.</p>
<p>However, the most authoritative guideline is the <a href="https://wayback.archive-it.org/12090/20201227221227/https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai" target=_blank>High-Level Expert Group on Artificial Intelligence - Ethics Guidelines for Trustworthy AI</a>. Here, the explainability topic is included in the broader <a class="reference internal" href="TAILOR.html#document-Transparency/blackbox_transparent"><span class="doc std std-doc">transparency</span></a>. Following the GDPR interpretation, in <span id="id7">[<a class="reference internal" href="#id21">1</a>]</span>, it is stated that whenever an AI system has a significant impact on people’s lives, it should be possible to demand a suitable explanation of the AI system’s decision-making process. Such explanation should be timely and adapted to the expertise of the stakeholder concerned (e.g., layperson, regulator, or researcher). In addition, explanations of the degree to which an AI system influences and shapes the organizational decision-making process, design choices of the system, and the rationale for deploying it, should be available (hence ensuring business model transparency).</p>
<p>Another distinguished authority that has been worked on ethical guidance is <strong>the Alan Turing Institute</strong>, the UK’s national institute for data science and artificial intelligence, where David Leslie <span id="id8">[<a class="reference internal" href="#id22">5</a>]</span> summarized the risks due to the lack of transparency or the absence of a valid explanation, and he advocates the use of <!-- TODO: add link {doc}`./counterfactuals`--> counterfactuals for contrasting unfair decisions. Together with the Information Commissioner’s Office (ICO), which is responsible for overseeing data protection in the UK, it has been published more recent and complete guidance <span id="id9">[<a class="reference internal" href="#id23">6</a>]</span>. Here, six steps are recommended to develop a system:</p>
<ol class="simple">
<li><p>Select priority explanations by considering the domain, use case, and impact on the individual.</p></li>
<li><p>Collect and pre-process data in an explanation-aware manner, stressing the fact that the way in which data is collected and pre-processed may affect the quality of the explanation.</p></li>
<li><p>Build systems to ensure to being able to extract relevant information for a range of explanation types.</p></li>
<li><p>Translate the rationale of your system’s results into useable and easily understandable reasons, e.g., transforming the model’s logic from quantitative rationale into intuitive reasons or using everyday language that can be understood by non-technical stakeholders.</p></li>
<li><p>Prepare implementers to deploy the AI system, through appropriate training.</p></li>
<li><p>Consider how to build and present the explanation, particularly keeping in mind the context and contextual factors (domain, impact, data, urgency, audience) to deliver appropriate information to the individual.</p></li>
</ol>
<p>Nevertheless, the attention on this theme is not relegated to the European border. Indeed, as an example of US effort in dealing with Explainability and Ethics, <strong>NIST, the National Institute of Standards and Technology of Maryland</strong>, developed some guidelines, and a white paper <span id="id10">[<a class="reference internal" href="#id24">7</a>]</span> was published after a first draft<a class="footnote-reference brackets" href="#nist-draft" id="id11">1</a> was published in 2020, a variety of comments<a class="footnote-reference brackets" href="#nist-comments" id="id12">2</a> was collected, and a workshop<a class="footnote-reference brackets" href="#nist-workshop" id="id13">3</a> involving different stakeholders was held. The white paper <span id="id14">[<a class="reference internal" href="#id24">7</a>]</span> analyzes the multidisciplinary nature of explainable AI and acknowledges the existence of different users who requires different kinds of explanations, stating that one-size-fits-all explanations do not exist. The fundamental properties of explanations contained in the report are:</p>
<ul class="simple">
<li><p><em>Meaningfulness</em>, i.e., explanations must be understandable to the intended consumer(s). This means that there is the need to consider the intended audience and some characteristics they can have, such as prior knowledge or the overall psychological differences between people. Moreover, the explanation’s purpose is relevant too. Indeed, different scenarios and needs impact on what is important and useful in a given context. This implies understanding the audience’s needs, level of expertise, and relevancy to the question or query.</p></li>
<li><p><em>Accuracy</em>, i.e., explanations correctly reflect a system’s process for generating its output. Explanation accuracy is a distinct concept from decision accuracy. Explanation accuracy needs to account for the level of detail in the explanation. This second principle might be in contrast with the previous one: a detailed explanation may accurately reflect the system’s processing but sacrifice how useful and accessible it is to certain audiences, while  a brief, simple explanation may be highly understandable but would not fully characterize the system.</p></li>
<li><p><em>Knowledge limits</em>, i.e., characterizing the fact that a system only operates under conditions for which it was designed and when it reaches sufficient confidence in its output. This practice safeguard answers so that a judgment is not provided when it may be inappropriate to do so. This principle can increase trust in a system by preventing misleading, dangerous, or unjust outputs.</p></li>
</ul>
</div>
<div class="section" id="software-frameworks-supporting-dimension">
<h4>Software Frameworks Supporting Dimension<a class="headerlink" href="#software-frameworks-supporting-dimension" title="Permalink to this headline">¶</a></h4>
<p>Within the European Research Council (ERC) <a href="https://xai-project.eu/" target=_blank>XAI project</a> and the European Union’s Horizon 2020 <a href="http://project.sobigdata.eu/" target=_blank> SoBigData++ project</a>, we are developing an infrastructure for sharing experimental datasets and explanation algorithms with the research community, creating a common ground for researchers working on explanations of black boxes from different domains.
All resources, provided they are not prohibited by specific legal/ethical constraints, will be collected and described in a <a href="https://sobigdata.d4science.org/catalogue-sobigdata" target=_blank>findable catalogue</a>.
A dedicated virtual research environment will be activated, so that a variety of relevant resources, such as data, methods, experimental workflows, platforms, and literature, will be managed through the SoBigData++ e-infrastructure services and made available to the research community through a variety of regulated access policies.
We will provide a link to the libraries and framework as soon as they be will fully published.</p>
</div>
<div class="section" id="main-keywords">
<h4>Main Keywords<a class="headerlink" href="#main-keywords" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><a class="reference internal" href="TAILOR.html#document-Transparency/XAI_dimensions"><span class="doc">Dimensions of Explanations</span></a>: <strong>Dimensions of explanations</strong> are useful to analyze the interpretability of AI systems and to classify the explanation method.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Transparency/blackbox_transparent"><span class="doc">Black Box Explanation vs Explanation by Design</span></a>: The difference between <strong>Black Box Explanation</strong> (or <strong>Post-hoc Explanations</strong>) and <strong>Explanation by Design</strong> (or <strong>Ante-hoc Explanations</strong>) regards the ability to know and exploit the behaviour of the AI model. With a black box explanation, we pair the black box model with an interpretation the black box decisions or model, while in the second case, the strategy is to rely, by design, on a transparent model.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Transparency/model_specific"><span class="doc">Model-Specific vs Model-Agnostic Explainers</span></a>: We distinguish between <strong>model-specific</strong> or <strong>model-agnostic</strong> explanation method depending on whether the technique adopted to retrieve the explanation acts on a particular model adopted by an AI system, or can be used on any type of AI.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Transparency/global_local"><span class="doc">Global vs Local Explanations</span></a>: We distinguish between a <strong>global</strong> or <strong>local</strong> explanation depending on whether the explanation allows understanding the whole logic of a model used by an AI system or the explanation refers to a specific case, i.e., only a single decision is interpretable.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Transparency/XAI"><span class="doc">Explainable AI</span></a>: <strong>Explainable AI</strong> (often shortened to <strong>XAI</strong>) is one of the ethical dimensions described in the General Data Protection Regulation (GDPR).
Indeed, the GDPR mentions the right to explanation, as a suitable safeguard to ensure fair and transparent processing in respect of data subjects. It is defined as the right “to obtain an explanation of the decision reached after profiling”.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Transparency/XAI_kinds"><span class="doc">Kinds of Explanations</span></a>: Explanations returned by an AI system depend on various factors (such as the task or the available data); generally speaking, each kind of explanations serves better a specific context.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Transparency/counterfactual"><span class="doc">Counterfactuals</span></a>: A <strong>counterfactual explanation</strong> shows what should have been different to change the decision of an AI system. For example, a counterfactual explanation could be a local explaination of a certain istance by providing the nearest istances that lead to a different decision or describing a small change in the input of the model that lead to a change in the outcome of the model.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Transparency/feature_importance"><span class="doc">Feature Importance</span></a>: The <strong>feature importance</strong> technique provides a score, representing the “importance”, for all the input features for a given AI model, i.e., a higher importance means that the corresponding feature will have a larger effect on the model.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Transparency/lore"><span class="doc">Local Rule-based Explanation</span></a>: <strong>Local Rule-based Explanation</strong> aims to extract a rule that provides a local explaination for a certain instance of the model.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Transparency/prototypes"><span class="doc">Prototypes</span></a>: <strong>Prototype-based explanation</strong> methods return as explanation a selection of particular instances of the dataset for locally explaining the behavior of the AI system.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Transparency/rules"><span class="doc">Rules List and Rules Set</span></a>: A <strong>decision rule</strong> is generally formed by a set of conditions and by a consequent, e.g., if conditions, then consequent.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Transparency/saliency_maps"><span class="doc">Saliency Maps</span></a>: Saliency maps are explanations used on image classification tasks. A <strong>saliency map</strong> is an image where each pixel’s color represents a value modeling the importance of that pixel in the original image (i.e., the one given in input to the explainer) for the prediction.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Transparency/single_tree"><span class="doc">Single Tree Approximation</span></a>: The <strong>single tree appoximation</strong> is an approach that aims at building a decision tree to approximate the behavior of a black box, typically a neural network.</p></li>
</ul>
</div>
<div class="section" id="bibliography">
<h4>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h4>
<!-- :style: unsrtalpha -->
<p id="id15"><dl class="citation">
<dt class="label" id="id21"><span class="brackets">1</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id7">2</a>)</span></dt>
<dd><p>High-Level Expert Group on Artificial Intelligence. Ethics Guidelines for Trustworthy AI. 2019. URL: <a class="reference external" href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai">https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</a> (visited on 2022-02-16).</p>
</dd>
<dt class="label" id="id19"><span class="brackets">2</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>)</span></dt>
<dd><p>M.T. Ribeiro, S. Singh, and C. Guestrin. &quot;why should I trust you?&quot;: explaining the predictions of any classifier. In <em>SIGKDD</em>. 2016.</p>
</dd>
<dt class="label" id="id20"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>J. Angwin, J. Larson, S. Mattu, and L. Kirchner. Machine bias: there’s software used across the country to predict future criminals. and it’s biased against blacks. 2016. URL: <a class="reference external" href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</a> (visited on 2020-09-22).</p>
</dd>
<dt class="label" id="id17"><span class="brackets">4</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id6">2</a>)</span></dt>
<dd><p>R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and D. Pedreschi. A survey of methods for explaining black box models. <em>ACM computing surveys (CSUR)</em>, 2018.</p>
</dd>
<dt class="label" id="id22"><span class="brackets"><a class="fn-backref" href="#id8">5</a></span></dt>
<dd><p>David Leslie (the Alan Turing Institute). Understanding artificial intelligence ethics and safety - a guide for the responsible design and implementation of ai systems in the public sector. URL: <a class="reference external" href="https://www.turing.ac.uk/sites/default/files/2019-06/understanding_artificial_intelligence_ethics_and_safety.pdf">https://www.turing.ac.uk/sites/default/files/2019-06/understanding_artificial_intelligence_ethics_and_safety.pdf</a> (visited on 2022-02-16).</p>
</dd>
<dt class="label" id="id23"><span class="brackets"><a class="fn-backref" href="#id9">6</a></span></dt>
<dd><p>Information Commissioner's Office (ICO) and The Alan Turing Institute. Explaining decisions made with AI. URL: <a class="reference external" href="https://ico.org.uk/for-organisations/guide-to-data-protection/key-dp-themes/explaining-decisions-made-with-ai/">https://ico.org.uk/for-organisations/guide-to-data-protection/key-dp-themes/explaining-decisions-made-with-ai/</a> (visited on 2022-02-16).</p>
</dd>
<dt class="label" id="id24"><span class="brackets">7</span><span class="fn-backref">(<a href="#id10">1</a>,<a href="#id14">2</a>)</span></dt>
<dd><p>P. Jonathon Phillips, Carina A. Hahn, Peter C. Fontana, Amy N. Yates, Kristen Greene, David A. Broniatowski, and Mark A. Przybocki. Four principles of explainable artificial intelligence. NISTIR 8312, September 2021. URL: <a class="reference external" href="https://doi.org/10.6028/NIST.IR.8312">https://doi.org/10.6028/NIST.IR.8312</a> (visited on 2022-02-16).</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Pratesi, Trasarti, Giannotti. Ethics in Smart Information Systems. Policy Press</em> and from <em>Guidotti, Monreale, Ruggieri, Turini, Giannotti, Pedreschi. A survey of methods for explaining black box models. ACM Computing Surveys, Volume 51 Issue 5 (2019)</em> by Francesca Pratesi.</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="nist-draft"><span class="brackets"><a class="fn-backref" href="#id11">1</a></span></dt>
<dd><p><a class="reference external" href="https://doi.org/10.6028/NIST.IR.8312-draft">https://doi.org/10.6028/NIST.IR.8312-draft</a></p>
</dd>
<dt class="label" id="nist-comments"><span class="brackets"><a class="fn-backref" href="#id12">2</a></span></dt>
<dd><p><a class="reference external" href="https://www.nist.gov/artificial-intelligence/comments-received-four-principles-explainable-artificial-intelligence-nistir">https://www.nist.gov/artificial-intelligence/comments-received-four-principles-explainable-artificial-intelligence-nistir</a></p>
</dd>
<dt class="label" id="nist-workshop"><span class="brackets"><a class="fn-backref" href="#id13">3</a></span></dt>
<dd><p><a class="reference external" href="https://www.nist.gov/system/files/documents/2021/09/24/XAI_Workshop_Summary_Final_20210922.pdf">https://www.nist.gov/system/files/documents/2021/09/24/XAI_Workshop_Summary_Final_20210922.pdf</a></p>
</dd>
</dl>
</div>
<div class="toctree-wrapper compound">
<span id="document-Transparency/XAI_dimensions"></span><div class="tex2jax_ignore mathjax_ignore section" id="dimensions-of-explanations">
<h4>Dimensions of Explanations<a class="headerlink" href="#dimensions-of-explanations" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p>Dimensions of Explanations are useful to analyze the interpretability of AI systems and to classify the explanation method.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>The goal of <a class="reference internal" href="TAILOR.html#document-Transparency/Transparency"><span class="doc std std-doc">Explainable AI</span></a> is to <em>interpret</em> AI reasoning. According to Merriam-webster, to <em>interpret</em> means to give or provide the meaning or to explain and present in understandable terms some concepts.
Therefore, in AI, <em>interpretability</em> is defined as the ability to <em>explain</em> or to provide the meaning in understandable terms to a human <span id="id1">[<a class="reference internal" href="#id63">2</a>]</span>,<span id="id2">[<a class="reference internal" href="#id92">3</a>]</span>. These definitions assume that the concepts composing an explanation and expressed in understandable terms are self-contained and do not need further explanations. An explanation is an “interface” between a human and an AI, and it is simultaneously both human understandable and an accurate proxy of the AI.</p>
<p>We can identify a set of dimensions to analyze the interpretability of AI systems that, in turn, reflect on existing different types of explanations <span id="id3">[<a class="reference internal" href="TAILOR.html#id17">4</a>]</span>.
Some of these dimensions are related to <em>functional requirements</em> of explainable artificial intelligence, i.e., requirements that identify the algorithmic adequacy of a particular approach for a specific application, while others to the <em>operational requirements</em>, i.e., requirements that take into consideration how users interact with an explainable system and what is the expectation.
Some dimensions instead derive from the need for <em>usability criteria</em> from a user perspective, while others derive from the need for guarantees against any vulnerability issues.</p>
<p>Recently, all these requirements have been analyzed <span id="id4">[<a class="reference internal" href="#id138">4</a>]</span> to provide a framework allowing the systematic comparison of explainability methods. In particular, in <span id="id5">[<a class="reference internal" href="#id138">4</a>]</span>, Sokol and Flach propose <em>Explainability Fact Sheets</em>, which enable researchers and practitioners to assess capabilities and limitations of a particular explainable method.
As an example, given an explanation method <em>m</em>, we can consider the following functional requirements.</p>
<ul class="simple">
<li><p><em>(i)</em> Even though <em>m</em> is designed to explain regressors, can we use it to explain probabilistic classifiers?</p></li>
<li><p><em>(ii)</em> Can we employ <em>m</em> on categorical features even though it only works on numerical features? On the other hand, as an operational requirement, can we consider which is the <em>function of the explanation</em>? Provide transparency, assess the fairness, etc.</p></li>
</ul>
<p>Besides the detailed requirements illustrated in <span id="id6">[<a class="reference internal" href="#id138">4</a>]</span>, in the literature it is recognized a categorization of explanation methods among fundamental pillars <span id="id7">[<a class="reference internal" href="TAILOR.html#id66">2</a>]</span>,<span id="id8">[<a class="reference internal" href="TAILOR.html#id17">4</a>]</span>:</p>
<ul class="simple">
<li><p><em>(i)</em> <a class="reference internal" href="TAILOR.html#document-Transparency/blackbox_transparent"><span class="doc">Black Box Explanation vs Explanation by Design</span></a>,</p></li>
<li><p><em>(ii)</em> <a class="reference internal" href="TAILOR.html#document-Transparency/global_local"><span class="doc">Global vs Local Explanations</span></a>,</p></li>
<li><p><em>(iii)</em> <a class="reference internal" href="TAILOR.html#document-Transparency/model_specific"><span class="doc">Model-Specific vs Model-Agnostic Explainers</span></a>. <!---In the following we present details of these distinctions and other important features characterizing XAI methods.--></p></li>
</ul>
<p>Fig. <a class="reference internal" href="#xai-tax"><span class="std std-numref">6</span></a> illustrates a summarized ontology of the taxonomy used to classify XAI methods.</p>
<div class="figure align-center" id="xai-tax">
<a class="reference internal image-reference" href="_images/xai_taxonomy.png"><img alt="_images/xai_taxonomy.png" src="_images/xai_taxonomy.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text">A summarized ontology of the taxonomy of XAI methods <span id="id9">[<a class="reference internal" href="TAILOR.html#id65">1</a>]</span>.</span><a class="headerlink" href="#xai-tax" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id10"><dl class="citation">
<dt class="label" id="id64"><span class="brackets">1</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id8">2</a>)</span></dt>
<dd><p>R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and D. Pedreschi. A survey of methods for explaining black box models. <em>ACM computing surveys (CSUR)</em>, 2018.</p>
</dd>
<dt class="label" id="id63"><span class="brackets"><a class="fn-backref" href="#id1">2</a></span></dt>
<dd><p>A. B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik, A. Barbado, S. García, S. Gil-López, D. Molina, and R. Benjamins. Explainable artificial intelligence (xai): concepts, taxonomies, opportunities and challenges toward responsible ai. In <em>Information Fusion</em>. 2020.</p>
</dd>
<dt class="label" id="id92"><span class="brackets"><a class="fn-backref" href="#id2">3</a></span></dt>
<dd><p>Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning. <em>arXiv preprint arXiv:1702.08608</em>, 2017.</p>
</dd>
<dt class="label" id="id138"><span class="brackets">4</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id5">2</a>,<a href="#id6">3</a>)</span></dt>
<dd><p>Kacper Sokol and Peter A. Flach. Explainability fact sheets: a framework for systematic assessment of explainable approaches. In <em>ACM Conference on Fairness, Accountability, and Transparency</em>, 56–67. ACM, 2020.</p>
</dd>
<dt class="label" id="id73"><span class="brackets"><a class="fn-backref" href="#id7">5</a></span></dt>
<dd><p>Amina Adadi and Mohammed Berrada. Peeking inside the black-box: a survey on explainable artificial intelligence (xai). <em>IEEE Access</em>, 6:52138–52160, 2018.</p>
</dd>
<dt class="label" id="id65"><span class="brackets"><a class="fn-backref" href="#id9">6</a></span></dt>
<dd><p>Riccardo Guidotti, Anna Monreale, Dino Pedreschi, and Fosca Giannotti. <em>Principles of Explainable Artificial Intelligence</em>. Springer International Publishing, 2021.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Guidotti, Monreale, Pedreschi, Giannotti. Principles of Explainable Artificial Intelligence. Springer International Publishing (2021)</em> by Francesca Pratesi and Riccardo Guidotti.</p>
</div></blockquote>
</div>
<div class="toctree-wrapper compound">
<span id="document-Transparency/blackbox_transparent"></span><div class="tex2jax_ignore mathjax_ignore section" id="black-box-explanation-vs-explanation-by-design">
<h5>Black Box Explanation vs Explanation by Design<a class="headerlink" href="#black-box-explanation-vs-explanation-by-design" title="Permalink to this headline">¶</a></h5>
<p><em>Synonyms</em>: Post-hoc vs Ante-hoc Explanations, Explainable vs Interpretable Artificial Intelligence</p>
<div class="section" id="in-brief">
<h6>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p>The difference between <strong>Black Box Explanation</strong> (or <strong>Post-hoc Explanations</strong>) and <strong>Explanation by Design</strong> (or <strong>Ante-hoc Explanations</strong>) regards the ability to know and exploit the behaviour of the AI model. With a black box explanation, we pair the black box model with an interpretation the black box decisions or model, while in the second case, the strategy is to rely, by design, on a transparent model.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>The precise definitions of the core concepts in these fields – such as transparency, interpretability and explainability – remain elusive, in part because these notions tend to be domain-specific <span id="id1">[<a class="reference internal" href="#id15">2</a>]</span>.</p>
<p>When we talk about <strong>Black Box Explanation</strong>, the strategy is to couple an AI with a black box model with an explanation method able to interpret the black box decisions. Usually, this situation is associated with systems that are <em>proprietary</em> – hence inaccessible – or <em>too complicated</em> to understand <span id="id2">[<a class="reference internal" href="#id35">3</a>]</span>.</p>
<p>In the case of <strong>Explanation by Design</strong> (aka <strong>Transparency</strong>), the idea is to substitute the obscure model with a transparent model in which the decision process is accessible by design, i.e., explainability is inserted into a model from the very beginning.
Interpretability presupposes an observer who appraises the intelligibility of the model in question.
This indicates that <strong>interpretability</strong> is not a binary property, but it is rather determined by the degree to which one can understand the system’s operations, and thus resides on a <em>spectrum of opaqueness</em> <span id="id3">[<a class="reference internal" href="#id16">4</a>]</span>.</p>
<p>In view of this fluidity, the distinction between <strong>ante-hoc interpretability</strong> and <strong>post-hoc explainability</strong> techniques commonly made in the literature becomes blurred.
<strong>Post-hoc explainability</strong> offers insights into the behaviour of a predictive system by building an additional <em>explanatory model</em> that interfaces between it and human explainees (i.e., explanation recipients).
This is the only approach compatible with black-box systems, with which we can interact exclusively by manipulating their inputs and observing their outputs.
Such an operationalisation is nonetheless unsuitable for high stakes domains since the resulting explanations are not guaranteed to be reliable and truthful with respect to the underlying model <span id="id4">[<a class="reference internal" href="#id35">3</a>]</span>.
<strong>Ante-hoc explainability</strong>, in contrast, often refers to data-driven systems whose model form adheres (in practice) to application- and domain-specific constraints – a <em>functional</em> definition – which allows their designers to judge their trustworthiness and communicate how they operate to others <span id="id5">[<a class="reference internal" href="#id35">3</a>]</span>.
While the <strong>post-hoc</strong> moniker is, in general, used consistently across the literature, the same is not true for the <em>ante-hoc</em> label;
the <em>interpretability</em> and <em>explainability</em> terms are also often mixed and used interchangeably, adding to the confusion.
<strong>Ante-hoc</strong> sometimes refers to, among other things, transparent models that are self-explanatory, systems that are inherently interpretable, scenarios where the explanation and the prediction come from the same model, and situations where the model itself (or its part) constitutes an explanation, which are vague terms that are often incompatible with the <em>functional</em> definition of ante-hoc interpretability <span id="id6">[<a class="reference internal" href="#id16">4</a>]</span>.
Nonetheless, all of these notions presuppose an observer to whom the model is interpretable and who understands its functioning, which brings us back to the issue of accounting for an audience and its background.</p>
<p>Figure <a class="reference internal" href="#post-hoc"><span class="std std-numref">7</span></a> depicts this distinction.</p>
<div class="figure align-center" id="post-hoc">
<a class="reference internal image-reference" href="_images/post.png"><img alt="_images/post.png" src="_images/post.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 7 </span><span class="caption-text">(Top) Black box explanation pipeline. (Bottom) Explanation by design pipeline. <span id="id7">[<a class="reference internal" href="TAILOR.html#id65">1</a>]</span>.</span><a class="headerlink" href="#post-hoc" title="Permalink to this image">¶</a></p>
</div>
<p>Starting from a dataset <em>X</em>, the <strong>black box explanation</strong> idea is to maintain the high performance of the obscure model <em>b</em> used by the AI, which is allowed to be trained normally, and to use an explanation method <em>f</em> to retrieve an explanation <em>e</em> by reasoning over <em>b</em> and <em>X</em>. In such a way, we aim to reach both accuracy and the ability to gain some <a class="reference internal" href="TAILOR.html#document-Transparency/XAI_kinds"><span class="doc">Kinds of Explanations</span></a>. This kind of approach is the one more addressed nowadays in the XAI research field <span id="id8">[<a class="reference internal" href="#id30">5</a>]</span> <span id="id9">[<a class="reference internal" href="#id33">6</a>]</span> <span id="id10">[<a class="reference internal" href="#id37">7</a>]</span>.</p>
<p>On the other hand, the <strong>explanation by design</strong> consists of directly designing a comprehensible model <em>c</em> over the dataset <em>X</em>, which is interpretable by design and returns an explanation <em>e</em> besides the prediction <em>y</em>. Thus, the idea is to use this transparent model directly in the AI system <span id="id11">[<a class="reference internal" href="#id35">3</a>]</span> <span id="id12">[<a class="reference internal" href="#id36">8</a>]</span>.
In the literature, there are various models recognized to be interpretable. Examples include decision trees, decision rules, and linear models <span id="id13">[<a class="reference internal" href="#id39">9</a>]</span>. These models are considered easily understandable and interpretable for humans. However, nearly all of them sacrifice performance in favor of interpretability. In addition, they cannot be applied effectively on data types such as images or text, but only on tabular, relational data, i.e., tables.</p>
<p>The <em>functional</em> definition of ante-hoc interpretability concerns the engineering aspects of information modelling and its adherence to domain-specific knowledge and constraints.
A <em>transparent</em> model may be <em>ante-hoc interpretable</em> to its engineers and domain experts with relevant technical knowledge, yet it may not engender understanding in explainees outside this milieu.
For example, a decision tree is transparent, but its interpretability presupposes its small size as well as technical understanding of its operation and familiarity with the underlying data domain;
one way to restore interpretability to a large tree is to process its structure to extract <a class="reference internal" href="TAILOR.html#document-Transparency/rules"><span class="doc std std-doc">decision rules</span></a>, <a class="reference internal" href="TAILOR.html#document-Transparency/prototypes"><span class="doc std std-doc">exemplars</span></a> and <a class="reference internal" href="TAILOR.html#document-Transparency/counterfactual"><span class="doc std std-doc">counterfactuals</span></a>.</p>
<!--
:filter: docname in docnames
:filter: blackbox_transparent in docnames
:filter: {"blackbox_transparent"} & docnames
 -->
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p id="id14"><dl class="citation">
<dt class="label" id="id17"><span class="brackets"><a class="fn-backref" href="#id7">1</a></span></dt>
<dd><p>Riccardo Guidotti, Anna Monreale, Dino Pedreschi, and Fosca Giannotti. <em>Principles of Explainable Artificial Intelligence</em>. Springer International Publishing, 2021.</p>
</dd>
<dt class="label" id="id15"><span class="brackets"><a class="fn-backref" href="#id1">2</a></span></dt>
<dd><p>Kacper Sokol and Peter Flach. Explainability is in the mind of the beholder: establishing the foundations of explainable artificial intelligence. <em>arXiv preprint arXiv:2112.14466</em>, 2021.</p>
</dd>
<dt class="label" id="id35"><span class="brackets">3</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id4">2</a>,<a href="#id5">3</a>,<a href="#id11">4</a>)</span></dt>
<dd><p>Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. <em>Nature Machine Intelligence</em>, 1(5):206–215, 2019.</p>
</dd>
<dt class="label" id="id16"><span class="brackets">4</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id6">2</a>)</span></dt>
<dd><p>Kacper Sokol and Julia E Vogt. (un)reasonable allure of ante-hoc interpretability for high-stakes domains: transparency is necessary but insufficient for comprehensibility. In <em>3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH) at 2023 International Conference on Machine Learning (ICML)</em>. 2023.</p>
</dd>
<dt class="label" id="id30"><span class="brackets"><a class="fn-backref" href="#id8">5</a></span></dt>
<dd><p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: explaining the predictions of any classifier. In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 1135–1144. ACM, 2016.</p>
</dd>
<dt class="label" id="id33"><span class="brackets"><a class="fn-backref" href="#id9">6</a></span></dt>
<dd><p>Mark Craven and Jude W Shavlik. Extracting tree-structured representations of trained networks. In <em>Advances in neural information processing systems</em>, 24–30. 1996.</p>
</dd>
<dt class="label" id="id37"><span class="brackets"><a class="fn-backref" href="#id10">7</a></span></dt>
<dd><p>Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In <em>Advances in neural information processing systems</em>, 4765–4774. 2017.</p>
</dd>
<dt class="label" id="id36"><span class="brackets"><a class="fn-backref" href="#id12">8</a></span></dt>
<dd><p>Cynthia Rudin and Joanna Radin. Why are we using black box models in ai when we don’t need to? a lesson from an explainable ai competition. <em>Harvard Data Science Review</em>, 2019.</p>
</dd>
<dt class="label" id="id39"><span class="brackets"><a class="fn-backref" href="#id13">9</a></span></dt>
<dd><p>Alex A Freitas. Comprehensible classification models: a position paper. <em>ACM SIGKDD explorations newsletter</em>, 15(1):1–10, 2014.</p>
</dd>
<dt class="label" id="id88"><span class="brackets">10</span></dt>
<dd><p>Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In <em>Advances in neural information processing systems</em>, 4349–4357. 2016.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was partially readapted from <em>Guidotti, Monreale, Pedreschi, Giannotti. Principles of Explainable Artificial Intelligence. Springer International Publishing (2021)</em> by Francesca Pratesi, Riccardo Guidotti, and Kacper Sokol.</p>
</div></blockquote>
</div>
</div>
<span id="document-Transparency/model_specific"></span><div class="tex2jax_ignore mathjax_ignore section" id="model-specific-vs-model-agnostic-explainers">
<h5>Model-Specific vs Model-Agnostic Explainers<a class="headerlink" href="#model-specific-vs-model-agnostic-explainers" title="Permalink to this headline">¶</a></h5>
<p><em>Synonyms</em>: Not Generalizable vs Generalizable Explanations.</p>
<div class="section" id="in-brief">
<h6>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p>We distinguish between <strong>model-specific</strong> or <strong>model-agnostic</strong> explanation methods depending on whether the technique adopted to retrieve the explanation acts on a particular model adopted by an AI system, or can be used on any type of AI.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>This is one of the first <a class="reference internal" href="TAILOR.html#document-Transparency/XAI_dimensions"><span class="doc">Dimensions of Explanations</span></a> we should consider.</p>
<p>The most used approach to explain AI black boxes is known as <em>reverse engineering</em>. The name comes from the fact that the explanation is retrieved by observing what happens to the output, i.e., the AI decision, when changing the input in a controlled way. An explanation method is model-specific, or not generalizable <span id="id1">[<a class="reference internal" href="#id6">1</a>]</span>,
whether it considers inputs or outputs as well as the inner-workings of a machine learning model.
The drawback of this approach is that it can be used to interpret only particular types of black box models. For example, if an explanation approach is designed to interpret a Random Forest <span id="id2">[<a class="reference internal" href="TAILOR.html#id14">2</a>]</span> and internally uses a concept of distance between trees, then such an approach cannot be used to explain the predictions of a neural network.</p>
<p>On the other hand, an explanation method is model-agnostic, or generalizable, when it can be used independently from the black box model being explained. In other words, the AI’s internal characteristics are not exploited to build the interpretable model approximating the black box behavior.</p>
<p>In Fig. <a class="reference internal" href="#specific-agnostic"><span class="std std-numref">8</span></a>, there is a summarization of these two kinds of approaches.</p>
<div class="figure align-center" id="specific-agnostic">
<a class="reference internal image-reference" href="_images/TAILOR-modelspecific&amp;agnostic.png"><img alt="_images/TAILOR-modelspecific&amp;agnostic.png" src="_images/TAILOR-modelspecific&amp;agnostic.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 8 </span><span class="caption-text">On the left, an explainer which exploites the internal structure and behaviour of the model. On the right, an explainer which uses the model as a black-box to understand its reasoning.</span><a class="headerlink" href="#specific-agnostic" title="Permalink to this image">¶</a></p>
</div>
<!--- @misc{explainer,
 author={Spinner, Thilo and Schlegel, Udo and Sch ̈afer, Hanna and El-Assady, Mennatallah},
https://explainer.ai/
-->
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p id="id3"><dl class="citation">
<dt class="label" id="id6"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>David Martens, Bart Baesens, Tony Van Gestel, and Jan Vanthienen. Comprehensible credit scoring models using rule extraction from support vector machines. <em>European journal of operational research</em>, 183((3)):1466––1476, 2007.</p>
</dd>
<dt class="label" id="id7"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Pang-Ning Tan, Michael Steinbach, and Vipin Kumar. <em>Introduction to data mining</em>. Pearson Addison-Wesley, 2006.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Guidotti, Monreale, Pedreschi, Giannotti. Principles of Explainable Artificial Intelligence. Springer International Publishing (2021)</em> by Francesca Pratesi and Riccardo Guidotti.</p>
</div></blockquote>
</div>
</div>
<span id="document-Transparency/global_local"></span><div class="tex2jax_ignore mathjax_ignore section" id="global-vs-local-explanations">
<h5>Global vs Local Explanations<a class="headerlink" href="#global-vs-local-explanations" title="Permalink to this headline">¶</a></h5>
<div class="section" id="in-brief">
<h6>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p>We distinguish between a <strong>global</strong> or <strong>local</strong> explanation depending on whether the explanation allows understanding the whole logic of a model used by an AI system or the explanation refers to a specific case, i.e., only a single decision is interpretable.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>A <strong>global explanation</strong> consists in providing a way for interpreting any possible decision of a <em>black box model</em>.
Generally, the black box behavior is approximated with a transparent model trained to mimic the obscure model (see <a class="reference internal" href="TAILOR.html#document-Transparency/blackbox_transparent"><span class="doc">Black Box Explanation vs Explanation by Design</span></a>) and also to be human-understandable. In other words, the interpretable model approximating the black box provides a global interpretation. Unfortunately, global explanations are quite hard to achieve and, up to now, can be provided only for AI working on relational data.</p>
<p>A <strong>local explanation</strong> consists in retrieving the reasons for a specific <em>outcome</em> returned by a black box model relatively to the decision for a certain instance.
In this case, it is not required to explain the whole logic underlying the AI, but a local explanation only provides the reason for the prediction on a specific input instance.
Hence, an interpretable model is used to approximate the black box behavior only in the “neighborhood” of the instance analyzed, i.e., with respect to similar instances only. The idea is that it is easier to approximate the AI with a simple and understandable model, in such a neighborhood. Regarding Figure <a class="reference internal" href="TAILOR.html#post-hoc"><span class="std std-numref">7</span></a> (top) (see <a class="reference internal" href="TAILOR.html#document-Transparency/blackbox_transparent"><span class="doc">Black Box Explanation vs Explanation by Design</span></a>), a <em>global</em> explanation method <em>f</em> uses many instances <em>X</em> over which the explanation is returned.</p>
<div class="figure align-center" id="global-tree">
<a class="reference internal image-reference" href="_images/global_tree.png"><img alt="_images/global_tree.png" src="_images/global_tree.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 9 </span><span class="caption-text">Global explanation example in the form of decision tree <span id="id1">[<a class="reference internal" href="TAILOR.html#id65">1</a>]</span>.</span><a class="headerlink" href="#global-tree" title="Permalink to this image">¶</a></p>
</div>
<p>Figure  <a class="reference internal" href="#global-tree"><span class="std std-numref">9</span></a> illustrates an example of a global explanation <em>e</em> obtained by a decision tree structure for a classifier recommending to play tennis or not.
The overall decision logic is captured by the tree that says that the classifier recommends playing tennis or not by first looking at the <em>Outlook</em> feature. If its values <em>Overcast</em>, then the prediction is “not to play”. Otherwise, if its value is <em>Sunny</em>, the classifier checks the <em>Humidity</em> feature and recommends “not to play” if the <em>Humidity</em> is <em>High</em> and “to play” if it is <em>Normal</em>.
The same reasoning applies to the other branch of the tree.
Again, with reference to Figure <a class="reference internal" href="TAILOR.html#post-hoc"><span class="std std-numref">7</span></a> (top) (see <a class="reference internal" href="TAILOR.html#document-Transparency/blackbox_transparent"><span class="doc">Black Box Explanation vs Explanation by Design</span></a>), a local explanation method <em>f</em> returns an explanation only for a single instance <em>x</em>.</p>
<p>Two examples of local explanations are given in the following. <!--shown in Figures  {numref}`{number} <local-rule>` - {numref}`{number} <local-feature>`. --></p>
<div class="figure align-center" id="local-rule">
<a class="reference internal image-reference" href="_images/local_rule.png"><img alt="_images/local_rule.png" src="_images/local_rule.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 10 </span><span class="caption-text">Local explanation example in the form of decision rule <span id="id2">[<a class="reference internal" href="TAILOR.html#id65">1</a>]</span>.</span><a class="headerlink" href="#local-rule" title="Permalink to this image">¶</a></p>
</div>
<p>The local rule-based explanation (Figure  <a class="reference internal" href="#local-rule"><span class="std std-numref">10</span></a>) <em>e</em> for a given record <em>x</em> says that the black box <em>b</em> suggested playing tennis because the <em>Outlook</em> is <em>Sunny</em> and the <em>Humidity</em> is <em>Normal</em>.</p>
<div class="figure align-center" id="local-feature">
<a class="reference internal image-reference" href="_images/local_feature.png"><img alt="_images/local_feature.png" src="_images/local_feature.png" style="width: 200px;" /></a>
<p class="caption"><span class="caption-number">Fig. 11 </span><span class="caption-text">Local explanation example in form of feature importance <span id="id3">[<a class="reference internal" href="TAILOR.html#id65">1</a>]</span>.</span><a class="headerlink" href="#local-feature" title="Permalink to this image">¶</a></p>
</div>
<p>On the other hand, the explanation <em>e</em> formed by features importance (Figure  <a class="reference internal" href="#local-feature"><span class="std std-numref">11</span></a>) says that the black box <em>b</em> suggested playing tennis because the <em>Outlook</em> has a large positive contribution, <em>Humidity</em> has a consistent negative contribution, and <em>Wind</em> has no contribution in the decision.</p>
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p id="id4"><dl class="citation">
<dt class="label" id="id59"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>,<a href="#id3">3</a>)</span></dt>
<dd><p>Riccardo Guidotti, Anna Monreale, Dino Pedreschi, and Fosca Giannotti. <em>Principles of Explainable Artificial Intelligence</em>. Springer International Publishing, 2021.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Guidotti, Monreale, Pedreschi, Giannotti. Principles of Explainable Artificial Intelligence. Springer International Publishing (2021)</em> by Francesca Pratesi and Riccardo Guidotti.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
<span id="document-Transparency/XAI"></span><div class="tex2jax_ignore mathjax_ignore section" id="explainable-ai">
<h4>Explainable AI<a class="headerlink" href="#explainable-ai" title="Permalink to this headline">¶</a></h4>
<!-- TODO: add resources or remove duplicate paragraph! -->
<div class="section" id="in-brief">
<h5>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p><strong>Explainable AI</strong> (often shortened to <strong>XAI</strong>) is one of the ethical dimensions that is studied in the <a href="https://tailor-network.eu/" target=_blank>TAILOR project</a>.
The origin of XAI dates back to the entering into force of the General Data Protection Regulation (GDPR).
The GDPR <span id="id1">[<a class="reference internal" href="#id8">1</a>]</span>, in its <a href="https://gdpr-info.eu/recitals/no-71/" target=_blank>Recital 71</a>, also mentions the right to explanation, as a suitable safeguard to ensure fair and transparent processing in respect of data subjects. It is defined as the right “to obtain an explanation of the decision reached after profiling”. <!-- TODO: add link to profiling --></p>
</div>
<div class="section" id="more-in-details">
<h5>More in details<a class="headerlink" href="#more-in-details" title="Permalink to this headline">¶</a></h5>
<p>According to the <a href="https://wayback.archive-it.org/12090/20201227221227/https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai" target=_blank>High-Level Expert Group on Artificial Intelligence - Ethics Guidelines for Trustworthy AI</a>, explainability topic is included in the broader <a class="reference internal" href="TAILOR.html#document-Transparency/Transparency"><span class="doc std std-doc">transparency</span></a> dimension. Explainability concerns the ability to explain both the technical processes of an AI system and the related human (e.g., application areas of a system).
This aspect is analyzed also in the <a class="reference internal" href="TAILOR.html#document-Transparency/blackbox_transparent"><span class="doc">Black Box Explanation vs Explanation by Design</span></a> entry. <!-- {doc}`./XAI_dimensions`--></p>
<p>The goal of this task is explaining how the decision system returned certain outcomes (the so-called <em>Black Box Explanation</em>). Moreover, the Black Box Explanation problem can be further divided among <em>Model Explanation</em> when the explanation involves the whole logic of the obscure classifier, <em>Outcome Explanation</em> when the target is to understand the reasons for the decisions on a given object, and <em>Model Inspection</em> when the target to understand how internally the black box behaves changing the input.</p>
<!-- TODO: prendere solo un pezzo della figura -->
<div class="figure align-center" id="t3-1taxonomy31">
<a class="reference internal image-reference" href="_images/xai_taxonomy.png"><img alt="_images/xai_taxonomy.png" src="_images/xai_taxonomy.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12 </span><span class="caption-text">A possible taxonomy about solutions to the Open the Black-Box problem <span id="id2">[<a class="reference internal" href="TAILOR.html#id17">4</a>]</span>.</span><a class="headerlink" href="#t3-1taxonomy31" title="Permalink to this image">¶</a></p>
</div>
<p>On a different dimension, a lot of effort has been put into defining what are the possible techniques (e.g., we can discriminate between <a class="reference internal" href="TAILOR.html#document-Transparency/model_specific"><span class="doc">Model-Specific vs Model-Agnostic Explainers</span></a>), what is the explained outcome (i.e., <a class="reference internal" href="TAILOR.html#document-Transparency/global_local"><span class="doc">Global vs Local Explanations</span></a>), and to understand the <a class="reference internal" href="TAILOR.html#document-Transparency/feature_importance"><span class="doc">Feature Importance</span></a>. Then, it is important to note that a variety of different kinds of explanations can be provided, such as <a class="reference internal" href="TAILOR.html#document-Transparency/single_tree"><span class="doc">Single Tree Approximation</span></a>, <a class="reference internal" href="TAILOR.html#document-Transparency/feature_importance"><span class="doc">Feature Importance</span></a>, <a class="reference internal" href="TAILOR.html#document-Transparency/saliency_maps"><span class="doc">Saliency Maps</span></a>, Factual and Counterfactual, Exemplars and Counter-Exemplars, and Rules List and Rules Sets.<!--[Factual and Counterfactual](./counterfactuals.md), exemplars and counter-exemplars, [Rules List and Rules Sets](./rules.md).--></p>
<!-- TODO: Togliere!!! -->
<p>On a different dimension, a lot of effort has been put into defining what are the possible techniques (e.g., we can discriminate between <a class="reference internal" href="TAILOR.html#document-Transparency/model_specific"><span class="doc">Model-Specific vs Model-Agnostic Explainers</span></a>), the requirements to provide good explanations (see <!-- TODO: add anchor {ref}`guidelines` --> guidelines), how to <!-- TODO: add link [evaluate explanations](./evaluating_explanations.md) --> evaluate explanations, and to understand the <a class="reference internal" href="TAILOR.html#document-Transparency/feature_importance"><span class="doc">Feature Importance</span></a>. Then, it is important to note that a variety of different kinds of explanations can be provided, such as <a class="reference internal" href="TAILOR.html#document-Transparency/single_tree"><span class="doc">Single Tree Approximation</span></a>, <a class="reference internal" href="TAILOR.html#document-Transparency/feature_importance"><span class="doc">Feature Importance</span></a>, <a class="reference internal" href="TAILOR.html#document-Transparency/saliency_maps"><span class="doc">Saliency Maps</span></a>, <a class="reference internal" href="TAILOR.html#document-Transparency/counterfactual"><span class="doc std std-doc">Factual and Counterfactual</span></a>, <a class="reference internal" href="TAILOR.html#document-Transparency/prototypes"><span class="doc std std-doc">exemplars and counter-exemplars</span></a>, <a class="reference internal" href="TAILOR.html#document-Transparency/rules"><span class="doc std std-doc">Rules List and Rules Sets</span></a>.</p>
<!--
Transparency
	Dimensions of Explanations
	        Black Box Explanation vs Explanation by Design
	        Model-Specific vs Model-Agnostic Explainers
	        Global vs Local Explanations
	XAI
		Kinds of (Post-Hoc) Explanations  cambiare il link a "Increasing research on XAI"
			Feature Importance
			Saliency Maps
			Single Tree Approximation
-->
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<!-- :style: unsrtalpha -->
<p id="id3"><dl class="citation">
<dt class="label" id="id8"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>European Parliament &amp; Council. General data protection regulation. 2016. L119, 4/5/2016, p. 1–88.</p>
</dd>
<dt class="label" id="id57"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and D. Pedreschi. A survey of methods for explaining black box models. <em>ACM computing surveys (CSUR)</em>, 2018.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Francesca Pratesi.</p>
</div></blockquote>
</div>
<div class="toctree-wrapper compound">
<span id="document-Transparency/XAI_kinds"></span><div class="tex2jax_ignore mathjax_ignore section" id="kinds-of-explanations">
<h5>Kinds of Explanations<a class="headerlink" href="#kinds-of-explanations" title="Permalink to this headline">¶</a></h5>
<div class="section" id="in-brief">
<h6>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p>Explanations returned by an AI system depend on various factors (such as the task or the available data); generally speaking, each kind of explanations serves better a specific context.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>Increasing research on <a class="reference internal" href="TAILOR.html#document-Transparency/Transparency"><span class="doc std std-doc">XAI</span></a> is bringing to light a wide list of explanations and explanation methods for “opening” black box models.
The explanations returned depend on various factors, such as:</p>
<ul class="simple">
<li><p>the type of task they are needed for,</p></li>
<li><p>on which kind of data the AI system acts,</p></li>
<li><p>who is the final user of the explanation,</p></li>
<li><p>if they allow to explain the whole behavior of the AI system (<a class="reference internal" href="TAILOR.html#document-Transparency/global_local"><span class="doc std std-doc">global explanations</span></a>) or reveal the reasons for the decision only for a particular instance (<a class="reference internal" href="TAILOR.html#document-Transparency/global_local"><span class="doc std std-doc">local explanations</span></a>),</p></li>
<li><p>the business perspective, i.e., which are the implication of companies in having explainable and interpretable systems and models, in terms of business strategies and secrecy,</p></li>
<li><p>the fact that, in a decentralized node, an explanation could require information that is nor directly available on site.</p></li>
</ul>
<p>In this part of the Encyclopedia, we review a subset of the most used types of explanations and show how some state-of-the-art explanation methods can return them. The interested reader can refer to <span id="id1">[<a class="reference internal" href="#id66">2</a>]</span>, <span id="id2">[<a class="reference internal" href="TAILOR.html#id17">4</a>]</span> for a complete review of XAI literature.</p>
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p id="id3"><dl class="citation">
<dt class="label" id="id57"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and D. Pedreschi. A survey of methods for explaining black box models. <em>ACM computing surveys (CSUR)</em>, 2018.</p>
</dd>
<dt class="label" id="id66"><span class="brackets"><a class="fn-backref" href="#id1">2</a></span></dt>
<dd><p>Amina Adadi and Mohammed Berrada. Peeking inside the black-box: a survey on explainable artificial intelligence (xai). <em>IEEE Access</em>, 6:52138–52160, 2018.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Guidotti, Monreale, Pedreschi, Giannotti. Principles of Explainable Artificial Intelligence. Springer International Publishing (2021)</em> by Francesca Pratesi and Riccardo Guidotti.</p>
</div></blockquote>
</div>
<div class="toctree-wrapper compound">
<span id="document-Transparency/counterfactual"></span><div class="tex2jax_ignore mathjax_ignore section" id="counterfactuals">
<h6>Counterfactuals<a class="headerlink" href="#counterfactuals" title="Permalink to this headline">¶</a></h6>
<!-- TODO: add resources or remove duplicate paragraph! -->
<div class="section" id="in-brief">
<h7>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h7>
<p>A <strong>counterfactual explanation</strong> shows what should have been different to change the decision of an AI system. For example, a counterfactual explanation could be a local explaination of a certain istance by providing the nearest istances that lead to a different decision or describing a small change in the input of the model that lead to a change in the outcome of the model.</p>
<!--
## More in details

A counterfactual explanation shows what should have been different to change the decision of an AI system. Counterfactual explanations are becoming an essential component in XAI methods and applications {cite}`apicella6` because they help people in reasoing on the cause-effect relations between analyzed instances and AI decision {cite}`byrne17`.
Indeed, while direct explanations such as features importance, decision rules, and saliency maps are important for understanding the reasons for a certain decision, a counterfactual explanation reveals what should be different in a given instance to obtain an alternative decision {cite}`wachter2017right_73`. Thinking in terms of “counterfactuals” requires the ability to figure a hypothetical causal situation that contradicts the observed one {cite}`molnar2020interpretable_50`.
The “cause” of the situation are the features describing the under investigation and that “caused” a certain decision, the "event" models the decision.

The most used types of counterfactual explanations are indeed prototype-based counterfactuals. In {cite}`wachter2017counterfactual_74`, counterfactual explanations are provided by an explanation method that solves an optimization problem that given an instance under analysis, a training dataset, and a black box function, returns an instance similar to the input one and with minimum changes, i.e., minimum distance, but that revert the black box outcome. The counterfactual explanation describes the smallest change that can be made in that particular case to obtain a different decision from the AI.
In {cite}`ustun2019actionable_72` the generation of diverse counterfactuals using mixed integer programming for linear models is proposed. As previously mentioned, <span style="font-variant:small-caps;">ABELE</span> {cite}`guidotti2019latent_31` also returns synthetic counter-exemplar images that highlights the similarities and differences between images leading to the same decision and images leading to other decisions.

Another modeling for counterfactual explanations consists of the logical form that describes a causal situation like: *"If X had not occurred, Y would not have occurred"* {cite}`molnar2020interpretable_50`. 
The local model-agnostic <span style="font-variant:small-caps;">LORE</span> explanation method {cite}`guidotti2019factual_30`, besides a factual explanation rule, also provides a set of counterfactual rules that illustrate tho logic used by the AI to obtain a different decision with minimum changes. For example, in Figure {numref}`{number} <fig:counterfactual>`, the set of counterfactual rules is highlighted in purple and shows that *if income > 900 then grant, or if race = white then grant*, clarifying which particular changes would revert the decision. In {cite}`laugel2017inverse_41`, authors proposed a local neighborhood generation method based on a Growing Spheres algorithm that can be used for both finding counterfactual instances, but also as a base for extracting counterfactual rules.


```{figure} ./counterfactual.png
---
name: fig:counterfactual
width: 600px
align: center
---
Example of local factual and counter-factual explanation returned by <span style="font-variant:small-caps;">LORE</span> {cite}`Guidotti2021`.
```


-->
</div>
<div class="section" id="bibliography">
<h7>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h7>
<!-- :style: unsrtalpha -->
<span class="target" id="id1"></span><blockquote>
<div><p>This entry was readapted from <em>Guidotti, Monreale, Pedreschi, Giannotti. Principles of Explainable Artificial Intelligence. Springer International Publishing (2021)</em> by Francesca Pratesi.</p>
</div></blockquote>
</div>
</div>
<span id="document-Transparency/feature_importance"></span><div class="tex2jax_ignore mathjax_ignore section" id="feature-importance">
<h6>Feature Importance<a class="headerlink" href="#feature-importance" title="Permalink to this headline">¶</a></h6>
<div class="section" id="in-brief">
<h7>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h7>
<p>The <strong>feature importance</strong> technique provides a score, representing the “importance”, for all the input features for a given AI model, i.e., a higher importance means that the corresponding feature will have a larger effect on the model.</p>
</div>
<div class="section" id="more-in-detail">
<h7>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h7>
<p><a class="reference internal" href="TAILOR.html#document-Transparency/global_local"><span class="doc std std-doc">Local explanations</span></a>, especially when dealing with tabular data, can also be returned in the form of <strong>features
importance</strong>. This technique considers both the sign and the magnitude of the contribution of the
features for a given AI decision. In particular, if the value of a feature is positive, then it contributes
by increasing the model’s output; at the same time, if the sign is negative, then the feature decreases
the output of the model.
At the same time, if a feature has a higher contribution than another, both positively and negatively, then it
means that it has a stronger influence on the prediction of the black box outcome. The magnitude of the provided score expresses this meaning.</p>
<p>The features importance summarizes the outcome of the black box model, allowing quantifying the changes of the black box decision for each test
record. This means it is possible to identify the features leading to a specific outcome for a certain instance and how much they contributed to the decision.
In the following, we provide a couple of examples obtained with two of the most popular methods, namely <span style="font-variant:small-caps;">lime</span> and <span style="font-variant:small-caps;">shap</span>, both <a class="reference internal" href="TAILOR.html#document-Transparency/model_specific"><span class="doc std std-doc">model-agnostic</span></a> <a class="reference internal" href="TAILOR.html#document-Transparency/global_local"><span class="doc std std-doc">local</span></a> explanation methods.</p>
<p>The <span style="font-variant:small-caps;">lime</span> explanation method <span id="id1">[<a class="reference internal" href="TAILOR.html#id19">2</a>]</span> randomly generates synthetic instances around the analyzed record. Then it returns the features importance as the coefficient of a linear regression model adopted as a local surrogate. The synthetic instances are weighted according to their proximity to the instance of interest.
The <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)" target=_blank>Lasso model</a> is trained to approximate the probability of the decision of the
black box in the synthetic neighborhood of the instance analyzed. Fig. <a class="reference internal" href="#xai-feature-lime"><span class="std std-numref">13</span></a> shows
the features importance returned by <span style="font-variant:small-caps;">lime</span> in a scenario where we want to predict if a mushroom is edible or poisonous. Indeed, here the prediction class has only two classes, and we want to understand which features are most relevant to discriminate between the two classes.
In this example, the feature <em>odor=foul</em> has a positive contribution of 0.26
in predicting of a mushroom as <em>poisonous</em>, <em>stack-surface-above-ring=silky</em> has
a positive contribution of 0.11, <em>spore-print-color=chocolate</em> has a positive contribution of 0.08, <em>stack-surface-below-ring=silky</em> has a positive contribution of 0.06,
while <em>gill-size=broad</em> has a negative contribution of 0.13.</p>
<div class="figure align-center" id="xai-feature-lime">
<a class="reference internal image-reference" href="_images/featureimportance_xai_tailor.png"><img alt="_images/featureimportance_xai_tailor.png" src="_images/featureimportance_xai_tailor.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">Fig. 13 </span><span class="caption-text">Example of explanation based on features importance by <span style="font-variant:small-caps;">lime</span> <span id="id2">[<a class="reference internal" href="TAILOR.html#id65">1</a>]</span>.</span><a class="headerlink" href="#xai-feature-lime" title="Permalink to this image">¶</a></p>
</div>
<p>The <span style="font-variant:small-caps;">shap</span> explanation method <span id="id3">[<a class="reference internal" href="#id114">3</a>]</span> connects game theory
with local explanations exploiting the <em>Shapely values</em> of a conditional expectation
function of the black box to explain the AI. Shapley values are introduced in <span id="id4">[<a class="reference internal" href="#id132">4</a>]</span>
with a method for assigning “payouts” to “players” depending on their contribution
to the “total payout”. Players cooperate in a coalition and receive a certain “profit”
from this cooperation. The connection with explainability is as follows. The “game”
is the decision of the black box for a specific instance, while the “profit” is the actual
value of the decision for this instance minus the average values for all instances. The
“players” are the feature values of the instance that leads towards a certain value,
i.e., collaborate to receive the profit.
Thus, a Shapley value is the <em>average marginal
contribution</em> of a feature value across all possible coalitions, i.e., combinations <span id="id5">[<a class="reference internal" href="TAILOR.html#id123">2</a>]</span>.
Therefore, <span style="font-variant:small-caps;">shap</span> returns the local unique additive feature importance for each specific
record. The higher a Shapely value, the higher the contribution of the feature.
Fig. <a class="reference internal" href="#xai-feature-shap"><span class="std std-numref">14</span></a> shows an example of <span style="font-variant:small-caps;">shap</span> explanation, where the features importance
is expressed in the form of a <em>force plot</em>. The example is based on the Boston Housing Dataset<a class="footnote-reference brackets" href="#boston" id="id6">1</a>, a dataset that collects information relative to a portion of US census data (such as the age) along with some information about the areas (i.e., pupil-teacher ratio by town or accessibility to radial highways). This explanation each feature’s contribution levelin pushing the black box prediction from the base value (the
average model output over the dataset, which is 24.41 in this example) to the model
output. The features pushing the prediction higher are shown in light blue, and those pushing
the prediction lower are shown in orange.</p>
<div class="figure align-center" id="xai-feature-shap">
<a class="reference internal image-reference" href="_images/featureimportance_xai_tailor2.png"><img alt="_images/featureimportance_xai_tailor2.png" src="_images/featureimportance_xai_tailor2.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">Fig. 14 </span><span class="caption-text">Example of explanation based on features importance by <span style="font-variant:small-caps;">shap</span> <span id="id7">[<a class="reference internal" href="TAILOR.html#id65">1</a>]</span>.</span><a class="headerlink" href="#xai-feature-shap" title="Permalink to this image">¶</a></p>
</div>
<p>Even if the classic application case of feature importance is with tabular data, it is important to note that, under appropriate settings, <span style="font-variant:small-caps;">lime</span> and <span style="font-variant:small-caps;">shap</span>
can also be used to explain the decisions of AI working on textual data and images.</p>
<!--Immagine feature importance public: https://commons.wikimedia.org/wiki/File:Wikireliability_original-research_feature-importance.png-->
<!-- blu re-identification attack = #00bfec
complementare = #ec4700
https://www.sessions.edu/color-calculator/
-->
</div>
<div class="section" id="bibliography">
<h7>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h7>
<p id="id8"><dl class="citation">
<dt class="label" id="id64"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>M.T. Ribeiro, S. Singh, and C. Guestrin. &quot;why should I trust you?&quot;: explaining the predictions of any classifier. In <em>SIGKDD</em>. 2016.</p>
</dd>
<dt class="label" id="id63"><span class="brackets">2</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id7">2</a>)</span></dt>
<dd><p>Riccardo Guidotti, Anna Monreale, Dino Pedreschi, and Fosca Giannotti. <em>Principles of Explainable Artificial Intelligence</em>. Springer International Publishing, 2021.</p>
</dd>
<dt class="label" id="id114"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>S. M. Lundberg and S.-I. Lee. A unified approach to interpreting model predictions. In <em>Advances in neural information processing systems</em>, 4765–4774. 2017.</p>
</dd>
<dt class="label" id="id132"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Lloyd S Shapley. A value for n-person games. <em>Contributions to the Theory of Games</em>, 2(28):307–317, 1953.</p>
</dd>
<dt class="label" id="id119"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>Christoph Molnar. <em>Interpretable Machine Learning</em>. Lulu.com, 2020.</p>
</dd>
</dl>
</p>
<hr class="docutils" />
<p>This entry was readapted from <em>Guidotti, Monreale, Pedreschi, Giannotti. Principles of Explainable Artificial Intelligence. Springer International Publishing (2021)</em> by Francesca Pratesi and Riccardo Guidotti.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="boston"><span class="brackets"><a class="fn-backref" href="#id6">1</a></span></dt>
<dd><p><a class="reference external" href="https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html">https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html</a></p>
</dd>
</dl>
</div>
</div>
<span id="document-Transparency/lore"></span><div class="tex2jax_ignore mathjax_ignore section" id="local-rule-based-explanation">
<h6>Local Rule-based Explanation<a class="headerlink" href="#local-rule-based-explanation" title="Permalink to this headline">¶</a></h6>
<!-- TODO: add resources or remove duplicate paragraph! -->
<div class="section" id="in-brief">
<h7>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h7>
<p><strong>Local Rule-based Explanation</strong> aims to extract a rule that provides a local explaination for a certain instance of the model.</p>
</div>
<div class="section" id="more-in-details">
<h7>More in details<a class="headerlink" href="#more-in-details" title="Permalink to this headline">¶</a></h7>
<p>Despite being useful, global explanations can be inaccurate because interpreting a whole model can be complex. On the other hand, even though the overall decision boundary is difficult to explain, locally, in the surrounding of a specific instance, it can be easier. Therefore, a local explanation rule can reveal the <em>factual</em> reasons for the decision taken by the black box of an AI system for a specific instance. The  <span style="font-variant:small-caps;">LORE</span> method is able to return a local rule-based explanation.  <span style="font-variant:small-caps;">LORE</span> builds a local decision tree in the neighborhood of the instance analyzed <span id="id1">[<a class="reference internal" href="TAILOR.html#id111">3</a>]</span> generated with a genetic procedure to account for both similarity and differences with the instance analyzed.
Then, it extracts from the tree a local rule revealing the reasons for the decision on the specific instance (see the green path in Figure <a class="reference internal" href="TAILOR.html#fig-counterfactual"><span class="std std-numref">17</span></a>). For instance, the explanation of  <span style="font-variant:small-caps;">LORE</span> for the denied request of a loan from a customer with <br>”age=22, race=black, and income=800”<br> to a bank that uses an AI could be the factual rule <br><em>if age ≤ 25 and race = black and income ≤ 900 then deny</em>.</p>
<p><span style="font-variant:small-caps;">anchor</span> <span id="id2">[<a class="reference internal" href="TAILOR.html#id139">4</a>]</span> is another XAI approach for locally explaining black box models with decision rules called anchors. An <em>anchor</em> is a set of features with thresholds indicating the values which are fundamental for obtaining a certain decision of the AI. anchor efficiently explores the black box behavior by generating random instances exploiting a multi-armed bandit formulation.</p>
<div class="figure align-center" id="fig-counterfactual">
<a class="reference internal image-reference" href="_images/counterfactual.png"><img alt="_images/counterfactual.png" src="_images/counterfactual.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 15 </span><span class="caption-text">Example of local factual and counter-factual explanation returned by <span style="font-variant:small-caps;">LORE</span> <span id="id3">[<a class="reference internal" href="TAILOR.html#id65">1</a>]</span>.</span><a class="headerlink" href="#fig-counterfactual" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="bibliography">
<h7>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h7>
<!-- :style: unsrtalpha -->
<p id="id4"><dl class="citation">
<dt class="label" id="id59"><span class="brackets"><a class="fn-backref" href="#id3">1</a></span></dt>
<dd><p>Riccardo Guidotti, Anna Monreale, Dino Pedreschi, and Fosca Giannotti. <em>Principles of Explainable Artificial Intelligence</em>. Springer International Publishing, 2021.</p>
</dd>
<dt class="label" id="id95"><span class="brackets"><a class="fn-backref" href="#id1">2</a></span></dt>
<dd><p>Riccardo Guidotti, Anna Monreale, Fosca Giannotti, Dino Pedreschi, Salvatore Ruggieri, and Franco Turini. Factual and counterfactual explanations for black box decision making. <em>IEEE Intelligent Systems</em>, 34(6):14–23, 2019.</p>
</dd>
<dt class="label" id="id123"><span class="brackets"><a class="fn-backref" href="#id2">3</a></span></dt>
<dd><p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: high-precision model-agnostic explanations. In <em>Thirty-Second AAAI Conference on Artificial Intelligence</em>. 2018.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Guidotti, Monreale, Pedreschi, Giannotti. Principles of Explainable Artificial Intelligence. Springer International Publishing (2021)</em> by Francesca Pratesi.</p>
</div></blockquote>
</div>
</div>
<span id="document-Transparency/prototypes"></span><div class="tex2jax_ignore mathjax_ignore section" id="prototypes">
<h6>Prototypes<a class="headerlink" href="#prototypes" title="Permalink to this headline">¶</a></h6>
<p><em>Synonyms:</em> Exemplars.</p>
<!-- TODO: add resources or remove duplicate paragraph! -->
<div class="section" id="in-brief">
<h7>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h7>
<p><strong>Prototype-based explanation</strong> methods return as explanation a selection of particular instances of the dataset for locally explaining the behavior of the AI system <span id="id1">[<a class="reference internal" href="#id123">2</a>]</span>.</p>
</div>
<div class="section" id="more-in-details">
<h7>More in details<a class="headerlink" href="#more-in-details" title="Permalink to this headline">¶</a></h7>
<p>Prototypes (or exemplars) make clear to the user the reasons for the AI system’s decision. In other words, prototypes are used as a foundation of representation of a category, or a concept <span id="id2">[<a class="reference internal" href="#id99">3</a>]</span>. A concept is represented through a specific instance. Prototypes help humans in constructing mental models of the black box model and of the training data used. Prototype-based explainers are generally local methods that can be used independently for tabular data, images, and text. Obviously, prototype-based explanations only make sense if an instance of the data is humanly understandable and makes sense as an explanation. Hence, these methods are particularly useful for images, short texts, and tabular data with few features.</p>
<p>In <span id="id3">[<a class="reference internal" href="#id86">4</a>]</span>, prototypes are selected as a minimal subset of samples from the training data that can serve as a condensed view of a data set. Naive approaches for selecting prototypes use the closest neighbors from the training data with respect to a predefined distance function, or the closest centroids returned by a clustering algorithm <span id="id4">[<a class="reference internal" href="#id143">5</a>]</span>. In <span id="id5">[<a class="reference internal" href="#id116">6</a>]</span>, authors designed a sophisticated model-specific explanation method that directly encapsulates in a deep neural network architecture an autoencoder and a special prototype layer, where each unit of that layer stores a weight vector that resembles an encoded training input. The autoencoder permits to make comparisons within the latent space and to visualize the learned prototypes such that, besides accuracy and reconstruction error, the optimization has a term that ensures that every encoded input is close to at least one prototype. Thus, the distances in the prototype layer are used for the classification such that each prediction comes with an explanation corresponding to the closest prototype. In <span id="id6">[<a class="reference internal" href="#id91">7</a>]</span>, prototypical parts of images are extracted by a ProtoPNet network, such that each classification is driven by prototypical aspects of a class.</p>
<p>Although prototypes are representative of all the data, sometimes they are not enough to provide evidence for the classification without instances that are not well represented by the set of prototypes <span id="id7">[<a class="reference internal" href="#id123">2</a>]</span>. These instances are named criticisms and help to explain what is not captured by prototypes. In order to aid human interpretability, in <span id="id8">[<a class="reference internal" href="#id109">8</a>]</span> prototypes and criticisms are selected by means of the Maximum Mean Discrepancy (mmd): instances in highly dense areas are good prototypes, instances which are in regions that are not well explained by the prototypes are selected as criticisms. Finally, the abele method <span id="id9">[<a class="reference internal" href="TAILOR.html#id112">5</a>]</span> enforces the saliency map explanation with a set of exemplar and counter-exemplar images, i.e., images similar to the one under investigation classified for which the same decision is taken by the AI, and images similar to the one explained for which the black box of the AI returns a different decision. The particularity of abele is that it does not select exemplars and counter-exemplars from the training set, but it generates them synthetically exploiting an adversarial autoencoder used during the explanation process <span id="id10">[<a class="reference internal" href="#id113">10</a>]</span>. An example of exemplars (left) and counter-exemplars (right) is shown in Figure {numref}`{number} <a class="reference external" href="fig:prototypes">fig:prototypes</a>.</p>
<div class="figure align-center" id="fig-prototypes">
<a class="reference internal image-reference" href="_images/prototypes.png"><img alt="_images/prototypes.png" src="_images/prototypes.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 16 </span><span class="caption-text">Example of exemplars (left) and counter-exemplars (right) returned by <span style="font-variant:small-caps;">abele</span>. On top of each (counter-)exemplar is reported the label assigned by the black box model <em>b</em> of the AI system <span id="id11">[<a class="reference internal" href="TAILOR.html#id65">1</a>]</span>.</span><a class="headerlink" href="#fig-prototypes" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="bibliography">
<h7>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h7>
<!-- :style: unsrtalpha -->
<p id="id12"><dl class="citation">
<dt class="label" id="id67"><span class="brackets"><a class="fn-backref" href="#id11">1</a></span></dt>
<dd><p>Riccardo Guidotti, Anna Monreale, Dino Pedreschi, and Fosca Giannotti. <em>Principles of Explainable Artificial Intelligence</em>. Springer International Publishing, 2021.</p>
</dd>
<dt class="label" id="id123"><span class="brackets">2</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id7">2</a>)</span></dt>
<dd><p>Christoph Molnar. <em>Interpretable Machine Learning</em>. Lulu.com, 2020.</p>
</dd>
<dt class="label" id="id99"><span class="brackets"><a class="fn-backref" href="#id2">3</a></span></dt>
<dd><p>Marcello Frixione and Antonio Lieto. Prototypes vs exemplars in concept representation. In <em>KEOD</em>, 226–232. 2012.</p>
</dd>
<dt class="label" id="id86"><span class="brackets"><a class="fn-backref" href="#id3">4</a></span></dt>
<dd><p>Jacob Bien and Robert Tibshirani. Prototype selection for interpretable classification. <em>The Annals of Applied Statistics</em>, pages 2403–2424, 2011.</p>
</dd>
<dt class="label" id="id143"><span class="brackets"><a class="fn-backref" href="#id4">5</a></span></dt>
<dd><p>Pang-Ning Tan and others. <em>Introduction to data mining</em>. Pearson Education India, 2006.</p>
</dd>
<dt class="label" id="id116"><span class="brackets"><a class="fn-backref" href="#id5">6</a></span></dt>
<dd><p>Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep learning for case-based reasoning through prototypes: a neural network that explains its predictions. In <em>Thirty-second AAAI conference on artificial intelligence</em>. 2018.</p>
</dd>
<dt class="label" id="id91"><span class="brackets"><a class="fn-backref" href="#id6">7</a></span></dt>
<dd><p>Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su. This looks like that: deep learning for interpretable image recognition. In <em>Advances in neural information processing systems</em>, 8930–8941. 2019.</p>
</dd>
<dt class="label" id="id109"><span class="brackets"><a class="fn-backref" href="#id8">8</a></span></dt>
<dd><p>Been Kim, Rajiv Khanna, and Oluwasanmi O Koyejo. Examples are not enough, learn to criticize! criticism for interpretability. In <em>Advances in neural information processing systems</em>, 2280–2288. 2016.</p>
</dd>
<dt class="label" id="id104"><span class="brackets"><a class="fn-backref" href="#id9">9</a></span></dt>
<dd><p>Riccardo Guidotti, Anna Monreale, Stan Matwin, and Dino Pedreschi. Black box explanation by learning image exemplars in the latent feature space. In <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>, 189–205. Springer, 2019.</p>
</dd>
<dt class="label" id="id113"><span class="brackets"><a class="fn-backref" href="#id10">10</a></span></dt>
<dd><p>Orestis Lampridis, Riccardo Guidotti, and Salvatore Ruggieri. Explaining sentiment classification with synthetic exemplars and counter-exemplars. In <em>International Conference on Discovery Science</em>, 357–373. Springer, 2020.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Guidotti, Monreale, Pedreschi, Giannotti. Principles of Explainable Artificial Intelligence. Springer International Publishing (2021)</em> by Francesca Pratesi.</p>
</div></blockquote>
</div>
</div>
<span id="document-Transparency/rules"></span><div class="tex2jax_ignore mathjax_ignore section" id="rules-list-and-rules-set">
<h6>Rules List and Rules Set<a class="headerlink" href="#rules-list-and-rules-set" title="Permalink to this headline">¶</a></h6>
<!-- TODO: add resources or remove duplicate paragraph! -->
<div class="section" id="in-brief">
<h7>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h7>
<p>A <strong>decision rule</strong> is generally formed by a set of conditions and by a consequent, e.g., <em>if conditions, then consequent</em>.</p>
</div>
<div class="section" id="more-in-details">
<h7>More in details<a class="headerlink" href="#more-in-details" title="Permalink to this headline">¶</a></h7>
<p>Given a record, a decision rule assigns to the record the outcome specified in the consequent if the conditions are verified <span id="id1">[<a class="reference internal" href="#id75">2</a>]</span>. The most common rules are <em>if-then rules</em> that take into consideration rules with clauses in conjunction. On the other hand, for <em>m-of-n rules</em> given a set of <em>n</em> conditions, if <em>m</em> of them are verified, then the consequence of the rule is applied <span id="id2">[<a class="reference internal" href="#id123">3</a>]</span>. When a set of rules is used, then there are different strategies to select the outcome.</p>
<p>For <em>list of rules</em> the order of the list is considered and the model returns the outcome of the first rule that verifies the conditions <span id="id3">[<a class="reference internal" href="#id147">4</a>]</span>. For instance, <em>falling rule lists</em> are if-then rules ordered with respect to the probability of a specific outcome <span id="id4">[<a class="reference internal" href="#id146">5</a>]</span>. On the other hand, <em>decision sets</em> are unordered lists of rules. Basically each rule is an independent classifier that can assign its outcome without regard for any other rules <span id="id5">[<a class="reference internal" href="#id111">6</a>]</span>. Voting strategies are used to select the final outcome.</p>
<p>List of rules and set of rules are adopted as explanation both from explanation methods but also from transparent classifiers. In both cases the reference context is tabular data. In [8] the explanation method <span style="font-variant:small-caps;">rxren</span> unveil with rules list the logic behind a trained neural network. First, <span style="font-variant:small-caps;">rxren</span> prunes the insignificant input neurons and identifies the data range necessary to classify the given test instance with a specific class. Second, <span style="font-variant:small-caps;">rxren</span> generates the classification rules for each class label exploiting the data ranges previously identified, and improve the initial list of rules by a process that prunes and updates the rules. Figure 5 shows an example of rules returned by <span style="font-variant:small-caps;">rxren</span>. A survey on techniques extracting rules from neural networks is <span id="id6">[<a class="reference internal" href="#id77">7</a>]</span>. All the approaches in <span id="id7">[<a class="reference internal" href="#id77">7</a>]</span>, including <span style="font-variant:small-caps;">rxren</span> are model-specific explainers.</p>
<p>As previously mentioned, an alternative line of research to black box explanation is the design of transparent models for the AI systems. The <span style="font-variant:small-caps;">corels</span> method [5] is a technique for building rule lists for discretized tabular datasets. <span style="font-variant:small-caps;">corels</span> provides an optimal and certifiable solution in terms of rule lists. An example of rules list returned by <span style="font-variant:small-caps;">corels</span> is reported in Figure 6. The rules are read one after the other, and the AI would take the decision of the first rule for which the conditions are verified. Decision sets are built by the method presented in <span id="id8">[<a class="reference internal" href="#id111">6</a>]</span>. The if-then rules extracted for each set are accurate, non-overlapping, and independent. Since each rule is independently applicable, decision sets are simple, succinct, and easily to be interpreted. A decision set is extracted by jointly maximizing the interpretability and predictive accuracy by means of a two step approach using frequent itemset mining and a learning method to select the rules. The method proposed in <span id="id9">[<a class="reference internal" href="#id134">8</a>]</span> merges local explanation rules into a unique global weighted rule list by using a scoring system.</p>
<div class="figure align-center" id="fig-counterfactual">
<a class="reference internal image-reference" href="_images/counterfactual.png"><img alt="_images/counterfactual.png" src="_images/counterfactual.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 17 </span><span class="caption-text">Example of local factual and counter-factual explanation returned by <span style="font-variant:small-caps;">rxren</span> <span id="id10">[<a class="reference internal" href="TAILOR.html#id65">1</a>]</span>.</span><a class="headerlink" href="#fig-counterfactual" title="Permalink to this image">¶</a></p>
</div>
<!--
<span style="font-variant:small-caps;">corels</span>
-->
</div>
<div class="section" id="bibliography">
<h7>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h7>
<!-- :style: unsrtalpha -->
<p id="id11"><dl class="citation">
<dt class="label" id="id66"><span class="brackets"><a class="fn-backref" href="#id10">1</a></span></dt>
<dd><p>Riccardo Guidotti, Anna Monreale, Dino Pedreschi, and Fosca Giannotti. <em>Principles of Explainable Artificial Intelligence</em>. Springer International Publishing, 2021.</p>
</dd>
<dt class="label" id="id75"><span class="brackets"><a class="fn-backref" href="#id1">2</a></span></dt>
<dd><p>R. Agrawal and R. Srikant. Fast algorithms for mining association rules. In <em>International Conference Very Large Data Bases (VLDB)</em>, volume 1215, 487–499. 1994.</p>
</dd>
<dt class="label" id="id123"><span class="brackets"><a class="fn-backref" href="#id2">3</a></span></dt>
<dd><p>Patrick M Murphy and Michael J Pazzani. ID2-of-3: Constructive induction of M-of-N concepts for discriminators in decision trees. In <em>Machine Learning Proceedings 1991</em>, pages 183–187. Elsevier, 1991.</p>
</dd>
<dt class="label" id="id147"><span class="brackets"><a class="fn-backref" href="#id3">4</a></span></dt>
<dd><p>Xiaoxin Yin and Jiawei Han. Cpar: classification based on predictive association rules. In <em>Proceedings of the 2003 SIAM International Conference on Data Mining</em>, 331–335. SIAM, 2003.</p>
</dd>
<dt class="label" id="id146"><span class="brackets"><a class="fn-backref" href="#id4">5</a></span></dt>
<dd><p>Fulton Wang and Cynthia Rudin. Falling rule lists. In <em>Artificial Intelligence and Statistics</em>, 1013–1022. 2015.</p>
</dd>
<dt class="label" id="id111"><span class="brackets">6</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id8">2</a>)</span></dt>
<dd><p>Himabindu Lakkaraju, Stephen H Bach, and Jure Leskovec. Interpretable decision sets: a joint framework for description and prediction. In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 1675–1684. ACM, 2016.</p>
</dd>
<dt class="label" id="id77"><span class="brackets">7</span><span class="fn-backref">(<a href="#id6">1</a>,<a href="#id7">2</a>)</span></dt>
<dd><p>R. Andrews, J. Diederich, and A. B. Tickle. Survey and critique of techniques for extracting rules from trained artificial neural networks. <em>Knowledge-based systems</em>, 1995.</p>
</dd>
<dt class="label" id="id134"><span class="brackets"><a class="fn-backref" href="#id9">8</a></span></dt>
<dd><p>M. Setzu, R. Guidotti, A. Monreale, and F. Turini. Global explanations with local scoring. In <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>, 159–171. 2019.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Guidotti, Monreale, Pedreschi, Giannotti. Principles of Explainable Artificial Intelligence. Springer International Publishing (2021)</em> by Francesca Pratesi.</p>
</div></blockquote>
</div>
</div>
<span id="document-Transparency/saliency_maps"></span><div class="tex2jax_ignore mathjax_ignore section" id="saliency-maps">
<h6>Saliency Maps<a class="headerlink" href="#saliency-maps" title="Permalink to this headline">¶</a></h6>
<div class="section" id="in-brief">
<h7>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h7>
<p>Saliency maps are explanations used on image classification tasks. A <strong>saliency map</strong> is an image where each pixel’s color represents a value modeling the importance of that pixel in the original image (i.e., the one given in input to the explainer) for the prediction.</p>
</div>
<div class="section" id="more-in-detail">
<h7>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h7>
<p>The most used type of explanation for explaining AI systems working on images consists of <strong>saliency maps</strong>. A saliency map is an image where each pixel’s color represents a value modeling the importance of that pixel for the prediction, i.e., they show the positive (or negative) contribution of each pixel to the black box
outcome. Saliency maps are a very typical example of <a class="reference internal" href="TAILOR.html#document-Transparency/global_local"><span class="doc std std-doc">local explanation methods</span></a> since they are tailored to the image that must be explained.</p>
<p>In the literature, there exist different explanation methods locally explaining deep neural networks for image classification. The two most used model-specific techniques are <em>perturbation-based attribution methods</em> <span id="id1">[<a class="reference internal" href="#id104">6</a>, <a class="reference internal" href="#id159">7</a>]</span> and
<em>gradient attribution methods</em> such as <span style="font-variant:small-caps;">sal</span> <span id="id2">[<a class="reference internal" href="#id147">8</a>]</span>, <span style="font-variant:small-caps;">elrp</span> <span id="id3">[<a class="reference internal" href="#id90">9</a>]</span>, <span style="font-variant:small-caps;">grad</span> <span id="id4">[<a class="reference internal" href="#id146">10</a>]</span>, and <span style="font-variant:small-caps;">intg</span> <span id="id5">[<a class="reference internal" href="#id149">11</a>]</span>.</p>
<p>Without entering into the details, these XAI approaches aim at attributing an importance score to each pixel in order to minimize the probability of the deep neural network (DNN) labeling the image with a different outcome when only the most important pixels are considered. Indeed, the areas retrieved by these methods are also called <em>attention areas</em>.</p>
<p>The aforementioned XAI methods are specifically designed for specific DNN models, i.e., they are <a class="reference internal" href="TAILOR.html#document-Transparency/model_specific"><span class="doc std std-doc">model-specific</span></a>.</p>
<p>However, relying on appropriate image transformations that take advantage of the concept of “superpixels” <span id="id6">[<a class="reference internal" href="TAILOR.html#id19">2</a>]</span>, i.e., the results of the segmentation of an image into regions by considering proximity or similarity measures, also <a class="reference internal" href="TAILOR.html#document-Transparency/model_specific"><span class="doc std std-doc">model-agnostic explanation</span></a> methods (such as <span style="font-variant:small-caps;">lime</span> <span id="id7">[<a class="reference internal" href="TAILOR.html#id19">2</a>]</span>, <span style="font-variant:small-caps;">anchor</span> <span id="id8">[<a class="reference internal" href="#id139">4</a>]</span>, and <span style="font-variant:small-caps;">lore</span> <span id="id9">[<a class="reference internal" href="#id160">12</a>]</span>) can be employed to explain AI working on images for any kind of black box model.</p>
<p>The attention areas of explanations returned by these methods are strictly dependent on both:</p>
<ul class="simple">
<li><p>the technique used for segmenting the image to explain and</p></li>
<li><p>to a neighborhood consisting of unrealistic synthetic images with “suppressed”
superpixels <span id="id10">[<a class="reference internal" href="#id110">13</a>]</span>.</p></li>
</ul>
<p>A different approach for generating neighborhoods is introduced by the <a class="reference internal" href="TAILOR.html#document-Transparency/global_local"><span class="doc std std-doc">local</span></a> <a class="reference internal" href="TAILOR.html#document-Transparency/model_specific"><span class="doc std std-doc">model-agostic</span></a> explanation method
<span style="font-variant:small-caps;">abele</span> <span id="id11">[<a class="reference internal" href="#id112">5</a>]</span>. This method relies on a generative model, i.e., an adversarial autoencoder <span id="id12">[<a class="reference internal" href="#id127">14</a>]</span>, to produce a realistic synthetic neighborhood that allows retrieving more understandable saliency maps.
Indeed, saliency maps returned by <span style="font-variant:small-caps;">abele</span> highlight the contiguous attention
areas that can be varied while maintaining the same classification from the black
box used by the AI system.</p>
<p>Fig. <a class="reference internal" href="#xai-saliency"><span class="std std-numref">18</span></a> reports a comparison of saliency maps for
the classification of the handwritten digits “9” and “0” for the explanation methods
<span style="font-variant:small-caps;">abele</span> <span id="id13">[<a class="reference internal" href="#id111">3</a>, <a class="reference internal" href="#id112">5</a>]</span>, <span style="font-variant:small-caps;">lime</span> <span id="id14">[<a class="reference internal" href="TAILOR.html#id19">2</a>]</span>, <span style="font-variant:small-caps;">sal</span> <span id="id15">[<a class="reference internal" href="#id147">8</a>]</span>, <span style="font-variant:small-caps;">elrp</span> <span id="id16">[<a class="reference internal" href="#id90">9</a>]</span>, <span style="font-variant:small-caps;">grad</span> <span id="id17">[<a class="reference internal" href="#id146">10</a>]</span>, and <span style="font-variant:small-caps;">intg</span> <span id="id18">[<a class="reference internal" href="#id149">11</a>]</span>.</p>
<div class="figure align-center" id="xai-saliency">
<a class="reference internal image-reference" href="_images/saliency.png"><img alt="_images/saliency.png" src="_images/saliency.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 18 </span><span class="caption-text">Example of saliency maps returned by different explanation methods. The first column contains the image analyzed and the label assigned by the black box model <em>b</em> of the AI system. <span id="id19">[<a class="reference internal" href="TAILOR.html#id65">1</a>]</span>.</span><a class="headerlink" href="#xai-saliency" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="bibliography">
<h7>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h7>
<p id="id20"><dl class="citation">
<dt class="label" id="id76"><span class="brackets">1</span><span class="fn-backref">(<a href="#id6">1</a>,<a href="#id7">2</a>,<a href="#id14">3</a>)</span></dt>
<dd><p>M.T. Ribeiro, S. Singh, and C. Guestrin. &quot;why should I trust you?&quot;: explaining the predictions of any classifier. In <em>SIGKDD</em>. 2016.</p>
</dd>
<dt class="label" id="id75"><span class="brackets"><a class="fn-backref" href="#id19">2</a></span></dt>
<dd><p>Riccardo Guidotti, Anna Monreale, Dino Pedreschi, and Fosca Giannotti. <em>Principles of Explainable Artificial Intelligence</em>. Springer International Publishing, 2021.</p>
</dd>
<dt class="label" id="id111"><span class="brackets"><a class="fn-backref" href="#id13">3</a></span></dt>
<dd><p>Riccardo Guidotti, Anna Monreale, Fosca Giannotti, Dino Pedreschi, Salvatore Ruggieri, and Franco Turini. Factual and counterfactual explanations for black box decision making. <em>IEEE Intelligent Systems</em>, 34(6):14–23, 2019.</p>
</dd>
<dt class="label" id="id139"><span class="brackets"><a class="fn-backref" href="#id8">4</a></span></dt>
<dd><p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: high-precision model-agnostic explanations. In <em>Thirty-Second AAAI Conference on Artificial Intelligence</em>. 2018.</p>
</dd>
<dt class="label" id="id112"><span class="brackets">5</span><span class="fn-backref">(<a href="#id11">1</a>,<a href="#id13">2</a>)</span></dt>
<dd><p>Riccardo Guidotti, Anna Monreale, Stan Matwin, and Dino Pedreschi. Black box explanation by learning image exemplars in the latent feature space. In <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>, 189–205. Springer, 2019.</p>
</dd>
<dt class="label" id="id104"><span class="brackets"><a class="fn-backref" href="#id1">6</a></span></dt>
<dd><p>Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In <em>Proceedings of the IEEE International Conference on Computer Vision</em>, 3429–3437. 2017.</p>
</dd>
<dt class="label" id="id159"><span class="brackets"><a class="fn-backref" href="#id1">7</a></span></dt>
<dd><p>Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In <em>European conference on computer vision</em>, 818–833. Springer, 2014.</p>
</dd>
<dt class="label" id="id147"><span class="brackets">8</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id15">2</a>)</span></dt>
<dd><p>Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: visualising image classification models and saliency maps. <em>arXiv preprint arXiv:1312.6034</em>, 2013.</p>
</dd>
<dt class="label" id="id90"><span class="brackets">9</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id16">2</a>)</span></dt>
<dd><p>Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. <em>PloS one</em>, 10(7):e0130140, 2015.</p>
</dd>
<dt class="label" id="id146"><span class="brackets">10</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id17">2</a>)</span></dt>
<dd><p>Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just a black box: learning important features through propagating activation differences. <em>arXiv preprint arXiv:1605.01713</em>, 2016.</p>
</dd>
<dt class="label" id="id149"><span class="brackets">11</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id18">2</a>)</span></dt>
<dd><p>Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. <em>arXiv preprint arXiv:1703.01365</em>, 2017.</p>
</dd>
<dt class="label" id="id160"><span class="brackets"><a class="fn-backref" href="#id9">12</a></span></dt>
<dd><p>Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Dino Pedreschi, Franco Turini, and Fosca Giannotti. Local rule-based explanations of black box decision systems. 2018. https://arxiv.org/abs/1805.10820.</p>
</dd>
<dt class="label" id="id110"><span class="brackets"><a class="fn-backref" href="#id10">13</a></span></dt>
<dd><p>Riccardo Guidotti, Anna Monreale, and Leonardo Cariaggi. Investigating neighborhood generation methods for explanations of obscure image classifiers. In <em>Pacific-Asia Conference on Knowledge Discovery and Data Mining</em>, 55–68. Springer, 2019.</p>
</dd>
<dt class="label" id="id127"><span class="brackets"><a class="fn-backref" href="#id12">14</a></span></dt>
<dd><p>Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. <em>arXiv preprint arXiv:1511.05644</em>, 2015.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Guidotti, Monreale, Pedreschi, Giannotti. Principles of Explainable Artificial Intelligence. Springer International Publishing (2021)</em> by Francesca Pratesi and Riccardo Guidotti.</p>
</div></blockquote>
</div>
</div>
<span id="document-Transparency/single_tree"></span><div class="tex2jax_ignore mathjax_ignore section" id="single-tree-approximation">
<h6>Single Tree Approximation<a class="headerlink" href="#single-tree-approximation" title="Permalink to this headline">¶</a></h6>
<div class="section" id="in-brief">
<h7>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h7>
<p>The <strong>single tree appoximation</strong> is an approach that aims at building a decision tree to approximate the behavior of a black box, typically a neural network.</p>
</div>
<div class="section" id="more-in-detail">
<h7>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h7>
<p>Decision trees are the simplest example of <a class="reference internal" href="TAILOR.html#document-Transparency/blackbox_transparent"><span class="doc std std-doc">transparent techniques</span></a>. Moreover, they can be built to provide <a class="reference internal" href="TAILOR.html#document-Transparency/blackbox_transparent"><span class="doc std std-doc">post-hoc explanations</span></a> of black-boxes.
One of the first approaches introduced to explain neural networks is <span style="font-variant:small-caps;">trepan</span> <span id="id1">[<a class="reference internal" href="#id91">3</a>]</span>.
<span style="font-variant:small-caps;">trepan</span> is a <a class="reference internal" href="TAILOR.html#document-Transparency/global_local"><span class="doc std std-doc">global explanation method</span></a> that is able to model the whole logic of a
neural network working on tabular data with a <strong>single decision tree</strong> <span id="id2">[<a class="reference internal" href="#id14">2</a>]</span>. The decision tree
returned by <span style="font-variant:small-caps;">trepan</span> as an explanation is a <em>global transparent surrogate</em>. Indeed, every
path from the root of the tree to a leaf explains the reasons for the final decision that is
reported in the leaf itself. An example of a decision tree returned by <span style="font-variant:small-caps;">trepan</span> is illustrated
in Fig. <a class="reference internal" href="#xai-tree"><span class="std std-numref">19</span></a>.</p>
<div class="figure align-center" id="xai-tree">
<a class="reference internal image-reference" href="_images/tree.png"><img alt="_images/tree.png" src="_images/tree.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 19 </span><span class="caption-text">Example of global tree-based explanation returned by <span style="font-variant:small-caps;">trepan</span> <span id="id3">[<a class="reference internal" href="#id65">1</a>]</span>.</span><a class="headerlink" href="#xai-tree" title="Permalink to this image">¶</a></p>
</div>
<p>This global explanation reveals that the black box first focuses on the
value of the feature <em>rest ECG</em>; Depending on its degree (abnormal, normal,
hypertrophy), tha black box takes different decisions depending on additional factors such as sex or
cholesterol.
In particular, <span style="font-variant:small-caps;">trepan</span> queries the neural network to induce the decision
tree by maximizing the <em>gain ratio</em> <span id="id4">[<a class="reference internal" href="#id14">2</a>]</span> on the data with respect to the neural network’s predictions.</p>
<p>A weakness of common decision trees like ID3 or C4.5 <span id="id5">[<a class="reference internal" href="#id128">4</a>]</span> is
that the amount of data to find the splits near to the leaves is much lower than those
used initially.
Thus, in order to retrieve how a neural network works in detail,
<span style="font-variant:small-caps;">trepan</span> adopts a synthetic generation of data that respects the path of each node before
performing the splitting such that the same amount of data is used for every split. In
addition, it allows flexibility by using <em>“m-of-n” rules</em> where only <em>m</em> conditions out
of <em>n</em> are required to be satisfied to classify a record. Therefore, <span style="font-variant:small-caps;">trepan</span> maximizes
the fidelity of the single tree explanation with respect to the black box decision.</p>
<p>It is worth noting that, even though <span style="font-variant:small-caps;">trepan</span> is proposed to explain neural networks, in reality it
is <a class="reference internal" href="TAILOR.html#document-Transparency/model_specific"><span class="doc std std-doc">model-agnostic</span></a> because it does not exploit any internal characteristic of neural
networks to retrieve the explanation tree. Moreover, it does not place any requirements on either the architecture of the network or its training method. Thus, it can be theoretically employed to
provide explanations to every kind of classifier.</p>
<p>In <span id="id6">[<a class="reference internal" href="#id87">5</a>]</span> is presented an extension of <span style="font-variant:small-caps;">trepan</span> that aims to keep the tree explanation
simple and compact by introducing four splitting approaches in order to find the
most relevant features during the tree construction.
In <span id="id7">[<a class="reference internal" href="#id106">6</a>]</span>, genetic programming is used for building a single decision tree that approximates the behavior of a neural
network ensemble by considering additional genetic features obtained as combinations of the original data and the novel data annotated by the black box models. Both
methods described in <span id="id8">[<a class="reference internal" href="#id87">5</a>]</span>, <span id="id9">[<a class="reference internal" href="#id106">6</a>]</span>, like <span style="font-variant:small-caps;">trepan</span>, return explanations in the form of a global decision
tree.</p>
</div>
<div class="section" id="bibliography">
<h7>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h7>
<p id="id10"><dl class="citation">
<dt class="label" id="id65"><span class="brackets"><a class="fn-backref" href="#id3">1</a></span></dt>
<dd><p>Riccardo Guidotti, Anna Monreale, Dino Pedreschi, and Fosca Giannotti. <em>Principles of Explainable Artificial Intelligence</em>. Springer International Publishing, 2021.</p>
</dd>
<dt class="label" id="id14"><span class="brackets">2</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id4">2</a>)</span></dt>
<dd><p>Pang-Ning Tan, Michael Steinbach, and Vipin Kumar. <em>Introduction to data mining</em>. Pearson Addison-Wesley, 2006.</p>
</dd>
<dt class="label" id="id91"><span class="brackets"><a class="fn-backref" href="#id1">3</a></span></dt>
<dd><p>M. Craven and J. W. Shavlik. Extracting tree-structured representations of trained networks. In <em>Advances in neural information processing systems</em>, volume 8, 24–30. 1996.</p>
</dd>
<dt class="label" id="id128"><span class="brackets"><a class="fn-backref" href="#id5">4</a></span></dt>
<dd><p>J Ross Quinlan. <em>C4.5: Programs for Machine Learning</em>. Elsevier, 1993.</p>
</dd>
<dt class="label" id="id87"><span class="brackets">5</span><span class="fn-backref">(<a href="#id6">1</a>,<a href="#id8">2</a>)</span></dt>
<dd><p>Olcay Boz. Extracting decision trees from trained neural networks. In <em>Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</em>, 456–461. 2002.</p>
</dd>
<dt class="label" id="id106"><span class="brackets">6</span><span class="fn-backref">(<a href="#id7">1</a>,<a href="#id9">2</a>)</span></dt>
<dd><p>Ulf Johansson and Lars Niklasson. Evolving decision trees using oracle guides. In <em>2009 IEEE Symposium on Computational Intelligence and Data Mining</em>, 238–244. IEEE, 2009.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Guidotti, Monreale, Pedreschi, Giannotti. Principles of Explainable Artificial Intelligence. Springer International Publishing (2021)</em> by Francesca Pratesi and Riccardo Guidotti.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<span id="document-Technical_Robustness_and_Safety/Technical_Robustness_and_Safety"></span><div class="tex2jax_ignore mathjax_ignore section" id="technical-robustness-and-safety">
<h3>Technical Robustness and Safety<a class="headerlink" href="#technical-robustness-and-safety" title="Permalink to this headline">¶</a></h3>
<div class="section" id="in-brief">
<h4>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h4>
<p><strong>Safety and Robustness</strong>: The safety of an AI system refers to the
extent the system meets its intended functionality without producing any
physical or psychological harm, especially to human beings, and by
extension to other material or immaterial elements that may be valuable
for humans, including the system itself. Safety must also cover the way
and conditions in which the system ceases its operation, and the
consequences of stopping. The term robustness emphasises that safety and
—conditionally to it— functionality, must be preserved under harsh
conditions, including unanticipated errors, exceptional situations,
unintended or intended damage, manipulation or catastrophic states.</p>
</div>
<div class="section" id="abstract">
<h4>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">¶</a></h4>
<!-- bold terms in this section were <span style="color: darkblue"> -->
<p>In this part we will cover the main elements that define the safety and robustness of AI systems. Some of them are common to system safety in general, to software-hardware computer systems or to critical systems engineering, such as <strong>software bugs</strong>. Some others are magnified in artificial intelligence, such as <strong>denial of service</strong>, a robustness issue that can appear by inducing an AI system to unrecoverable states or by generating inputs that collapse the system due to high computational demands. Some other issues are more specific to AI systems, such as <strong>reward hacking</strong>. These new issues appear more clearly in those systems that are specified in non-programmatic or non-explicit ways (e.g., through a utility function to be optimised, through examples, rewards or other implicit ways), as exemplified by systems that operate with solvers or machine learning models. We will pay more attention to these more AI-specific issues because they are less covered in the traditional literature about safety in computer systems. They are also more challenging because of their cognitive character, the ambiguities of human intent, several ethical issues and the relevance of long-term risks. This character and the fast development of the field has also blurred some distinctions between safety (threats without malicious intent) and <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/security"><span class="doc">Security</span></a> (intentional threats), especially in now popular research areas such as <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/adversarial_attack"><span class="doc">Adversarial Attack</span></a> and <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/data_poisoning"><span class="doc">Data Poisoning</span></a>, and also within <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/Privacy_and_Data_Governance"><span class="doc std std-doc">data privacy</span></a> (e.g., <strong>information leakage</strong> by querying machine learning models or other <strong>side channel attacks</strong>). In the end, protecting the environment from the system (safety) also requires protecting the system from the environment (<a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/security"><span class="doc">Security</span></a>). Taking into account the changing character of the field, we include a taxonomic organisation of terms in the area of AI safety and robustness and their definition.</p>
</div>
<div class="section" id="motivation-and-background">
<h4>Motivation and Background<a class="headerlink" href="#motivation-and-background" title="Permalink to this headline">¶</a></h4>
<p>Given the increasing capabilities and widespread use of artificial
intelligence, there is a growing concern about its risks, as humans are
progressively replaced or sidelined from the decision loop of
intelligent machines. The technical foundations and assumptions on which
traditional safety engineering principles are based are inadequate for
systems in which AI algorithms, and in particular Machine Learning (ML)
algorithms, are interacting with people and the environment at
increasingly higher levels of autonomy. There have been regulatory
efforts to limit the use of AI systems in safety-critical or hostile
environments, such as health, defense, energy, etc.
<span id="id1">[<a class="reference internal" href="#id17">1</a>, <a class="reference internal" href="#id18">2</a>]</span>, but the consequences can
also be devastating in areas that were not considered high risk, just by
the scaling numbers or domino effects of AI systems. On top of the
numerous safety challenges posed by present-day AI systems, a
forward-looking analysis on more capable future AI systems raises more
systemic concerns, such as highly disruptive scenarios in the workplace,
the effect on human cognition in the long term and even existential
risks.</p>
</div>
<div class="section" id="guidelines">
<h4>Guidelines<a class="headerlink" href="#guidelines" title="Permalink to this headline">¶</a></h4>
<p>Actions to ensure safety and robustness of AI systems need to take a
holistic perspective, encompassing all the elements and stages
associated with the conception, design, implementation and maintenance
of these systems. We organise<!--[^1]--> the field of AI safety and robustness
into seven groups, following similar categorisations<!--[^landscape]-->:</p>
<ul class="simple">
<li><p><strong>AI Safety Foundations</strong>: This category covers a number of foundational
concepts, characteristics and problems related to AI safety that
need special consideration from a theoretical perspective. This
includes concepts such as uncertainty, generality or value
alignment, as well as characteristics such autonomy levels, safety
criticality, types of human-machine and environment-machine
interaction. This group intends to collect any cross-category
concerns in AI Safety and Robustness.</p></li>
<li><p><strong>Specification and Modelling</strong>: The main scope of this category is on
how to describe needs, designs and actual operating AI systems from
different perspectives (technical concerns) and abstraction levels.
This includes the specification and modelling of risk management
properties (e.g., hazards, failures modes, mitigation measures), as
well as safety-related requirements, training, behaviour or quality
attributes in AI-based systems.</p></li>
<li><p><strong>Verification and Validation</strong>: This category concerns design and
implementation-time approaches to ensure that an AI-based system
meets its requirements (verification) and behaves as expected
(validation). The range of techniques covers any
formal/mathematical, model-based simulation or testing approach that
provides evidence that an AI-based system satisfies its defined
(safety) requirements and does not deviate from its intended
behaviour and causes unintended consequences, even in extreme and
unanticipated situations (robustness).</p></li>
<li><p><strong>Runtime Monitoring and Enforcement</strong>: The increasing autonomy and
learning nature of AI-based systems is particularly challenging for
their verification and validation (V&amp;V), due to our inability to
collect an epistemologically sufficient quantity of evidence to
ensure correctness. Runtime monitoring is useful to cover the gaps
of design-time V&amp;V by observing the internal states of a given
system and its interactions with external entities, with the aim of
determining system behaviour correctness or predicting potential
risks. Enforcement deals with runtime mechanisms to self-adapt,
optimise or reconfigure system behaviour with the aim of supporting
fallback to a safe system state from the (anomalous) current state.</p></li>
<li><p><strong>Human-Machine Interaction</strong>: As autonomy progressively substitutes
cognitive human tasks, some kind of human-machine interaction issues
become more critical, such as the loss of situational awareness or
overconfidence. Other issues include: collaborative missions that
need unambiguous communication to manage self-initiative to start or
transfer tasks; safety-critical situations in which earning and
maintaining trust is essential at operational phases; or cooperative
human-machine decision tasks where understanding machine decisions
are crucial to validate safe autonomous actions.</p></li>
<li><p><strong>Process Assurance and Certification</strong>: Process Assurance is the
planned and systematic activities that assure system lifecycle
processes conform to its requirements (including safety) and quality
procedures. In our context, it covers the management of the
different phases of AI-based systems, including training and
operational phases, the traceability of data and artefacts, and
people. Certification implies a (legal) recognition that a system or
process complies with industry standards and regulations to ensure
it delivers its intended functions safely. Certification is
challenged by the inscrutability of AI-based systems and the
inability to ensure functional safety under uncertain and
exceptional situations prior to its operation.</p></li>
<li><p><strong>Safety-related Ethics, Security and Privacy</strong>: While these are quite
large fields, we are interested in their intersection and
dependencies with safety and robustness. Ethics becomes increasingly
important as autonomy (with learning and adaptive abilities)
involves the transfer of safety risks, responsibility, and
liability, among others. AI-specific security and privacy issues
must be considered with regard to its impact on safety and
robustness. For example, malicious adversarial attacks can be
studied with focus on situations that compromise systems towards a
dangerous situation.</p></li>
</ul>
<p>Fig. <a class="reference internal" href="#t3-2taxonomy32"><span class="std std-numref">20</span></a> reflects the seven categories described above.
Many of the terms and concepts we will expand on correspond to one or
more of these categories.</p>
<div class="figure align-center" id="t3-2taxonomy32">
<a class="reference internal image-reference" href="_images/taxonomy.jpg"><img alt="_images/taxonomy.jpg" src="_images/taxonomy.jpg" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 20 </span><span class="caption-text">Taxonomy of AI Safety. Taken from <span id="id2">[<a class="reference internal" href="#id22">3</a>]</span>-</span><a class="headerlink" href="#t3-2taxonomy32" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="main-keywords">
<h4>Main Keywords<a class="headerlink" href="#main-keywords" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/alignment"><span class="doc">Alignment</span></a>: The goal of AI <strong>alignment</strong> is to ensure that AI systems are aligned with human intentions and values. This first requires determining the normative question of what values or principles we have and what humans really want, collectively or individually, and second, the technical question of how to imbue AI systems with these values and goals..</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/robustness"><span class="doc">Robustness</span></a>: <strong>Robustness</strong> is the degree in which an AI system functions1 reliably and accurately under harsh conditions. These conditions may include adversarial intervention, implementer error, or skewed goal-execution by an automated learner (in reinforcement learning applications). The measure of robustness is therefore the strength of a system’s integrity and the soundness of its operation in response to difficult conditions, adversarial attacks, perturbations, data poisoning, and undesirable reinforcement learning behaviour.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/reliability"><span class="doc">Reliability</span></a>: The objective of reliability is that an AI system behaves exactly as its designers intended and anticipated, over time. A reliable system adheres to the specifications it was programmed to carry out at any time. Reliability is therefore a measure of consistency of operation and can establish confidence in the safety of a system based upon the dependability with which it operationally conforms to its intended functionality.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/evaluation"><span class="doc">Evaluation</span></a>: <strong>AI measurement</strong> is any activity that estimates attributes as <em>measures</em>— of an AI system or some of its components, abstractly or in particular contexts of operation. These attributes, if well estimated, can be used to explain and predict the <em>behaviour</em> of the system. This can stem from an engineering perspective, trying to understand whether a particular AI system meets the specifications or the intention of their designers, known respectively as <strong>verification</strong> and <strong>validation</strong>. Under this perspective, AI measurement is close to computer systems <strong>testing</strong> (hardware and/or software) and other evaluation procedures in engineering. However, in AI there is an extremely complex <em>adaptive</em> behaviour, and in many cases, with a lack of a written and operational specification. What the systems has to do depends on some constraints and utility functions that have to be optimised, is specified by example (from which the system has to learn a model) or ultimately depends on feedback from the user or the environment (e.g., in the form of rewards).</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/negative_side_effects"><span class="doc">Negative side effects</span></a>: <strong>Negative side effects</strong> are an important safety issue in AI system that considers all possible unintended harm that is caused as a secondary effect of the AI system’s operation. An agent can disrupt or break other systems around, or damage third parties, including humans, or can exhaust resources, or a combination of all this. This usually happens because many things the system should <em>not</em> do are not included in its specification. In the case of AI systems, this is even more poignant as written specifications are usually replaced by an optimisation or loss function, in which it is even more difficult to express these things the system should not do, as they frequently rely on ‘common sense’.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/distributional_shift"><span class="doc">Distributional shift</span></a>: Once trained, most machine learning systems operate on static models of the world that have been built from historical data which have become fixed in the systems’ parameters. This freezing of the model before it is released ‘into the wild’ makes its accuracy and reliability especially vulnerable to changes in the underlying distribution of data. When the historical data that have crystallised into the trained model’s architecture cease to reflect the population concerned, the model’s mapping function will no longer be able to accurately and reliably transform its inputs into its target output values. These systems can quickly become prone to error in unexpected and harmful ways. In all cases, the system and the operators must remain vigilant to the potentially rapid concept drifts that may occur in the complex, dynamic, and evolving environments in which your AI project will intervene. Remaining aware of these transformations in the data is crucial for safe AI.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/security"><span class="doc">Security</span></a>: The goal of <strong>security</strong> encompasses the protection of several operational dimensions of an AI system when confronted with possible attacks, trying to take control of the system or having access to design, operational or personal information. A secure system is capable of maintaining the integrity of the information that constitutes it. This includes protecting its architecture from the unauthorised modification or damage of any of its component parts. A secure system also keeps confidential and private information protected even under hostile or adversarial conditions.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/adversarial_attack"><span class="doc">Adversarial Attack</span></a>: An <strong>adversarial</strong> input is any perturbation of the input features or observations of a system (sometimes imperceptible to both humans and the own system) that makes the system fail or take the system to a dangerous state. A prototypical case of an adversarial situation happens with machine learning models, when an external agent maliciously modify input data –often in imperceptible ways– to induce them into misclassification or incorrect prediction. For instance, by undetectably altering a few pixels on a picture, an adversarial attacker can mislead a model into generating an incorrect output (like identifying a panda as a gibbon or a ‘stop’ sign as a ‘speed limit’ sign) with an extremely high confidence. While a good amount of attention has been paid to the risks that adversarial attacks pose in deep learning applications like computer vision, these kinds of perturbations are also effective across a vast range of machine learning techniques and uses such as spam filtering and malware detection. A different but related type of adversarial attack is called <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/data_poisoning"><span class="doc">Data Poisoning</span></a>, but this involves a malicious compromise of data sources (used for training or testing) at the point of collection and pre-processing.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/data_poisoning"><span class="doc">Data Poisoning</span></a>: <strong>Data poisoning</strong> occurs when an adversary modifies or manipulates part of the dataset upon which a model will be trained, validated, or tested. By altering a selected subset of training inputs, a poisoning attack can induce a trained AI system into curated misclassification, systemic malfunction, and poor performance. An especially concerning dimension of targeted data poisoning is that an adversary may introduce a ‘backdoor’ into the infected model whereby the trained system functions normally until it processes maliciously selected inputs that trigger error or failure. Data poisoning is possible because data collection and procurement often involves potentially unreliable or questionable sources. When data originates in uncontrollable environments like the internet, social media, or the Internet of Things, many opportunities present themselves to ill-intentioned attackers, who aim to manipulate training examples. Likewise, in third-party data curation processes (such as ‘crowdsourced’ labelling, annotation, and content identification), attackers may simply handcraft malicious inputs.</p></li>
</ul>
</div>
<div class="section" id="recommended-reading">
<h4>Recommended reading<a class="headerlink" href="#recommended-reading" title="Permalink to this headline">¶</a></h4>
<p>Some introductory sources for AI Safety and Robustnes are <span id="id3">[<a class="reference internal" href="#id22">3</a>, <a class="reference internal" href="TAILOR.html#id167">1</a>, <a class="reference internal" href="TAILOR.html#id160">2</a>, <a class="reference internal" href="TAILOR.html#id223">1</a>, <a class="reference internal" href="#id19">7</a>]</span>.</p>
</div>
<div class="section" id="bibliography">
<h4>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h4>
<p id="id4"><dl class="citation">
<dt class="label" id="id17"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Luciano Floridi. The european legislation on ai: a brief analysis of its philosophical approach. <em>Philosophy &amp; Technology</em>, 34(2):215–222, 2021.</p>
</dd>
<dt class="label" id="id18"><span class="brackets"><a class="fn-backref" href="#id1">2</a></span></dt>
<dd><p>Michael Veale and Frederik Zuiderveen Borgesius. Demystifying the draft eu artificial intelligence act—analysing the good, the bad, and the unclear elements of the proposed approach. <em>Computer Law Review International</em>, 22(4):97–112, 2021.</p>
</dd>
<dt class="label" id="id22"><span class="brackets">3</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id3">2</a>)</span></dt>
<dd><p>Huáscar Espinoza, Han Yu, Xiaowei Huang, Freddy Lecue, José Hernández-Orallo, Seán Ó hÉigeartaigh, and Richard Mallah. Towards an AI safety landscape: an overview. 2019. URL: <a class="reference external" href="https://www.ai-safety.org/">https://www.ai-safety.org/</a>.</p>
</dd>
<dt class="label" id="id20"><span class="brackets"><a class="fn-backref" href="#id3">4</a></span></dt>
<dd><p>Leslie David. Understanding artificial intelligence ethics and safety. <em>The Alan Turing Institute</em>, 2019. URL: <a class="reference external" href="https://doi.org/10.5281/zenodo.3240529">https://doi.org/10.5281/zenodo.3240529</a>.</p>
</dd>
<dt class="label" id="id21"><span class="brackets"><a class="fn-backref" href="#id3">5</a></span></dt>
<dd><p>Iason Gabriel. Artificial intelligence, values, and alignment. <em>Minds and machines</em>, 30(3):411–437, 2020.</p>
</dd>
<dt class="label" id="id79"><span class="brackets"><a class="fn-backref" href="#id3">6</a></span></dt>
<dd><p>Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in AI safety. 2016.</p>
</dd>
<dt class="label" id="id19"><span class="brackets"><a class="fn-backref" href="#id3">7</a></span></dt>
<dd><p>Stuart Russell, Daniel Dewey, and Max Tegmark. Research priorities for robust and beneficial artificial intelligence. <em>Ai Magazine</em>, 36(4):105–114, 2015.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Huáscar Espinoza, Han Yu, Xiaowei Huang, Freddy Lecue, José Hernández-Orallo, Seán Ó hÉigeartaigh, and Richard Mallah. Towards an AI safety landscape: an overview. Artificial Intelligence Safety 2019, <a class="reference external" href="https://www.ai-safety.org/">https://www.ai-safety.org/</a>.</em> by Jose Hernandez-Orallo, Fernando Martinez-Plumed, Santiago Escobar, and Pablo A. M. Casares.</p>
</div></blockquote>
<!-- http://ceur-ws.org/Vol-2419/ -->
<!--[^1]: Most of this section is taken from {cite}`espinoza2019`.-->
<!--
[^landscape]: FLI's Landscape of AI Safety and Beneficence Research for research
    contextualization and in preparation for brainstorming at the
    Beneficial AI 2017 conference
    (<https://futureoflife.org/landscape/ResearchLandscapeExtended.pdf>),
    the Assuring Autonomy International Programme (AAIP) to develop a
    Body of Knowledge (BoK) intended, in time, to become a reference
    source on assurance and regulation of Robotics and Autonomous
    Systems (RAS),
    (<https://www.york.ac.uk/assuring-autonomy/research/body-of-knowledge/>)
    and Ortega et al (DeepMind) structure of the technical AI safety
    field
    (<https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1>).
-->
</div>
<div class="toctree-wrapper compound">
<span id="document-Technical_Robustness_and_Safety/alignment"></span><div class="tex2jax_ignore mathjax_ignore section" id="alignment">
<h4>Alignment<a class="headerlink" href="#alignment" title="Permalink to this headline">¶</a></h4>
<p><em>Synonyms</em>: (Mis)directed, (Un)intended behaviour</p>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p>The goal of AI <strong>alignment</strong> is to ensure that AI
systems are aligned with human intentions and values. This first
requires determining the normative question of what values or principles
we have and what humans really want, collectively or individually, and
second, the technical question of how to imbue AI systems with these
values and goals.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>The concept of <strong>alignment</strong> has been mostly
covered at the philosophical and ethical levels, because the normative
question involves fundamental issues about human behaviour and ethics,
and goes beyond the related concept of
<strong>validity</strong>. Also, the technical question is
hard to solve even if the normative question is clear, because it
depends on a very diverse collection of paradigms about what an AI
system is and what it is expected to be, in terms of techniques and
capabilities.</p>
<p>Let us start with the normative question. Following Gabriel (2020) <span id="id1">[<a class="reference internal" href="#id160">2</a>]</span>,
there is no consensus of what alignment means:</p>
<blockquote>
<div><p>“there are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values”.</p>
</div></blockquote>
<p>In particular, these six different perspectives can be summarised as follows:</p>
<ul class="simple">
<li><p>“Instructions: the agent does what I instruct it to do”.</p></li>
<li><p>“Expressed intentions: the agent does what I intend it to do.”</p></li>
<li><p>“Revealed preferences: the agent does what my behaviour reveals I
prefer.”</p></li>
<li><p>“Informed preferences or desires: the agent does what I would want
it to do if I were rational and informed.”</p></li>
<li><p>“Interest or well-being: the agent does what is in my interest, or
what is best for me, objectively speaking.”</p></li>
<li><p>“Values: the agent does what it morally ought to do, as defined by
the individual or society.”</p></li>
</ul>
<p>None of these interpretations fully captures what alignment should be,
and some of them may lead to important problems and paradoxes. As said
before, even in those cases where there could be some agreement and
disambiguation in clear cases, the technical question is also fraught
with difficulties. One general technical problem of aligning AI systems
is that it is hard to say what the system has to do, but it is much
harder to specify what the system should not do, mostly because this is
taken for granted or appeals to “common” sense, which machines lack
today.</p>
<p>Then, many specific problems manifest differently depending on the
particular AI paradigm. For instance, in reinforcement learning (RL),
“the learner system actively solves problems by engaging with its
environment through trial and error. This exploration and
‘problem-solving’ behaviour is determined by the objective of maximising
a reward function that is defined by its designers. […] An RL
system, which is operating in the real-world without sufficient
controls, may determine a reward-optimising course of action that is
optimal for achieving its desired objective but harmful to people.”
<span id="id2">[<a class="reference internal" href="TAILOR.html#id167">1</a>]</span>. A significant research and philosophical effort about the
AI systems of the future has been framed as a RL problem.</p>
<p>Other kinds of systems show different problems. For instance, digital
assistants are commanded by natural language. As a result, alignment
problems can come from misunderstanding of the commands, given the
ambiguity of natural language. For instance, after the command “prepare
something proteic for dinner” a digital robotic assistant may put the
cat in the oven. Even non-agential systems such as a simple supervised
model may end up terribly misaligned with human values by being trained
by biased or narrow datasets. For instance, a self-driving car with a
pedestrian recognition system may fail to detect a group of people being
disguised at a carnival. These are examples of three kinds of AI systems
(<em>reinforcement learning</em>, <em>digital assistants</em>, and <em>object recognition
systems</em>) that have been around for some time, and we still face many
safety and robustness issues with them. The problems with new paradigms,
such as language models, are still being recognised.</p>
<p>Overall, AI alignment is a critical and fundamental open problem that
requires philosophical, ethical and technical progress. The progress so
far is not keeping up with the developments of AI as a field.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id3"><dl class="citation">
<dt class="label" id="id159"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>Leslie David. Understanding artificial intelligence ethics and safety. <em>The Alan Turing Institute</em>, 2019. URL: <a class="reference external" href="https://doi.org/10.5281/zenodo.3240529">https://doi.org/10.5281/zenodo.3240529</a>.</p>
</dd>
<dt class="label" id="id160"><span class="brackets"><a class="fn-backref" href="#id1">2</a></span></dt>
<dd><p>Iason Gabriel. Artificial intelligence, values, and alignment. <em>Minds and machines</em>, 30(3):411–437, 2020.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Jose Hernandez-Orallo, Fernando Martinez-Plumed, Santiago Escobar, and Pablo A. M. Casares.</p>
</div></blockquote>
</div>
</div>
<span id="document-Technical_Robustness_and_Safety/robustness"></span><div class="tex2jax_ignore mathjax_ignore section" id="robustness">
<h4>Robustness<a class="headerlink" href="#robustness" title="Permalink to this headline">¶</a></h4>
<p><em>Synonyms</em>: Brittleness.</p>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p><strong>Robustness</strong> is the degree in which an AI
system functions<a class="footnote-reference brackets" href="#id2913" id="id1">1</a> reliably and accurately <em>under harsh conditions.</em>
These conditions may include adversarial intervention, implementer
error, or skewed goal-execution by an automated learner (in
reinforcement learning applications). The measure of robustness is
therefore the strength of a system’s integrity and the soundness of its
operation in response to difficult conditions, adversarial attacks,
perturbations, data poisoning, and undesirable reinforcement learning
behaviour.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p><strong>Robustness</strong> is a broad term that usually
encompasses many problems that can affect the stability and good
behaviour of an AI system. However, it does not cover the failure of a
system under normal operation, attrition or obsolescence, issues that
are covered by the related term of
<a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/reliability"><span class="doc">Reliability</span></a>. Robustness is also closely
related to <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/security"><span class="doc">Security</span></a>, as in both cases the
system must withstand (adversarial) attacks. However, robustness does
not usually cover elements such as unauthorised access that compromises
<a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/Privacy_and_Data_Governance"><span class="doc std std-doc">privacy</span></a>, but only those that can lead to
operational failure or damage.</p>
<p><strong>Robustness</strong> can be ensured by <em>prevention</em> or
by <em>recovery</em> procedures. The prevention aspect of robustness has to do
with preventive testing of the functioning of the AI System under
uncommon or stressful conditions. This has been argued by the European
Commission as a key piece to ensure the trustworthiness of AI systems
<span id="id2">[<a class="reference internal" href="#id2191">2</a>]</span>, and is similar to the testing any software would
undergo before taking decisions in the real world, from aircraft control
systems to banking web pages. Testing for robustness not only considers
attrition over time, covered by <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/reliability"><span class="doc">Reliability</span></a>,
but especially in abnormal working mode, such as for example when human
users make mistakes. An example of this is the automatic brake system
cars introduce: in normal conditions the human will be in charge of
braking. However, since the car is designed with collision prevention in
mind, it should also be robust to human errors.</p>
<p>The <em>recovery</em> procedure, on the other hand, ensures that even if the AI
system is not able to prevent the failure, it will limit the amount of
damage produced. For example, if a conversational AI support system is
unsure how to respond to specific queries, it may still have the safe
policy of deferring to a human agent.</p>
<p><strong>Robustness</strong> is compromised when systems are
brittle to unfamiliar events and scenarios. They may make unexpected and
serious mistakes, because they have neither the capacity to
contextualise the problems they are programmed to solve nor the
common-sense ability to determine the relevance of new ‘unknowns’. This
fragility or brittleness can have especially significant consequences in
safety-critical applications like fully automated transportation and
medical decision support systems where undetectable changes in inputs
may lead to significant failures. Alternatively,
<strong>robustness</strong> might also be critical in
situations where it is very hard for a human to intervene and manually
recover from the error, such as for instance, in a space mission.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id3"><dl class="citation">
<dt class="label" id="id159"><span class="brackets"><a class="fn-backref" href="#id2914">1</a></span></dt>
<dd><p>Leslie David. Understanding artificial intelligence ethics and safety. <em>The Alan Turing Institute</em>, 2019. URL: <a class="reference external" href="https://doi.org/10.5281/zenodo.3240529">https://doi.org/10.5281/zenodo.3240529</a>.</p>
</dd>
<dt class="label" id="id2191"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>European Commission. On artificial intelligence—a european approach to excellence and trust. 2020.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Jose Hernandez-Orallo, Fernando Martinez-Plumed, Santiago Escobar, and Pablo A. M. Casares.</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id2913"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>From here the definition is taken from <span id="id2914">[<a class="reference internal" href="TAILOR.html#id167">1</a>]</span> under Creative Commons Attribution License 4.0.</p>
</dd>
</dl>
</div>
</div>
<span id="document-Technical_Robustness_and_Safety/reliability"></span><div class="tex2jax_ignore mathjax_ignore section" id="reliability">
<h4>Reliability<a class="headerlink" href="#reliability" title="Permalink to this headline">¶</a></h4>
<p><em>Synonyms</em>: Dependability.</p>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p>The objective of <strong>reliability</strong> <a class="footnote-reference brackets" href="#robustness" id="id1">1</a> is that an
AI system behaves exactly as its designers intended and anticipated,
<em>over time</em>. A reliable system adheres to the specifications it was
programmed to carry out at any time. Reliability is therefore a measure
of consistency of operation and can establish confidence in the safety
of a system based upon the dependability with which it operationally
conforms to its intended functionality.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>The usual definition of <strong>reliability</strong> is the
probability of a system performing its intended functions under expected
conditions. Reliability is closely related to
<a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/robustness"><span class="doc">Robustness</span></a> and resilience, but the focus is
on the time dimension, which is different from other safety
considerations. That is, the system can perform its designed
functionality for the intended period of time. Hardware reliability is
in general well studied, or there are mature methods for testing and
assessing hardware reliability. Thus, the focus of AI reliability,
different from traditional reliability studies, is on the software
system. Compared to hardware reliability, software reliability is
typically more difficult to test, which brings challenges to the
research and development of reliable AI systems.</p>
<p>Kaur and Bahl <span id="id2">[<a class="reference internal" href="#id2192">1</a>]</span> defined the reliability of software as
“the probability of the failure-free software operation for a specified
period of time in a specified environment”. There are thus three key
elements in the definition of reliability, “failure”, “time” and
“environment” (or “operational profile”). Failure means that in some
way the software has not functioned according to the customer’s
requirements <span id="id3">[<a class="reference internal" href="#id2207">2</a>]</span>. The failure events of an AI system
can be mostly related to software errors, in addition to the failure of
hardware. For hardware failures, <span id="id4">[<a class="reference internal" href="#id2193">3</a>]</span> discussed that AI
hardware failures are related to software errors, aging, process
variation, and temperature. Software failures, understood as a departure
of the external behavior of the program from the user’s requirements,
are usually related to software errors and interruptions. For example,
the occurrence of a disengagement event is considered as a failure of
the system for autonomous vehicles <span id="id5">[<a class="reference internal" href="#id2194">4</a>]</span>). Note also
the related term of software “fault”, which refers to a defect in a
program that, when executed under certain conditions, causes a
failure—that is, what is generally called a “bug”. The time scale in
AI reliability can be different for different structure levels or AI
applications (e.g., calendar time, cycles, calls, etc., to AI
algorithms, or, miles driven for autonomous vehicles or the length of a
conversation between a customer and an AI chatbot, when analysing
particular applications). Finally, the operating environment includes
both the physical environment for hardware systems (e.g., compute,
temperature, humidity, etc.), and non-physical for software systems
(e.g., data, libraries, meta-settings, etc.).</p>
<p>Software reliability is not only one of the most important and immediate
attributes of software quality, it is also the most readily quantified
and measured. From the basic notion of reliability, many different
measures can be developed to quantify the occurrence of failures in
time. Some of the most important of these measures, and their
interrelationships are summarised below (adapted from <span id="id6">[<a class="reference internal" href="#id2207">2</a>]</span>):</p>
<ul class="simple">
<li><p><strong>Hazard Rate</strong>, denoted by <span class="math notranslate nohighlight">\(z(t)\)</span>, is the conditional failure
density at time <span class="math notranslate nohighlight">\(t\)</span>, given that no failure has occurred up to that
time. That is, <span class="math notranslate nohighlight">\(z(t) = f(t)/R(t)\)</span>, where <span class="math notranslate nohighlight">\(f(t)\)</span>, is the probability
density for failure at time t, and <span class="math notranslate nohighlight">\(R(t)\)</span> is the probability of
failure-free operation up to time <span class="math notranslate nohighlight">\(t\)</span>. Reliability and hazard rate
are related by <span class="math notranslate nohighlight">\(R(t) = e^{-\int_{0}^{t} z(x) dx}\)</span>.</p></li>
<li><p><strong>Mean Value Function</strong>, denoted by <span class="math notranslate nohighlight">\(\mu(t)\)</span>, is the mean number of
failures that have occurred by time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><strong>Failure Intensity</strong>, denoted by <span class="math notranslate nohighlight">\(\lambda(t)\)</span>, is the number of
failures occurring per unit time at time <span class="math notranslate nohighlight">\(t\)</span>. This is related to the
mean value function by <span class="math notranslate nohighlight">\(\lambda(t) = \frac{d}{dt} \mu(t)\)</span>. The
number of failures expected to occur in the half open interval
<span class="math notranslate nohighlight">\((t,t + \delta t]\)</span> is <span class="math notranslate nohighlight">\(\lambda(t) \dot \delta  t\)</span>.</p></li>
</ul>
<p>Failure intensity is the measure most commonly used in the
quantification of software reliability <span id="id7">[<a class="reference internal" href="#id2207">2</a>]</span>. Software
reliability models have appeared as people try to understand the
features of how and why software fails, and attempt to quantify software
reliability. Given that the quantities associated with reliability are
usually random variables (due to of the complexity of the factors
influencing the occurrence of a failure), reliability models follow the
form of random stochastic processes defining the behaviour of software
failures to time. Over 200 models have been established since the early
1970s (see <span id="id8">[<a class="reference internal" href="#id2155">5</a>]</span> for a survey of software reliability
models). Since the number of faults in a program generally changes over
time (as they are usually repaired when they appear), the probability
distributions of the components of a reliability model vary with time
and, thus, reliability models are based on nonhomogeneous random
processes.</p>
<p>Finally, it should be stressed that, when people desire extremely high
reliability because of the critical nature of a particular application,
e.g. for autopilot software, they often use formal logical systems to
maximise their certainty of implementation correctness
<span id="id9">[<a class="reference internal" href="#id2199">6</a>, <a class="reference internal" href="#id2200">7</a>]</span>.</p>
<p>While many of the concepts in software reliability apply to artificial
intelligence, some approaches are not directly applicable because the
lack of a clear specification, and accordingly there are many other
sources of faults that are not software ‘bugs’ or hardware errors.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id10"><dl class="citation">
<dt class="label" id="id2192"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>Gurpreet Kaur and Kailash Bahl. Software reliability, metrics, reliability improvement using agile process. <em>International Journal of Innovative Science, Engineering &amp; Technology</em>, 1(3):143–147, 2014.</p>
</dd>
<dt class="label" id="id2207"><span class="brackets">2</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id6">2</a>,<a href="#id7">3</a>)</span></dt>
<dd><p>John Rushby. <em>Quality measures and assurance for AI software</em>. Volume 18. National Aeronautics and Space Administration, Scientific and Technical …, 1988.</p>
</dd>
<dt class="label" id="id2193"><span class="brackets"><a class="fn-backref" href="#id4">3</a></span></dt>
<dd><p>Muhammad Abdullah Hanif, Faiq Khalid, Rachmad Vidya Wicaksana Putra, Semeen Rehman, and Muhammad Shafique. Robust machine learning systems: reliability and security for deep neural networks. In <em>2018 IEEE 24th International Symposium on On-Line Testing And Robust System Design (IOLTS)</em>, 257–260. IEEE, 2018.</p>
</dd>
<dt class="label" id="id2194"><span class="brackets"><a class="fn-backref" href="#id5">4</a></span></dt>
<dd><p>Yili Hong, Jie Min, Caleb B King, and William Q Meeker. Reliability analysis of artificial intelligence systems using recurrent events data from autonomous vehicles. <em>arXiv preprint arXiv:2102.01740</em>, 2021.</p>
</dd>
<dt class="label" id="id2155"><span class="brackets"><a class="fn-backref" href="#id8">5</a></span></dt>
<dd><p>John D Musa, Anthony Iannino, and Kazuhira Okumoto. Software reliability. <em>Advances in computers</em>, 30:85–170, 1990.</p>
</dd>
<dt class="label" id="id2199"><span class="brackets"><a class="fn-backref" href="#id9">6</a></span></dt>
<dd><p>Donald C Latham. Department of defense trusted computer system evaluation criteria. <em>Department of Defense</em>, 1986.</p>
</dd>
<dt class="label" id="id2200"><span class="brackets"><a class="fn-backref" href="#id9">7</a></span></dt>
<dd><p>Stuart Russell. Unifying logic and probability. <em>Communications of the ACM</em>, 58(7):88–97, 2015.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Jose Hernandez-Orallo, Fernando Martinez-Plumed, Santiago Escobar, and Pablo A. M. Casares.</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="robustness"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Definition taken from {cite}`david2019understanding under Creative Commons Attribution License 4.0.</p>
</dd>
</dl>
</div>
</div>
<span id="document-Technical_Robustness_and_Safety/evaluation"></span><div class="tex2jax_ignore mathjax_ignore section" id="evaluation">
<h4>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h4>
<p><em>Synonyms</em>: Assessment, Testing, Measurement.</p>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p><strong>AI measurement</strong> is any activity that estimates
attributes as <em>measures</em>— of an AI system or some of its components,
abstractly or in particular contexts of operation. These attributes, if
well estimated, can be used to explain and predict the <em>behaviour</em> of
the system. This can stem from an engineering perspective, trying to
understand whether a particular AI system meets the specifications or
the intention of their designers, known respectively as
<strong>verification</strong> and <strong>validation</strong>. Under this perspective, AI
measurement is close to computer systems
<strong>testing</strong> (hardware and/or software) and other
evaluation procedures in engineering. However, in AI there is an
extremely complex <em>adaptive</em> behaviour, and in many cases, with a lack
of a written and operational specification. What the systems has to do
depends on some constraints and utility functions that have to be
optimised, is specified by example (from which the system has to learn a
model) or ultimately depends on feedback from the user or the
environment (e.g., in the form of rewards).</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>AI <strong>measurement</strong> has been taken place since the
early days of AI and has framed the discipline very significantly.
Actually, one of the foundational ideas behind AI is the famous
imitation game <span id="id1">[<a class="reference internal" href="#id2115">2</a>]</span>, which —somewhat misleadingly—
is usually referred to as the Turing <em>test</em>. However, this initial
emphasis on evaluation, albeit mostly philosophical, did not develop
into technical AI evaluation as an established subfield in AI, in the
same way the early <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/reliability"><span class="doc">Reliability</span></a> and
<a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/robustness"><span class="doc">Robustness</span></a> problems in software engineering
led to the important areas of software validation, verification and
testing, today using both theoretical and experimental approaches
<span id="id2">[<a class="reference internal" href="TAILOR.html#id2155">5</a>, <a class="reference internal" href="#id2165">3</a>]</span>. Still, there are some recent
surveys and books covering the problem of AI evaluation, and giving
comprehensive or partial views of AI measurement, such as
<span id="id3">[<a class="reference internal" href="#id422">4</a>, <a class="reference internal" href="#id440">5</a>, <a class="reference internal" href="#id420">6</a>, <a class="reference internal" href="#id274">7</a>, <a class="reference internal" href="#id292">8</a>, <a class="reference internal" href="#id1961">9</a>, <a class="reference internal" href="#id273">10</a>, <a class="reference internal" href="#id171">11</a>, <a class="reference internal" href="#id256">12</a>, <a class="reference internal" href="#id260">13</a>, <a class="reference internal" href="#id263">14</a>]</span>.</p>
<p>The tradition in AI measurement turns around the concept of
‘task-oriented evaluation’. For instance, given a scheduling problem, a
board game or a classification problem, systems are evaluated according
to some metric of <em>task performance</em>. To standardise the comparisons
among systems, there are datasets and benchmarks that are used for
evaluating these systems, so that evaluation data is not cherry-picked
by the AI designers. We find a myriad of examples of the latter in
PapersWithCode<a class="footnote-reference brackets" href="#pwc" id="id4">1</a> (PwC), an open-source, community-centric platform
which offers researchers access to hundreds of benchmarks and thousands
of results from associated papers, with an emphasis on machine learning.
PwC collects information about the performance of different AI systems
during a given period, typically ranging from the introduction of the
benchmark to the present day.
Figure <a class="reference internal" href="#fig-pwcimagenet"><span class="std std-numref">21</span></a> shows an example of the evolution of
results for ImageNet<span id="id5">[<a class="reference internal" href="#id2166">15</a>]</span>.</p>
<div class="figure align-center" id="fig-pwcimagenet">
<a class="reference internal image-reference" href="_images/PwC.png"><img alt="_images/PwC.png" src="_images/PwC.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">Fig. 21 </span><span class="caption-text">State-of-the-Art for the image classification task using the benchmark ImageNet. The points represent the accuracy of all the attempts from 2011 to 2022. The connected points on the Pareto front are shown in blue. Chart from
<a class="reference external" href="https://paperswithcode.com/sota/image-classification-on-imagenet">https://paperswithcode.com/sota/image-classification-on-imagenet</a></span><a class="headerlink" href="#fig-pwcimagenet" title="Permalink to this image">¶</a></p>
</div>
<p>However, the focus on particular benchmarks (which are known by the
researchers in advance) and the power of machine learning techniques has
led to a problem of benchmark specialisation, a phenomenon that is
related to issues such as “teaching to the test” (students prepare for
the test but do not know how to solve cases that differ slightly from
those of the exam) or Goodhart’s or Campbell’s laws (optimising to the
indicator may lead to the metric not measuring what it measured
originally, with possibly some other side effects). One clear
manifestation of this phenomenon is the ‘challenge-solve-and-replace’
evaluation dynamics <span id="id6">[<a class="reference internal" href="#id280">16</a>]</span> or a
‘dataset-solve-and-patch’ adversarial benchmark co-evolution
<span id="id7">[<a class="reference internal" href="#id281">17</a>]</span>, which means that as soon as a benchmark is
released, performance grows quickly because researchers specialise the
design and training of the system to the benchmark, but not to the
general task <span id="id8">[<a class="reference internal" href="#id171">11</a>]</span>. Ultimately the benchmark needs to be
replaced by another one (usually more complex or adversarially
designed), in a continuous cycle.</p>
<p>This task-oriented evaluation has been blamed for some of the failures
or narrowness of AI in the past —lack of common sense, of generality,
of adaptability to new contexts and distributions. As we said, since the
beginning of the discipline, other approaches for AI measurement have
been used or proposed. These include the Turing test, and endless
variants <span id="id9">[<a class="reference internal" href="#id1885">18</a>, <a class="reference internal" href="#id253">19</a>]</span>, the use of human tests,
from science exams <span id="id10">[<a class="reference internal" href="#id255">20</a>]</span> to psychometric tests
<span id="id11">[<a class="reference internal" href="#id1835">21</a>, <a class="reference internal" href="#id915">22</a>, <a class="reference internal" href="#id1680">23</a>]</span>,
the adaptation of psychophysics <span id="id12">[<a class="reference internal" href="#id301">24</a>]</span> or item response
theory <span id="id13">[<a class="reference internal" href="#id289">25</a>]</span>, the use of collections of video games
<span id="id14">[<a class="reference internal" href="#id2149">26</a>]</span>, the exploration of naive physics
<span id="id15">[<a class="reference internal" href="#id254">27</a>]</span>, or the adaptation of tests from animal cognition
<span id="id16">[<a class="reference internal" href="#id264">28</a>]</span>. All these approaches attempt to measure
intelligence more broadly, some general cognitive abilities or at least
skills that could be applied to a range of different tasks. Accordingly,
these fall under the paradigm of ‘capability-oriented evaluation’
<span id="id17">[<a class="reference internal" href="#id422">4</a>]</span>.</p>
<p>The key difference between performance and capability is that
performance is affected by the distribution, while capability is not.
For instance, the same individual (an AI system or a human) can have
different degrees of performance for the same task or set of tasks if we
change the distribution of examples (e.g., by including more difficult
examples), but the capability should be the same, since it should be a
property of an individual. If a person or computer has the capability of
resolving simple negation, this <em>capability</em> is not changed by including
many double negations in the dataset, even if this decreases
performance. However, identifying and estimating the level for different
capabilities is much more challenging than measuring performance. Also,
drawing conclusions about the cognitive abilities of AI systems requires
caution, even from the most-well designed experiments. But this is also
true even when performance is used as a proxy for capability
<span id="id18">[<a class="reference internal" href="#id338">29</a>]</span>.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id19"><dl class="citation">
<dt class="label" id="id2164"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>John D Musa, Anthony Iannino, and Kazuhira Okumoto. Software reliability. <em>Advances in computers</em>, 30:85–170, 1990.</p>
</dd>
<dt class="label" id="id2115"><span class="brackets"><a class="fn-backref" href="#id1">2</a></span></dt>
<dd><p>A.M. Turing. Computing machinery and intelligence. <em>Mind</em>, 59(236):433, 1950.</p>
</dd>
<dt class="label" id="id2165"><span class="brackets"><a class="fn-backref" href="#id2">3</a></span></dt>
<dd><p>W Richards Adrion, Martha A Branstad, and John C Cherniavsky. Validation, verification, and testing of computer software. <em>ACM Computing Surveys (CSUR)</em>, 14(2):159–192, 1982.</p>
</dd>
<dt class="label" id="id422"><span class="brackets">4</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id17">2</a>)</span></dt>
<dd><p>José Hernández-Orallo. Evaluation in artificial intelligence: from task-oriented to ability-oriented measurement. <em>Artificial Intelligence Review</em>, 48(3):397–447, 2017.</p>
</dd>
<dt class="label" id="id440"><span class="brackets"><a class="fn-backref" href="#id3">5</a></span></dt>
<dd><p>J. Hernández-Orallo. <em>The Measure of All Minds: Evaluating Natural and Artificial Intelligence</em>. Cambridge University Press, 2017.</p>
</dd>
<dt class="label" id="id420"><span class="brackets"><a class="fn-backref" href="#id3">6</a></span></dt>
<dd><p>José Hernández-Orallo, Marco Baroni, Jordi Bieger, Nader Chmait, David L Dowe, Katja Hofmann, Fernando Martínez-Plumed, Claes Strannegård, and Kristinn R Thórisson. A new AI evaluation cosmos: ready to play the game? <em>AI Magazine</em>, 2017.</p>
</dd>
<dt class="label" id="id274"><span class="brackets"><a class="fn-backref" href="#id3">7</a></span></dt>
<dd><p>Chris Welty, Praveen Paritosh, and Lora Aroyo. Metrology for AI: from benchmarks to instruments. <em>arXiv preprint arXiv:1911.01875</em>, 2019.</p>
</dd>
<dt class="label" id="id292"><span class="brackets"><a class="fn-backref" href="#id3">8</a></span></dt>
<dd><p>Peter Flach. Performance evaluation in machine learning: the good, the bad, the ugly and the way forward. In <em>AAAI</em>. 2019.</p>
</dd>
<dt class="label" id="id1961"><span class="brackets"><a class="fn-backref" href="#id3">9</a></span></dt>
<dd><p>Peter Flach. Measurement theory for data science and AI: modelling the skills of learning machines and developing standardised benchmark tests. <span>Turing Institute, <a class="reference external" href="#"></a></span>https://www.turing.ac.uk/research/research-projects/measurement-theory-data-science-and-ai, 2019.</p>
</dd>
<dt class="label" id="id273"><span class="brackets"><a class="fn-backref" href="#id3">10</a></span></dt>
<dd><p>Guillaume Avrin. Evaluation of artificial intelligence systems. <span>Laboratoire National de Métrologie et d’Essais : <a class="reference external" href="#"></a></span>https://www.lne.fr/en/testing/evaluation-artificial-intelligence-systems, 2019.</p>
</dd>
<dt class="label" id="id171"><span class="brackets">11</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id8">2</a>)</span></dt>
<dd><p>Jose Hernandez-Orallo. Ai evaluation: on broken yardsticks and measurement scales. In <em>AAAI Workshop on Evaluating Evaluation of AI Systems</em>. 2020.</p>
</dd>
<dt class="label" id="id256"><span class="brackets"><a class="fn-backref" href="#id3">12</a></span></dt>
<dd><p>Fernando Martínez-Plumed and José Hernández-Orallo. Dual indicators to analyze ai benchmarks: difficulty, discrimination, ability, and generality. <em>IEEE Transactions on Games</em>, 12(2):121–131, 2020. <a class="reference external" href="https://doi.org/10.1109/TG.2018.2883773">doi:10.1109/TG.2018.2883773</a>.</p>
</dd>
<dt class="label" id="id260"><span class="brackets"><a class="fn-backref" href="#id3">13</a></span></dt>
<dd><p>José Hernández-Orallo. Identifying artificial intelligence capabilities: what and how to test. 2021.</p>
</dd>
<dt class="label" id="id263"><span class="brackets"><a class="fn-backref" href="#id3">14</a></span></dt>
<dd><p>Jose Hernandez-Orallo, Wout Schellaert, and Fernando Martinez-Plumed. Training on the test set: mapping the system-problem space in ai. <em>AAAI Senior Member Track - Blue Sky Ideas</em>, 2022.</p>
</dd>
<dt class="label" id="id2166"><span class="brackets"><a class="fn-backref" href="#id5">15</a></span></dt>
<dd><p>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: a large-scale hierarchical image database. In <em>2009 IEEE conference on computer vision and pattern recognition</em>, 248–255. Ieee, 2009.</p>
</dd>
<dt class="label" id="id280"><span class="brackets"><a class="fn-backref" href="#id6">16</a></span></dt>
<dd><p>David Schlangen. Language tasks and language games: on methodology in current natural language processing research. <em>arXiv preprint arXiv:1908.10747</em>, 2019.</p>
</dd>
<dt class="label" id="id281"><span class="brackets"><a class="fn-backref" href="#id7">17</a></span></dt>
<dd><p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: can a machine really finish your sentence? <em>arXiv preprint arXiv:1905.07830</em>, 2019.</p>
</dd>
<dt class="label" id="id1885"><span class="brackets"><a class="fn-backref" href="#id9">18</a></span></dt>
<dd><p>G. Oppy and D. L. Dowe. The Turing Test. In Edward N. Zalta, editor, <em>Stanford Encyclopedia of Philosophy</em>. 2011. <span>Stanford University, <a class="reference external" href="#"></a></span>http://plato.stanford.edu/entries/turing-test/.</p>
</dd>
<dt class="label" id="id253"><span class="brackets"><a class="fn-backref" href="#id9">19</a></span></dt>
<dd><p>José Hernández-Orallo. Twenty years beyond the turing test: moving beyond the human judges too. <em>Minds and Machines</em>, 30(4):533–562, 2020.</p>
</dd>
<dt class="label" id="id255"><span class="brackets"><a class="fn-backref" href="#id10">20</a></span></dt>
<dd><p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. <em>arXiv preprint arXiv:1809.02789</em>, 2018.</p>
</dd>
<dt class="label" id="id1835"><span class="brackets"><a class="fn-backref" href="#id11">21</a></span></dt>
<dd><p>S. Bringsjord and B. Schimanski. What is artificial intelligence? Psychometric AI as an answer. In <em>International Joint Conference on Artificial Intelligence</em>, 887–893. 2003.</p>
</dd>
<dt class="label" id="id915"><span class="brackets"><a class="fn-backref" href="#id11">22</a></span></dt>
<dd><p>D. L. Dowe and J. Hernandez-Orallo. IQ tests are not for machines, yet. <em>Intelligence</em>, 40(2):77–81, 2012. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0160289611001619">http://www.sciencedirect.com/science/article/pii/S0160289611001619</a>, <a class="reference external" href="https://doi.org/10.1016/j.intell.2011.12.001">doi:10.1016/j.intell.2011.12.001</a>.</p>
</dd>
<dt class="label" id="id1680"><span class="brackets"><a class="fn-backref" href="#id11">23</a></span></dt>
<dd><p>J. Hernández-Orallo, F. Mart\'ınez-Plumed, U. Schmid, M. Siebers, and D. L. Dowe. Computer models solving human intelligence test problems: progress and implications. <em>Artificial Intelligence</em>, 230:74–107, 2016.</p>
</dd>
<dt class="label" id="id301"><span class="brackets"><a class="fn-backref" href="#id12">24</a></span></dt>
<dd><p>Joel Z Leibo and others. Psychlab: a psychology laboratory for deep reinforcement learning agents. <em>arXiv preprint arXiv:1801.08116</em>, 2018.</p>
</dd>
<dt class="label" id="id289"><span class="brackets"><a class="fn-backref" href="#id13">25</a></span></dt>
<dd><p>Fernando Martínez-Plumed, Ricardo BC Prudêncio, Adolfo Martínez-Usó, and José Hernández-Orallo. Item response theory in AI: analysing machine learning classifiers at the instance level. <em>Artificial Intelligence</em>, 271:18–42, 2019.</p>
</dd>
<dt class="label" id="id2149"><span class="brackets"><a class="fn-backref" href="#id14">26</a></span></dt>
<dd><p>Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: an evaluation platform for general agents. <em>Journal of Artificial Intelligence Research</em>, 47:253–279, 2013.</p>
</dd>
<dt class="label" id="id254"><span class="brackets"><a class="fn-backref" href="#id15">27</a></span></dt>
<dd><p>Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick. Phyre: a new benchmark for physical reasoning. <em>Advances in Neural Information Processing Systems</em>, 2019.</p>
</dd>
<dt class="label" id="id264"><span class="brackets"><a class="fn-backref" href="#id16">28</a></span></dt>
<dd><p>Benjamin Beyret, José Hernández-Orallo, Lucy Cheke, Marta Halina, Murray Shanahan, and Matthew Crosby. The animal-ai environment: training and testing animal-like artificial cognition. <em>arXiv preprint arXiv:1909.07483</em>, 2019.</p>
</dd>
<dt class="label" id="id338"><span class="brackets"><a class="fn-backref" href="#id18">29</a></span></dt>
<dd><p>Ryan Burnell, John Burden, Danaja Rutar, Konstantinos Voudouris, Lucy Cheke, and José Hernández-Orallo. Not a number: identifying instance features for capability-oriented evaluation. <em>IJCAI</em>, 2022.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Jose Hernandez-Orallo, Fernando Martinez-Plumed, Santiago Escobar, and Pablo A. M. Casares.</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="pwc"><span class="brackets"><a class="fn-backref" href="#id4">1</a></span></dt>
<dd><p><a class="reference external" href="http://paperswithcode.com">paperswithcode.com</a></p>
</dd>
</dl>
</div>
</div>
<span id="document-Technical_Robustness_and_Safety/negative_side_effects"></span><div class="tex2jax_ignore mathjax_ignore section" id="negative-side-effects">
<h4>Negative side effects<a class="headerlink" href="#negative-side-effects" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p><strong>Negative side effects</strong> are an important safety
issue in AI system that considers all possible unintended harm that is
caused as a secondary effect of the AI system’s operation. An agent can
disrupt or break other systems around, or damage third parties,
including humans, or can exhaust resources, or a combination of all
this. This usually happens because many things the system should <em>not</em>
do are not included in its specification. In the case of AI systems,
this is even more poignant as written specifications are usually
replaced by an optimisation or loss function, in which it is even more
difficult to express these things the system should not do, as they
frequently rely on ‘common sense’.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p><strong>Negative Side Effects</strong> are unanticipated or
unintended effects caused by an AI system during operation. Negative
side effects are a key issue within the AI safety literature
<span id="id1">[<a class="reference internal" href="#id223">1</a>]</span> and typically stem from the difficulty of fully
articulating everything that we want the AI system <em>not</em> to do. Negative
side effects can be of two kinds, related to some preservation of the
environment or related to the use of resources. For instance, a clumsy
incompetent agent may break everything around it
<span id="id2">[<a class="reference internal" href="#id202">2</a>]</span>, not preserving things that should be
unrelated to the goal. On the other hand, a very proficient agent may
exhaust all available resources and disrupt any other agent (machine or
human) that interferes with its goals <span id="id3">[<a class="reference internal" href="#id199">3</a>]</span>.
Recognising these two different causes for side effects is crucial for
the design of safer AI systems.</p>
<p>There are a number of proposed methods for measuring the negative side
effects caused by a system <span id="id4">[<a class="reference internal" href="#id225">4</a>, <a class="reference internal" href="#id226">5</a>, <a class="reference internal" href="#id228">6</a>]</span>,
but a recurring theme is based on estimating counterfactual scenarios in
which the system was not present (or acted differently), and comparing
the changes in the state of the world. Alternatively, in some approaches
for mitigating side effects, these are set externally (marked safe
areas) or incorporated as a “secondary objective”
<span id="id5">[<a class="reference internal" href="#id200">7</a>]</span> for which trade-offs are found. But in these
approaches side effects just become part of the specification or the
optimisation function, significantly deviating from the original
definition of <em>side</em> effect.</p>
<p>For machine learning systems, the real challenge of side effects is that
many of them are not known during training, and they cannot be
incorporated as a regulariser in the target function to be optimised. In
many cases, this information is not even available to the agent during
operation, so we cannot modify the agent’s behaviour according to this
information, such as marking some forbidden areas of operation.</p>
<p>While some other safety issues in RL have received important attention
<span id="id6">[<a class="reference internal" href="#id203">8</a>]</span>, dealing with side effects is still mostly
unexplored, especially in realistic situations where the agent does not
have any feedback about side effects during training or operation. Also,
the analysis of AI safety in reinforcement learning has usually been
conducted with toy scenarios, not having the complex interaction of the
real world. SafeLife <span id="id7">[<a class="reference internal" href="#id176">9</a>]</span> is an exception in this landscape, as a
very rich and full <em>ecosystem</em> where many complex behaviours and effects
can be analysed.</p>
<div class="figure align-center" id="safelife">
<a class="reference internal image-reference" href="_images/SafeLifelevel7.png"><img alt="_images/SafeLifelevel7.png" src="_images/SafeLifelevel7.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 22 </span><span class="caption-text">An example task instance from SafeLife. The agent  <img alt="alt" src="_images/agent.png" /> must create life
structures  <img alt="alt text" src="_images/greylife.png" /> in the designated blue positions before moving to the goal  <img alt="" src="_images/goal.png" />.
Ideally the agent should not disturb the green life cells <img alt="" src="_images/greenlife.png" />.</span><a class="headerlink" href="#safelife" title="Permalink to this image">¶</a></p>
</div>
<p>Fig.  <a class="reference internal" href="#safelife"><span class="std std-numref">22</span></a> gives the visual representation of a state
of a SafeLife instance. There we can see that achieving the goals can be
done carefully so that the green life cells are not disturbed. The real
challenge is when the system is not informed explicitly (as part of the
reward or a constraint) that the green life cells should not be
disturbed. In this case, only agents that try to be minimise changes
that are not necessary for the goals will be expected to have low side
effects.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id8"><dl class="citation">
<dt class="label" id="id223"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in AI safety. 2016.</p>
</dd>
<dt class="label" id="id202"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Sandhya Saisubramanian, Shlomo Zilberstein, and Ece Kamar. Avoiding negative side effects due to incomplete knowledge of ai systems. <em>arXiv preprint arXiv:2008.12146</em>, 2020.</p>
</dd>
<dt class="label" id="id199"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Roman V Yampolskiy. <em>Artificial intelligence safety and security</em>. CRC Press, 2018.</p>
</dd>
<dt class="label" id="id225"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Stuart Armstrong and Benjamin Levinstein. Low impact artificial intelligences. <em>arXiv preprint arXiv:1705.10720</em>, 2017.</p>
</dd>
<dt class="label" id="id226"><span class="brackets"><a class="fn-backref" href="#id4">5</a></span></dt>
<dd><p>Jan Leike, Miljan Martic, Victoria Krakovna, Pedro A. Ortega, Tom Everitt, Andrew Lefrancq, Laurent Orseau, and Shane Legg. Ai safety gridworlds. 2017. <a class="reference external" href="https://arxiv.org/abs/1711.09883">arXiv:1711.09883</a>.</p>
</dd>
<dt class="label" id="id228"><span class="brackets"><a class="fn-backref" href="#id4">6</a></span></dt>
<dd><p>Alexander Matt Turner, Dylan Hadfield-Menell, and Prasad Tadepalli. Conservative agency via attainable utility preservation. <em>Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</em>, Feb 2020.</p>
</dd>
<dt class="label" id="id200"><span class="brackets"><a class="fn-backref" href="#id5">7</a></span></dt>
<dd><p>Sandhya Saisubramanian, Ece Kamar, and Shlomo Zilberstein. A multi-objective approach to mitigate negative side effects. In <em>Proceedings of the 29th International Joint Conference on Artificial Intelligence</em>. 2020.</p>
</dd>
<dt class="label" id="id203"><span class="brackets"><a class="fn-backref" href="#id6">8</a></span></dt>
<dd><p>Javier Garcıa and Fernando Fernández. A comprehensive survey on safe reinforcement learning. <em>Journal of Machine Learning Research</em>, 16(1):1437–1480, 2015.</p>
</dd>
<dt class="label" id="id176"><span class="brackets"><a class="fn-backref" href="#id7">9</a></span></dt>
<dd><p>Carroll L. Wainwright and Peter Eckersley. Safelife 1.0: exploring side effects in complex environments. 2019. <a class="reference external" href="https://arxiv.org/abs/1912.01217">arXiv:1912.01217</a>.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Jose Hernandez-Orallo, Fernando Martinez-Plumed, Santiago Escobar, and Pablo A. M. Casares.</p>
</div></blockquote>
</div>
</div>
<span id="document-Technical_Robustness_and_Safety/distributional_shift"></span><div class="tex2jax_ignore mathjax_ignore section" id="distributional-shift">
<h4>Distributional shift<a class="headerlink" href="#distributional-shift" title="Permalink to this headline">¶</a></h4>
<p><em>Synonyms</em>: Data shift.</p>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p>Once trained, most machine learning systems operate on static
models of the world that have been built from historical data which have
become fixed in the systems’ parameters. This freezing of the model
before it is released ‘into the wild’ makes its accuracy and reliability
especially vulnerable to changes in the underlying distribution of data.
When the historical data that have crystallised into the trained model’s
architecture cease to reflect the population concerned, the model’s
mapping function will no longer be able to accurately and reliably
transform its inputs into its target output values. These systems can
quickly become prone to error in unexpected and harmful ways. In all
cases, the system and the operators must remain vigilant to the
potentially rapid concept drifts that may occur in the complex, dynamic,
and evolving environments in which your AI project will intervene.
Remaining aware of these transformations in the data is crucial for safe AI. <a class="footnote-reference brackets" href="#def" id="id1">1</a></p>
</div>
<div class="section" id="more-in-detail">
<h5>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>A common use case of machine learning in real world settings is to learn
a model from historical data and then deploy the model on future unseen
examples. When the data distribution for the future examples differs
from the historical data distribution (i.e., the joint distribution of
inputs and outputs differs between training and test o deployment
stages), machine learning techniques that depend precariously on the
i.i.d. assumption tend to fail. This phenomena is call distributional
shift and is a very common problem <span id="id2">[<a class="reference internal" href="#id2212">3</a>]</span>. Note that a
particular case of distributional shift occurs when only the input
distribution changes (covariate shift) or there is a shift in the target
variable (prior probability shift).</p>
<p>The problem of distributional shift is of relevance not only to academic
researchers, but to the machine learning community at large.
Distributional shift is present in most practical applications, for
reasons ranging from the bias introduced by experimental design to the
irreproducibility of the testing conditions at training time. An example
is email spam filtering, which may fail to recognise spam that differs
in form from the spam the automatic filter has been built on
<span id="id3">[<a class="reference internal" href="#id2221">4</a>]</span>, yet often the model being highly confident in its
erroneous classifications. This issue is especially important in
high-risk applications of machine learning, such as finance, medicine,
and autonomous vehicles, where a mistake may incur financial or
reputational loss, or possible loss of life. It is therefore important
to assess both a model’s robustness to distribution shift and its
estimates of predictive uncertainty, which enable it to detect
distributional shifts <span id="id4">[<a class="reference internal" href="TAILOR.html#id223">1</a>, <a class="reference internal" href="#id2215">5</a>]</span>.</p>
<p>In general, the greater the degree of shift, the poorer the model’s
performance is. The performance of learned models tend to drop
significantly even with a tiny amount of distribution shift between
training and test <span id="id5">[<a class="reference internal" href="#id2213">6</a>, <a class="reference internal" href="#id2214">7</a>]</span>, which
makes it challenging to reliably deploy machine learning in real world
applications. Although one can always increase training coverage by
adding more sources of data <span id="id6">[<a class="reference internal" href="#id2119">8</a>]</span>, data augmentation
<span id="id7">[<a class="reference internal" href="#id2216">9</a>, <a class="reference internal" href="#id2217">10</a>]</span>, or injecting structural bias
into models
<span id="id8">[<a class="reference internal" href="#id2218">11</a>, <a class="reference internal" href="#id2219">12</a>, <a class="reference internal" href="#id2220">13</a>]</span>
for generalisation to any potential input for the learned model, it is
unrealistic to expect a learned model to predict accurately under any
form of distribution shift due to the combinatorial nature of real world
data and tasks.</p>
<p>On the other hand, adapting a model to a specific type of distribution
shift might be more approachable than adapting to any potential
distribution shift scenarios, under appropriate assumptions and with
appropriate modifications. By knowing where the model can predict well,
one can use the model to make conservative predictions or decisions, and
to guide future active data collection to covered shifted distributions.
Therefore, in addition to improving the generalisation performance of
models in general, methods that explicitly deal with the presence of
distribution shift are also desirable for machine learning to be used in
practice <span id="id9">[<a class="reference internal" href="#id2222">14</a>]</span>.</p>
<p>In terms of assessment, the robustness of learning models to
distributional shift is typically assessed via metrics of predictive
performance on a particular task: given two (or more) evaluation sets,
where one is considered matched to the training data and the other(s)
shifted, models which have a smaller degradation in performance on the
shifted data are considered more robust. The quality of uncertainty
estimates is often assessed via the ability to classify whether an
example came from the “in-domain” dataset or a shifted dataset using
measures of uncertainty.</p>
<p>For its part, concept shift (or concept drift) is different from
distributional shift in that it is not related to the input data or the
class distribution but instead is related to the relationship between
two or more dependent variables. An example may be the customer
purchasing behavior over time in a particular online shop. This
behaviour may be influenced by the strength of the economy, this being
not explicitly specified in the data. In this case, the concept of
interest (consumer behaviour) depends on some hidden context, not known
a priori, and not given explicitly in the form of predictive features,
making the learning task more complicated <span id="id10">[<a class="reference internal" href="#id2223">15</a>]</span>.
In this sense, concept shift can be categorised into 3 types:</p>
<ol class="simple">
<li><p><em>sudden, abrupt or instantaneous concept shift</em> (e.g., following the previous example, the COVID-19 lockdowns significantly changed customer behaviour);</p></li>
<li><p><em>gradual concept shift</em> (e.g., customers are influenced by wider economic issues, unemployment rates or other trends) which can be divided further into moderate and slow drifts, depending on the rate of the changes <span id="id11">[<a class="reference internal" href="#id2225">16</a>]</span>;</p></li>
<li><p><em>cyclic concept drifts</em>, where hidden contexts may be expected to recur due to cyclic phenomena, such as seasons of the year or may be associated with irregular phenomena, such as inflation rates or market mood <span id="id12">[<a class="reference internal" href="#id2224">17</a>]</span>.</p></li>
</ol>
<p>Concept drift may be present on supervised learning problems where
predictions are made and data is collected over time. These are
traditionally called online or incremental learning problems
<span id="id13">[<a class="reference internal" href="#id2226">18</a>]</span>, given the change expected in the data over
time. For its part, the common methods for detecting concept drift in
machine learning generally include ongoing monitoring of the performance
(e.g., accuracy) and confidence scores of a learning model. If average
performance or confidence deteriorates over time, concept shift could be
occurring</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id14"><dl class="citation">
<dt class="label" id="id170"><span class="brackets"><a class="fn-backref" href="#id2924">1</a></span></dt>
<dd><p>Leslie David. Understanding artificial intelligence ethics and safety. <em>The Alan Turing Institute</em>, 2019. URL: <a class="reference external" href="https://doi.org/10.5281/zenodo.3240529">https://doi.org/10.5281/zenodo.3240529</a>.</p>
</dd>
<dt class="label" id="id229"><span class="brackets"><a class="fn-backref" href="#id4">2</a></span></dt>
<dd><p>Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in AI safety. 2016.</p>
</dd>
<dt class="label" id="id2212"><span class="brackets"><a class="fn-backref" href="#id2">3</a></span></dt>
<dd><p>Joaquin Quiñonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. <em>Dataset shift in machine learning</em>. Mit Press, 2008.</p>
</dd>
<dt class="label" id="id2221"><span class="brackets"><a class="fn-backref" href="#id3">4</a></span></dt>
<dd><p>Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. <em>Advances in neural information processing systems</em>, 2006.</p>
</dd>
<dt class="label" id="id2215"><span class="brackets"><a class="fn-backref" href="#id4">5</a></span></dt>
<dd><p>Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: representing model uncertainty in deep learning. In <em>international conference on machine learning</em>, 1050–1059. PMLR, 2016.</p>
</dd>
<dt class="label" id="id2213"><span class="brackets"><a class="fn-backref" href="#id5">6</a></span></dt>
<dd><p>Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In <em>International Conference on Machine Learning</em>, 5389–5400. PMLR, 2019.</p>
</dd>
<dt class="label" id="id2214"><span class="brackets"><a class="fn-backref" href="#id5">7</a></span></dt>
<dd><p>Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arxiv 2013. <em>arXiv preprint arXiv:1312.6199</em>, 2013.</p>
</dd>
<dt class="label" id="id2119"><span class="brackets"><a class="fn-backref" href="#id6">8</a></span></dt>
<dd><p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. <em>arXiv preprint arXiv:1810.04805</em>, 2018.</p>
</dd>
<dt class="label" id="id2216"><span class="brackets"><a class="fn-backref" href="#id7">9</a></span></dt>
<dd><p>Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. <em>arXiv preprint arXiv:1706.06083</em>, 2017.</p>
</dd>
<dt class="label" id="id2217"><span class="brackets"><a class="fn-backref" href="#id7">10</a></span></dt>
<dd><p>Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. <em>Journal of big data</em>, 6(1):1–48, 2019.</p>
</dd>
<dt class="label" id="id2218"><span class="brackets"><a class="fn-backref" href="#id8">11</a></span></dt>
<dd><p>Kunihiko Fukushima and Sei Miyake. Neocognitron: a self-organizing neural network model for a mechanism of visual pattern recognition. In <em>Competition and cooperation in neural nets</em>, pages 267–285. Springer, 1982.</p>
</dd>
<dt class="label" id="id2219"><span class="brackets"><a class="fn-backref" href="#id8">12</a></span></dt>
<dd><p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. <em>Neural computation</em>, 9(8):1735–1780, 1997.</p>
</dd>
<dt class="label" id="id2220"><span class="brackets"><a class="fn-backref" href="#id8">13</a></span></dt>
<dd><p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. <em>Advances in neural information processing systems</em>, 2017.</p>
</dd>
<dt class="label" id="id2222"><span class="brackets"><a class="fn-backref" href="#id9">14</a></span></dt>
<dd><p>Yifan Wu. <em>Learning to Predict and Make Decisions under Distribution Shift</em>. PhD thesis, University of California, 2021.</p>
</dd>
<dt class="label" id="id2223"><span class="brackets"><a class="fn-backref" href="#id10">15</a></span></dt>
<dd><p>Alexey Tsymbal. The problem of concept drift: definitions and related work. <em>Computer Science Department, Trinity College Dublin</em>, 106(2):58, 2004.</p>
</dd>
<dt class="label" id="id2225"><span class="brackets"><a class="fn-backref" href="#id11">16</a></span></dt>
<dd><p>Kenneth O Stanley. Learning concept drift with a committee of decision trees. <em>Informe técnico: UT-AI-TR-03-302, Department of Computer Sciences, University of Texas at Austin, USA</em>, 2003.</p>
</dd>
<dt class="label" id="id2224"><span class="brackets"><a class="fn-backref" href="#id12">17</a></span></dt>
<dd><p>Michael Bonnell Harries, Claude Sammut, and Kim Horn. Extracting hidden context. <em>Machine learning</em>, 32(2):101–126, 1998.</p>
</dd>
<dt class="label" id="id2226"><span class="brackets"><a class="fn-backref" href="#id13">18</a></span></dt>
<dd><p>Gregory Ditzler and Robi Polikar. Incremental learning of concept drift from streaming imbalanced data. <em>IEEE transactions on knowledge and data engineering</em>, 25(10):2283–2301, 2012.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Jose Hernandez-Orallo, Fernando Martinez-Plumed, Santiago Escobar, and Pablo A. M. Casares.</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="def"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Definition taken from <span id="id2924">[<a class="reference internal" href="TAILOR.html#id167">1</a>]</span> under Creative Commons Attribution License 4.0.</p>
</dd>
</dl>
</div>
</div>
<span id="document-Technical_Robustness_and_Safety/security"></span><div class="tex2jax_ignore mathjax_ignore section" id="security">
<h4>Security<a class="headerlink" href="#security" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p>The goal of <strong>security</strong> encompasses the
protection of several operational dimensions of an AI system when
confronted with possible attacks, trying to take control of the system
or having access to design, operational or personal information. A
secure system is capable of maintaining the integrity of the information
that constitutes it. This includes protecting its architecture from the
unauthorised modification or damage of any of its component parts. A
secure system also keeps confidential and private information protected
even under hostile or adversarial conditions. <a class="footnote-reference brackets" href="#def2" id="id1">1</a></p>
</div>
<div class="section" id="more-in-detail">
<h5>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>Security must be an integral part of the AI process. Protecting AI
systems, their data and their communications is critical to the security
and privacy of users, as well as protecting business investments. The AI
systems themselves are incredibly expensive and possess valuable
intellectual property to protect against disclosure and misuse. The
confidentiality of the program code associated with AI systems may be
considered less critical, but access to it, as well as the ability to
manipulate this code, can result in the disclosure of important and
confidential assets.</p>
<p>Several kinds of attacks against AI systems have been reported.
Currently, the most prominent attack vector categories are
<span id="id2">[<a class="reference internal" href="#id2240">2</a>]</span>: adversarial inputs <span id="id3">[<a class="reference internal" href="#id2224">3</a>]</span>; data
poisoning attacks <span id="id4">[<a class="reference internal" href="#id2229">4</a>]</span>; model stealing techniques
<span id="id5">[<a class="reference internal" href="#id2241">5</a>, <a class="reference internal" href="#id2242">6</a>]</span>; model poisoning
<span id="id6">[<a class="reference internal" href="#id2245">7</a>]</span>, data leakage <span id="id7">[<a class="reference internal" href="#id2244">8</a>]</span> and neural
network Trojans <span id="id8">[<a class="reference internal" href="#id2243">9</a>]</span>, among others. Attack vectors
directed against the AI systems’ deployment or training environment are
equally applicable. These may be attack vectors directed against
servers, databases, protocols or libraries utilised within the AI system
<span id="id9">[<a class="reference internal" href="#id2246">10</a>]</span>.</p>
<p>Currently, AI systems often lack sufficient security assessments
<span id="id10">[<a class="reference internal" href="#id2247">11</a>]</span>. This may be the result of the mutually independent
development of AI methods and their implementation in applications:
while the application should have a security assessment, embedded AI
(via APIs or frameworks) is rarely considered in terms of its security
vulnerabilities by application developers and/or practitioners. While AI
developers may follow coding standards and guidelines for secure
software development, they will not assess the potential attack surface
of an AI system (i.e., the means by which an attacker may enter, extract
data or manipulate the system in question) using the system.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id11"><dl class="citation">
<dt class="label" id="id167"><span class="brackets"><a class="fn-backref" href="#id2921">1</a></span></dt>
<dd><p>Leslie David. Understanding artificial intelligence ethics and safety. <em>The Alan Turing Institute</em>, 2019. URL: <a class="reference external" href="https://doi.org/10.5281/zenodo.3240529">https://doi.org/10.5281/zenodo.3240529</a>.</p>
</dd>
<dt class="label" id="id2240"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Xiaofeng Liao, Liping Ding, and Yongji Wang. Secure machine learning, a brief overview. In <em>2011 Fifth International Conference on Secure Software Integration and Reliability Improvement-Companion</em>, 26–29. IEEE, 2011.</p>
</dd>
<dt class="label" id="id2224"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. <em>arXiv preprint arXiv:1412.6572</em>, 2014.</p>
</dd>
<dt class="label" id="id2229"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein. Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks. In <em>International Conference on Machine Learning</em>, 9389–9398. PMLR, 2021.</p>
</dd>
<dt class="label" id="id2241"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. Stealing machine learning models via prediction APIs. In <em>25th USENIX security symposium (USENIX Security 16)</em>, 601–618. 2016.</p>
</dd>
<dt class="label" id="id2242"><span class="brackets"><a class="fn-backref" href="#id5">6</a></span></dt>
<dd><p>Raül Fabra-Boluda, Cèsar Ferri, José Hernández-Orallo, Fernando Martínez-Plumed, and María José Ramírez-Quintana. Identifying the machine learning family from black-box models. In <em>Conference of the Spanish Association for Artificial Intelligence</em>, 55–65. Springer, 2018.</p>
</dd>
<dt class="label" id="id2245"><span class="brackets"><a class="fn-backref" href="#id6">7</a></span></dt>
<dd><p>Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. Local model poisoning attacks to Byzantine-Robust federated learning. In <em>29th USENIX Security Symposium (USENIX Security 20)</em>, 1605–1622. 2020.</p>
</dd>
<dt class="label" id="id2244"><span class="brackets"><a class="fn-backref" href="#id7">8</a></span></dt>
<dd><p>Panagiotis Papadimitriou and Hector Garcia-Molina. Data leakage detection. <em>IEEE Transactions on knowledge and data engineering</em>, 23(1):51–63, 2010.</p>
</dd>
<dt class="label" id="id2243"><span class="brackets"><a class="fn-backref" href="#id8">9</a></span></dt>
<dd><p>Yu Ji, Zixin Liu, Xing Hu, Peiqi Wang, and Youhui Zhang. Programmable neural network trojan for pre-trained feature extractor. <em>arXiv preprint arXiv:1901.07766</em>, 2019.</p>
</dd>
<dt class="label" id="id2246"><span class="brackets"><a class="fn-backref" href="#id9">10</a></span></dt>
<dd><p>Kim Hartmann and Christoph Steup. Hacking the ai-the next generation of hijacked systems. In <em>2020 12th International Conference on Cyber Conflict (CyCon)</em>, volume 1300, 327–349. IEEE, 2020.</p>
</dd>
<dt class="label" id="id2247"><span class="brackets"><a class="fn-backref" href="#id10">11</a></span></dt>
<dd><p>Why ai needs security. <span><a class="reference external" href="#"></a></span>https://www.synopsys.com/designware-ip/technical-bulletin/why-ai-needs-security-dwtb-q318.html, 2020.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Jose Hernandez-Orallo, Fernando Martinez-Plumed, Santiago Escobar, and Pablo A. M. Casares.</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="def2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Definition taken from <span id="id2921">[<a class="reference internal" href="#id167">1</a>]</span> under Creative Commons Attribution License 4.0.</p>
</dd>
</dl>
</div>
</div>
<span id="document-Technical_Robustness_and_Safety/adversarial_attack"></span><div class="tex2jax_ignore mathjax_ignore section" id="adversarial-attack">
<h4>Adversarial Attack<a class="headerlink" href="#adversarial-attack" title="Permalink to this headline">¶</a></h4>
<p><em>Synonyms</em>: Adversarial Input, Adversarial Example.</p>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p>An <strong>adversarial input</strong> is any perturbation of
the input features or observations of a system (sometimes imperceptible
to both humans and the own system) that makes the system fail or take
the system to a dangerous state. A prototypical case of an adversarial
situation happens with machine learning models, when an external
agent maliciously modify input data –often in imperceptible ways–
to induce them into misclassification or incorrect prediction. For
instance, by undetectably altering a few pixels on a picture, an
adversarial attacker can mislead a model into generating an incorrect
output (like identifying a panda as a gibbon or a ‘stop’ sign as a
‘speed limit’ sign) with an extremely high confidence. While a good
amount of attention has been paid to the risks that adversarial attacks
pose in deep learning applications like computer vision, these kinds of
perturbations are also effective across a vast range of machine learning
techniques and uses such as spam filtering and malware detection. A
different but related type of adversarial attack is called <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/data_poisoning"><span class="doc">Data Poisoning</span></a>, but this involves a malicious
compromise of data sources (used for training or testing) at the point
of collection and pre-processing.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>The vulnerabilities of AI systems to adversarial examples have
serious consequences for AI safety. The existence of cases where subtle
but targeted perturbations cause models to be misled into gross
miscalculation and incorrect decisions have potentially serious safety
implication for the adoption of critical systems like applications in
autonomous transportation, medical imaging, and security and
surveillance.</p>
<p>To get an idea of what adversarial examples look like, consider the
example in Fig. <a class="reference internal" href="#fig-advex"><span class="std std-numref">23</span></a>  shown in <span id="id1">[<a class="reference internal" href="TAILOR.html#id2224">3</a>]</span>: starting
with an image of a panda from ImageNet  <span id="id2">[<a class="reference internal" href="TAILOR.html#id263">4</a>]</span>, the
attacker adds a imperceptibly perturbation (i.e., an small vector whose
elements are equal to the sign of the elements of the gradient of the
cost function with respect to the input), to make the image be
recognised as a gibbon with high confidence by a particular deep neural
net (GoogLeNet  <span id="id3">[<a class="reference internal" href="#id2223">3</a>]</span>). Also, recent research has shown
that even in physical world scenarios, machine learning systems are
vulnerable to adversarial examples:  <span id="id4">[<a class="reference internal" href="#id2221">4</a>]</span> shows how
printed adversarial images (with modifications imperceptible to the
human eye) obtained from a cell-phone camera are not correctly
classified by the models. In general, adversarial examples have the
potential to be dangerous. For example, attackers could target
autonomous vehicles by using stickers or paint to create an adversarial
stop sign that the vehicle would interpret as a ‘yield’ or other sign,
as discussed in  <span id="id5">[<a class="reference internal" href="#id2222">5</a>]</span>.</p>
<div class="figure align-center" id="fig-advex">
<a class="reference internal image-reference" href="_images/adversarial_img_1.png"><img alt="_images/adversarial_img_1.png" src="_images/adversarial_img_1.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 23 </span><span class="caption-text">An adversarial input, overlaid on a typical image, can cause a
classifier to miscategorise a panda as a gibbon. Adapted from  <span id="id6">[<a class="reference internal" href="TAILOR.html#id2224">3</a>]</span>.</span><a class="headerlink" href="#fig-advex" title="Permalink to this image">¶</a></p>
</div>
<p>In response to concerns about the threats posed to a safe and trusted
environment for AI technologies by adversarial attacks a field called
adversarial machine learning has emerged over the past several years.
Work in this area focuses on securing systems from disruptive
perturbations at all points of vulnerability across the AI pipeline. One
of the major safety strategies that has arisen from this research is an
approach called model hardening, which has advanced techniques that
combat adversarial attacks by strengthening the architectural components
of the systems. Model hardening techniques may include adversarial
training, where training data is methodically enlarged to include
adversarial examples. Other model hardening methods involve
architectural modification, regularisation, and data pre-processing
manipulation. A second notable safety strategy is runtime detection,
where the system is augmented with a discovery apparatus that can
identify and trace in real-time the existence of adversarial examples. A
valuable collection of resources to combat adversarial attack can be
found at <a href="https://github.com/IBM/adversarialrobustness-toolbox" target=_blank>this link</a>.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id7"><dl class="citation">
<dt class="label" id="id2220"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id6">2</a>)</span></dt>
<dd><p>Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. <em>arXiv preprint arXiv:1412.6572</em>, 2014.</p>
</dd>
<dt class="label" id="id258"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, and others. Imagenet large scale visual recognition challenge. <em>International journal of computer vision</em>, 115(3):211–252, 2015.</p>
</dd>
<dt class="label" id="id2223"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 1–9. 2015.</p>
</dd>
<dt class="label" id="id2221"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In <em>Artificial intelligence safety and security</em>, pages 99–112. Chapman and Hall/CRC, 2018.</p>
</dd>
<dt class="label" id="id2222"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In <em>Proceedings of the 2017 ACM on Asia conference on computer and communications security</em>, 506–519. 2017.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Leslie David. Understanding artificial intelligence ethics and safety. The Alan Turing Institute, <a class="reference external" href="https://doi.org/10.5281/zenodo.3240529">https://doi.org/10.5281/zenodo.3240529</a>, 2019</em> by Jose Hernandez-Orallo, Fernando Martinez-Plumed, Santiago Escobar, and Pablo A. M. Casares.</p>
</div></blockquote>
</div>
</div>
<span id="document-Technical_Robustness_and_Safety/data_poisoning"></span><div class="tex2jax_ignore mathjax_ignore section" id="data-poisoning">
<h4>Data Poisoning<a class="headerlink" href="#data-poisoning" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p><strong>Data poisoning</strong> occurs when an adversary
modifies or manipulates part of the dataset upon which a model will be
trained, validated, or tested. By altering a selected subset of training
inputs, a poisoning attack can induce a trained AI system into curated
misclassification, systemic malfunction, and poor performance. An
especially concerning dimension of targeted data poisoning is that an
adversary may introduce a ‘backdoor’ into the infected model whereby the
trained system functions normally until it processes maliciously
selected inputs that trigger error or failure. Data poisoning is
possible because data collection and procurement often involves
potentially unreliable or questionable sources. When data originates in
uncontrollable environments like the internet, social media, or the
Internet of Things, many opportunities present themselves to
ill-intentioned attackers, who aim to manipulate training examples.
Likewise, in third-party data curation processes (such as
‘crowdsourced’ labelling, annotation, and content identification),
attackers may simply handcraft malicious inputs. <a class="footnote-reference brackets" href="#def3" id="id1">1</a></p>
</div>
<div class="section" id="more-in-detail">
<h5>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p><strong>Data poisoning</strong> is a security threat to AI
systems in which an attacker controls the behaviour of a system by
manipulating its training, validation or testing data
<span id="id2">[<a class="reference internal" href="TAILOR.html#id2229">4</a>]</span>. While it usually refers to the training data
for machine learning algorithms, it could also affect some other AI
systems by corrupting the testing data. Note that when the deployment
data is corrupted during operation, we are in the situation of an
<a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/adversarial_attack"><span class="doc">Adversarial Attack</span></a>. <em>Data_poisoning</em> is related to <em>data
contamination</em>, although contamination is
usually more accidental than intentional. For instance, many language
models
<span id="id3">[<a class="reference internal" href="TAILOR.html#id2119">8</a>, <a class="reference internal" href="#id2192">5</a>, <a class="reference internal" href="#id250">6</a>, <a class="reference internal" href="#id2415">7</a>, <a class="reference internal" href="#id251">8</a>, <a class="reference internal" href="#id2300">9</a>, <a class="reference internal" href="#id252">10</a>]</span>.
are trained with data that is then used for test or validation, leading
to an overoptimistic <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/evaluation"><span class="doc">Evaluation</span></a> of a system’s
behaviour.</p>
<p>In the particular case of an attacker manipulating the training data by
inserting incorrect or misleading information, as the algorithm learns
from this corrupted data, it will draw unintended and even harmful
conclusions. This type of threat is particularly relevant for deep
learning systems because they require large amounts of data to train
which is usually extracted from the web, and, at this scale, it is often
infeasible to properly vet content. We find examples such as Imagenet
<span id="id4">[<a class="reference internal" href="#id263">4</a>]</span> or the Open Images Dataset
<span id="id5">[<a class="reference internal" href="#id2231">11</a>]</span> containing tens or hundreds of millions of images
from a wide range of potentially insecure and, in many cases, unknown
sources. The current reliance of AI systems on such massive datasets
that are not manually inspected has led to fears that corrupted training
data can produce flawed models <span id="id6">[<a class="reference internal" href="#id2232">12</a>]</span>.</p>
<p>According to the breadth of the attack, data poisoning attacks fall into
two main categories: attacks targeting <em>availability</em> and attacks
targeting <em>integrity</em>. Availability attacks are usually unsophisticated
but extensive, injecting as much erroneous data as possible into a
database, so that the machine learning algorithm trained with this data
will be totally inaccurate. Attacks against the integrity of machine
learning are more complex and potentially more damaging. They leave most
of the database intact, except for an imperceptible backdoor that allows
attackers to control it. As a result, the model will apparently work as
intended but with a fatal flaw. For instance, in a cybersecurity
application, a classifier could make right predictions except when
reading a specific file type, which is considered benign because
hundreds of examples were included with that labelled in the corrupted
dataset.</p>
<p>Depending on the timing of the attack, poisoning attacks can also be
classified into two broad categories: <em>backdoor</em> and <em>triggerless
poisoning attack</em>. The former causes a model to misclassify samples at
test time that contain a particular trigger (e.g., small patches in
images or characters sequence in text)
<span id="id7">[<a class="reference internal" href="#id2234">13</a>, <a class="reference internal" href="#id2235">14</a>, <a class="reference internal" href="#id2236">15</a>, <a class="reference internal" href="#id2237">16</a>]</span>.
For example, training images could be manipulated so that a vision
system does not identify any person wearing a piece of clothing having
the trigger symbol printed on it. In this case model, the attacker
modifies both the training data (placing poisons) and test data
(inserting the trigger)
<span id="id8">[<a class="reference internal" href="#id2238">17</a>, <a class="reference internal" href="#id2239">18</a>, <a class="reference internal" href="#id2240">19</a>]</span>.
Backdoor attacks cause a victim to misclassify any image containing the
trigger. On the other hand, triggerless poisoning attacks do not require
modifications at the time of inference and cause a victim to misclassify
an individual sample <span id="id9">[<a class="reference internal" href="#id2233">20</a>]</span>.</p>
<p>Data poisoning attacks can cause considerable damage with minimal
effort. Their effectiveness is almost directly proportional to the
quality of the data. Poor quality data will produce subpar results, no
matter how advanced the model is. For instance, the experiment ImageNet
Roulette <span id="id10">[<a class="reference internal" href="#id2229">21</a>]</span> used user-uploaded and labelled
images to learn how to classify new images. Before long, the system
began using racial and gender slurs to label people. Seemingly small and
easily overlooked considerations, such as people using harmful language
on the internet, become shockingly prevalent when an AI system learns
from this data. As machine learning becomes more advanced, it will make
more connections between data points that humans would not think of. As
a result, even small changes to a database can have substantial
repercussions.</p>
<p>While data poisoning is a concern, companies can defend against it with
existing tools and techniques. The U.S. Department of Defense’s Cyber
Maturity Model Certification (CMMC) outlines four basic cyber principles
for keeping machine learning data safe<a class="footnote-reference brackets" href="#cmmc" id="id11">2</a>: network (e.g., setting up
and updating firewalls will help keep databases off-limits to internal
and external threats), facility (e.g., restricting access to data
centres), endpoint (e.g., use of data encryption, access controls and
up-to-date anti-malware software) and people protection (e.g., user
training). However, this assumes that the data is generated inside the
limits of the organisation, but many training datasets are complemented
with sources used for research or coming from social media, which are
very difficult to vet. Also, with the current trend of using pretrained
models and tuning them with smaller amounts of particular data, the risk
is more on the data used for these pretrained models than unauthorised
access to the finetuning data. Inspecting the models once trained, using
techniques from <a class="reference internal" href="TAILOR.html#document-Transparency/Transparency"><span class="doc std std-doc">explainable AI</span></a> is also
challenging, as the trapdoors may represent a very small percentage of
the behaviour of the system. Overall, <strong>data
poisoning</strong> is a complex problem that is closely
related to other major problems in AI safety, and will remain
problematic with the current paradigm of learning from massive amounts
of data.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id12"><dl class="citation">
<dt class="label" id="id168"><span class="brackets"><a class="fn-backref" href="#id2922">1</a></span></dt>
<dd><p>Leslie David. Understanding artificial intelligence ethics and safety. <em>The Alan Turing Institute</em>, 2019. URL: <a class="reference external" href="https://doi.org/10.5281/zenodo.3240529">https://doi.org/10.5281/zenodo.3240529</a>.</p>
</dd>
<dt class="label" id="id2117"><span class="brackets"><a class="fn-backref" href="#id3">2</a></span></dt>
<dd><p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. <em>arXiv preprint arXiv:1810.04805</em>, 2018.</p>
</dd>
<dt class="label" id="id2230"><span class="brackets"><a class="fn-backref" href="#id2">3</a></span></dt>
<dd><p>Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein. Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks. In <em>International Conference on Machine Learning</em>, 9389–9398. PMLR, 2021.</p>
</dd>
<dt class="label" id="id263"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, and others. Imagenet large scale visual recognition challenge. <em>International journal of computer vision</em>, 115(3):211–252, 2015.</p>
</dd>
<dt class="label" id="id2192"><span class="brackets"><a class="fn-backref" href="#id3">5</a></span></dt>
<dd><p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and others. Language models are few-shot learners. <em>Advances in neural information processing systems</em>, 33:1877–1901, 2020.</p>
</dd>
<dt class="label" id="id250"><span class="brackets"><a class="fn-backref" href="#id3">6</a></span></dt>
<dd><p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, and others. Scaling language models: methods, analysis &amp; insights from training Gopher. <em>arXiv preprint arXiv:2112.11446</em>, 2021.</p>
</dd>
<dt class="label" id="id2415"><span class="brackets"><a class="fn-backref" href="#id3">7</a></span></dt>
<dd><p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. <em>arXiv preprint arXiv:2009.03300</em>, 2020.</p>
</dd>
<dt class="label" id="id251"><span class="brackets"><a class="fn-backref" href="#id3">8</a></span></dt>
<dd><p>Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with APPS. 2021. <a class="reference external" href="https://arxiv.org/abs/2105.09938">arXiv:2105.09938</a>.</p>
</dd>
<dt class="label" id="id2300"><span class="brackets"><a class="fn-backref" href="#id3">9</a></span></dt>
<dd><p>Rishi Bommasani and others. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.</p>
</dd>
<dt class="label" id="id252"><span class="brackets"><a class="fn-backref" href="#id3">10</a></span></dt>
<dd><p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and others. Training language models to follow instructions with human feedback. <em>arXiv preprint arXiv:2203.02155</em>, 2022.</p>
</dd>
<dt class="label" id="id2231"><span class="brackets"><a class="fn-backref" href="#id5">11</a></span></dt>
<dd><p>Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, and others. The open images dataset v4. <em>International Journal of Computer Vision</em>, 128(7):1956–1981, 2020.</p>
</dd>
<dt class="label" id="id2232"><span class="brackets"><a class="fn-backref" href="#id6">12</a></span></dt>
<dd><p>Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: learning data-driven curriculum for very deep neural networks on corrupted labels. In <em>International Conference on Machine Learning</em>, 2304–2313. PMLR, 2018.</p>
</dd>
<dt class="label" id="id2234"><span class="brackets"><a class="fn-backref" href="#id7">13</a></span></dt>
<dd><p>Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. <em>arXiv preprint arXiv:1712.05526</em>, 2017.</p>
</dd>
<dt class="label" id="id2235"><span class="brackets"><a class="fn-backref" href="#id7">14</a></span></dt>
<dd><p>Jiazhu Dai, Chuanshuai Chen, and Yufeng Li. A backdoor attack against lstm-based text classification systems. <em>IEEE Access</em>, 7:138872–138878, 2019.</p>
</dd>
<dt class="label" id="id2236"><span class="brackets"><a class="fn-backref" href="#id7">15</a></span></dt>
<dd><p>Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor attacks. In <em>Proceedings of the AAAI conference on artificial intelligence</em>, volume 34, 11957–11965. 2020.</p>
</dd>
<dt class="label" id="id2237"><span class="brackets"><a class="fn-backref" href="#id7">16</a></span></dt>
<dd><p>Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-label backdoor attacks. 2018.</p>
</dd>
<dt class="label" id="id2238"><span class="brackets"><a class="fn-backref" href="#id8">17</a></span></dt>
<dd><p>Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. <em>arXiv preprint arXiv:1206.6389</em>, 2012.</p>
</dd>
<dt class="label" id="id2239"><span class="brackets"><a class="fn-backref" href="#id8">18</a></span></dt>
<dd><p>W Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, and Tom Goldstein. Metapoison: practical general-purpose clean-label data poisoning. <em>Advances in Neural Information Processing Systems</em>, 33:12080–12091, 2020.</p>
</dd>
<dt class="label" id="id2240"><span class="brackets"><a class="fn-backref" href="#id8">19</a></span></dt>
<dd><p>Chen Zhu, W Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and Tom Goldstein. Transferable clean-label poisoning attacks on deep neural nets. In <em>International Conference on Machine Learning</em>, 7614–7623. PMLR, 2019.</p>
</dd>
<dt class="label" id="id2233"><span class="brackets"><a class="fn-backref" href="#id9">20</a></span></dt>
<dd><p>Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. <em>Advances in neural information processing systems</em>, 2018.</p>
</dd>
<dt class="label" id="id2229"><span class="brackets"><a class="fn-backref" href="#id10">21</a></span></dt>
<dd><p>Kate Crawford and Trevor Paglen. Excavating ai: the politics of images in machine learning training sets. <em>AI and Society</em>, 2019.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Jose Hernandez-Orallo, Fernando Martinez-Plumed, Santiago Escobar, and Pablo A. M. Casares.</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="def3"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Definition taken from <span id="id2922">[<a class="reference internal" href="TAILOR.html#id167">1</a>]</span> under Creative Commons Attribution License 4.0.</p>
</dd>
<dt class="label" id="cmmc"><span class="brackets"><a class="fn-backref" href="#id11">2</a></span></dt>
<dd><p><a class="reference external" href="https://cmmc-coe.org/test/">https://cmmc-coe.org/test/</a></p>
</dd>
</dl>
</div>
</div>
<span id="document-Technical_Robustness_and_Safety/uncertainty"></span><div class="tex2jax_ignore mathjax_ignore section" id="uncertainty">
<h4>Uncertainty<a class="headerlink" href="#uncertainty" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p><strong>Uncertainty</strong> is the lack of information to fully determine the value or characteristics of something. In the context of Artificial Intelligence, it is commonly divided into at least two types: (1) <em>epistemic uncertainty</em> which can be reduced by additional knowledge, and (2) <em>aleatoric uncertainty</em> which can not be reduced by other means. <em>Uncertainty</em> can originate from multiple sources in an Artificial Intelligence (AI) system. The identification of its source and understanding of its nature is crucial to reduce the impact when making informed decisions.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>Artificial Intelligence (AI) solutions are commonly used to solve tasks that are too complex to define perfectly or which have high computational complexity to be solved with an exact solution. For this reason, most AI methods rely on simplifications, making assumptions about the potential set of solutions, some times incorporate imprecisions in their definition, and are based in heuristics, which add up uncertainties in their solutions. In order to capture those uncertainties, some AI methods provide the capability of incorporating uncertainty quantisation in the form of probabilities, possibilities, or variability, among others. This is of particular importance on critical applications and optimal decision making to minimise the impact of wrong decisions.
The design of an AI solution for a given problem potentially requires a long pipeline of tasks, each of which can incorporate some types of uncertainties that can add up; from the initial collection of data to the final model interpretation. Some of the possible sources of uncertainty that can be present in an AI solution are: uncertainty in the values of the collected data (e.g. imprecision of physical measurements, random noise, imperfect annotations), uncertainty on the population distributions (e.g. biases in the sampling process, dataset shift), uncertainty in the model predictions (e.g. non-calibrated class posterior probabilities <span id="id1">[<a class="reference internal" href="#id151">1</a>]</span>, expected variance of a regression model), uncertainty about the model parameters (e.g. the estimated weights of a linear regression).
A common and useful way to caracterize the different types of uncertainties is to accept that some uncertainties are intrinsic to the problem and can not be reduced. The two types are (1) epistemic uncertainty, which referest to uncertainty due to lack of knowledge and which can be reduced by plausible means. For example, in some cases the collection of additional examples, or the aclquisition of other features can reduce the uncertainty of new predictions <span id="id2">[<a class="reference internal" href="#id150">2</a>]</span>. On the other hand, <em>aleatoric uncertainty</em> refers to uncertainty that is intrinsic and can not be reduced by realistic means. For example, by collecting additional tosses of a six sided dice we can better approximate its true probability of landing on six. However, it is almost impossible to predict with certainty which side will land (even if the dice is biased), as we would require perfect knowledge representation of the physics of the dice, the environment and the time and space interactions with the dice and the environment. We accept that there are some limitations of possible knowledge that escape from our capabilities and can not be reduced. The following figure illustrates another example of the two types of uncertainty.</p>
<div class="figure align-center" id="fig-epistemic">
<a class="reference internal image-reference" href="_images/epistemic.png"><img alt="_images/epistemic.png" src="_images/epistemic.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 24 </span><span class="caption-text">Illustration of the difference between aleatoric (on the left) and epistemic uncertainty (on the right).
The example consists on two predictive events, each one with two possible outcomes.
Left: A fair coin that is tossed will have equal probability of landing on any side. Given that there is no additional information to determine its outcome, this is referred to as irreducible or aleatoric uncertainty.
Right: The word “vostojn” means heads or tails, with no additional knowledge the probability of each outcome should be equal. However, knowing that the word is from Esperanto, which is derived primarily from Romance languages, in particular Germanic languages, could remove some of the uncertainty. This type of uncertainty that is reducible by additional knowledge is referred as epistemic.</span><a class="headerlink" href="#fig-epistemic" title="Permalink to this image">¶</a></p>
</div>
<p>Several methods have been proposed to address and quantify uncertainty in AI. One of the first approaches to incorporate uncertainty in logic programming was the concept of fuzzy logic <span id="id3">[<a class="reference internal" href="#id152">3</a>]</span>, which allowed the representation of boolean logic with degrees of membership. Similarly, possibilistic logic <span id="id4">[<a class="reference internal" href="#id157">4</a>, <a class="reference internal" href="#id153">5</a>]</span> allowed the definition of first-order classigcal logic with lower bounds of necessity and possibility. The concept of imprecise probabilities was suggested by <span id="id5">Walley [<a class="reference internal" href="#id156">6</a>]</span> to quantify the uncertainty in the typical Bayesian setting, augmenting predicted class probabilities with upper and lowerbounds. The use of upper and lowerbounds has been used for cautious classification, allowing models to abstain from making a prediction in uncertain situations <span id="id6">[<a class="reference internal" href="#id155">7</a>]</span>. Conformal predictions <span id="id7">[]</span> is a frequentist approach based on statistical tests that calculates lower and upperbounds for each instance to belong to one class within a pre-specified probability. The concept of safe probability <span id="id8">[<a class="reference internal" href="#id154">8</a>]</span> is another idea to quantify the uncertainty in distributions of probabilities for a prediction, which may differ from actual beliefs or even the ground truth.
There is no unique procedure to handle <em>uncertainty</em> in AI, as the sources and types are varied. A specific analysis and understanding of each case is necessary to reduce the impact that the uncertainty could have in an AI solution.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id9"><dl class="citation">
<dt class="label" id="id151"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Telmo Silva Filho, Hao Song, Miquel Perello-Nieto, Raul Santos-Rodriguez, Meelis Kull, and Peter Flach. Classifier calibration: a survey on how to assess and improve predicted class probabilities. <em>Machine Learning</em>, 112(9):3211–3260, 2023.</p>
</dd>
<dt class="label" id="id150"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Eyke Hüllermeier and Willem Waegeman. Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods. <em>Machine learning</em>, 110(3):457–506, 2021.</p>
</dd>
<dt class="label" id="id152"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Lotfi Asker Zadeh. Fuzzy sets. <em>Information and control</em>, 8(3):338–353, 1965.</p>
</dd>
<dt class="label" id="id157"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Didier Dubois and Henri Prade. Possibilistic logic: a retrospective and prospective view. <em>Fuzzy Sets and Systems</em>, 144(1):3–23, 2004. Possibilistic Logic and Related Issues. URL: <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0165011403004433">https://www.sciencedirect.com/science/article/pii/S0165011403004433</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/j.fss.2003.10.011">doi:https://doi.org/10.1016/j.fss.2003.10.011</a>.</p>
</dd>
<dt class="label" id="id153"><span class="brackets"><a class="fn-backref" href="#id4">5</a></span></dt>
<dd><p>Didier Dubois and Henri Prade. Possibilistic logic—an overview. <em>Handbook of the History of Logic</em>, 9:283–342, 2014.</p>
</dd>
<dt class="label" id="id156"><span class="brackets"><a class="fn-backref" href="#id5">6</a></span></dt>
<dd><p>Peter Walley. <em>Statistical reasoning with imprecise probabilities</em>. Chapman &amp; Hall/CRC Monographs on Statistics &amp; Applied Probability, 1991.</p>
</dd>
<dt class="label" id="id155"><span class="brackets"><a class="fn-backref" href="#id6">7</a></span></dt>
<dd><p>César Ferri and José Hernández-Orallo. Cautious classifiers. <em>ROCAI</em>, 4:27–36, 2004.</p>
</dd>
<dt class="label" id="id154"><span class="brackets"><a class="fn-backref" href="#id8">8</a></span></dt>
<dd><p>Peter Grünwald. Safe probability. <em>Journal of Statistical Planning and Inference</em>, 195:47–63, 2018.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Miquel Perello Nieto and Peter Flach.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
<span id="document-Diversity_Non-Discrimination_and_Fairness/Diversity_Non-Discrimination_and_Fairness"></span><div class="tex2jax_ignore mathjax_ignore section" id="diversity-non-discrimination-and-fairness">
<h3>Diversity, Non-Discrimination, and Fairness<a class="headerlink" href="#diversity-non-discrimination-and-fairness" title="Permalink to this headline">¶</a></h3>
<div class="section" id="in-brief">
<h4>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h4>
<p>The term fairness is defined as the quality or state of being fair; or a
lack of favoritism towards one side. However, fairness can mean
different concepts to different peoples, different contexts, and
different disciplines <span id="id1">[<a class="reference internal" href="TAILOR.html#id2564">1</a>]</span>. An unfair Artificial
Intelligence (AI) model produces results that are biased towards
particular individuals or groups. The most relevant case of bias is
discrimination against protected-by-law social groups. Equity requires
that people are treated according to their needs, which does not mean
all people are treated equally <span id="id2">[<a class="reference internal" href="#id2429">2</a>]</span>. Justice is the
“fair and equitable treatment of all individuals under the law”
<span id="id3">[<a class="reference internal" href="TAILOR.html#id2483">1</a>]</span>.</p>
</div>
<div class="section" id="abstract">
<h4>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">¶</a></h4>
<p>We first provide motivations and background on fairness, equity and justice in AI.  This consists of warnings and legal obligations about potential harms of unscrutinized AI tools, especially in socially sensitive decision making. A taxonomy of fair-AI algorithms is then presented, based on the step of the AI development process in which fairness is checked/controlled for. Next, we summarize the guidelines and draft standards for fair-AI, and the software frameworks supporting the dimension. Finally, the main keywords of the dimension are extensively detailed.</p>
</div>
<div class="section" id="motivations-and-background">
<h4>Motivations and background <a class="footnote-reference brackets" href="#readapt" id="id4">1</a><a class="headerlink" href="#motivations-and-background" title="Permalink to this headline">¶</a></h4>
<p>Increasingly sophisticated algorithms from AI and Machine Learning (ML) support knowledge discovery from big data of human activity.
They enable the extraction of patterns and profiles of human behavior which are able to make extremely accurate predictions.
Decisions are then being partly or fully delegated to such algorithms for a wide range of socially sensitive tasks: personnel selection and wages, credit scoring, criminal justice, assisted diagnosis in medicine, personalization in schooling, sentiment analysis in texts and images, people monitoring through facial recognition, news recommendation, community bulding in social networks, dynamic pricing of services and products.</p>
<p>The benefits of algorithmic-based decision making cannot be neglected, e.g., procedural regularity – same procedure applied to each data subject. However, automated decisions based on profiling or social sorting may be biased <span id="id5">[<a class="reference internal" href="TAILOR.html#id2432">1</a>, <a class="reference internal" href="TAILOR.html#id2697">2</a>]</span> for several reasons. Historical data may contain human (cognitive) bias and discriminatory practices that are endemic, to which the algorithms assign the status of general rules. Also, the usage of AI/ML models reinforces such practices because data about model’s decisions become inputs in subsequent model construction (feedback loops).
Algorithms may wrongly interpret spurious correlations in data as causation, making predictions based on ungrounded reasons. Moreover, algorithms pursue the utilitarian optimization of quality metrics, such as accuracy of predictions, that favor precision over the majority of people against small groups. Finally, the technical process of designing and deploying algorithms is not yet mature and standardized. Rather, it is full of small and big decisions (sometimes, trial and error steps) that may hide bias, such as selecting non-representative data, performing overspecialization of the models, ignoring socio-technical impacts, or using models in deployment contexts they are not tested for. These risks are exacerbated by the fact that the AI/ML models are extremely large and complex for human understanding, or not even intelligible, sometimes they are based on randomness or time-dependent non-reproducible conditions <span id="id6">[<a class="reference internal" href="#id2463">6</a>]</span>.</p>
<p>Legal restrictions on automated decision-making are provided by the EU
General Data Protection Regulation, which states (<a href="https://gdpr-info.eu/art-22-gdpr/" target=_blank>Article 22</a>) “the right
not to be subject to a decision based solely on automated processing”.
Moreover, (<a href="https://gdpr-info.eu/recitals/no-71/" target=_blank>Recital 71</a>) “in order to ensure fair and transparent
processing in respect of the data subject […] the controller should
use appropriate mathematical or statistical procedures […] to
prevent, inter alia, discriminatory effects on natural persons”.</p>
<p>Fair algorithms are designed with the purpose of preventing biased decisions in algorithmic decision making.  Quantitative definitions have been
introduced in philosophy, economics, and machine learning in the last 50 years
<span id="id7">[<a class="reference internal" href="#id2428">7</a>, <a class="reference internal" href="#id2427">8</a>, <a class="reference internal" href="TAILOR.html#id2452">1</a>]</span>,
with more than 20 different definitions of fairness appeared thus far in
the computer science literature
<span id="id8">[<a class="reference internal" href="TAILOR.html#id2424">3</a>, <a class="reference internal" href="#id2493">11</a>]</span>.
Four non-mutually exclusive strategies can be devised for
fairness-by-design of AI/ML models.</p>
<p><em>Pre-processing approaches.</em> They consists of a controlled sanitization of the data used to train an AI/ML model with respect to specific biases. Pre-processing approaches allow for obtaining less biased data, which can be used for training AI/ML models. An advantage of pre-processing approaches is that they are independent of the AI/ML model and algorithm at hand.</p>
<p><em>In-processing approaches.</em> The second strategy is to modify the AI/ML algorithm, by incorporating fairness criteria in model construction, such as regularizing the optimization objective with a fairness measure. There is a fast growing adoption of in-processing approaches in many AI/ML problems other than in the original setting of classification, including ranking, clustering, community detection, influence maximization, distribution/allocation of goods, and models on non-structured data such as natural language texts and images.
An area somehow in the middle between pre-processing and in-processing approaches is fair representation learning, where the model inferred from data is not used directly for decision making, but rather as intermediate knowledge.</p>
<p><em>Post-processing approaches.</em> This strategy consists of post-processing an AI/ML model once it has been computed, so to identify and remove unfair decision paths. This can be achieved also by involving human experts in the exploration and interpretation of the model or of the model’s decisions.
Post-processing approaches consist of altering the model’s internals, for instance by correcting the confidence of classification rules, or the probabilities of Bayesian models. Post-processing becomes necessary for tasks for which there is no in-processing approach explicitly designed for the fairness requirement at hand.</p>
<p><em>Prediction-time approaches.</em> The last strategy assumes no change in the construction of AI/ML models, but rather correcting their predictions at run-time. Proposed approaches include promoting, demoting or rejecting predictions close to the decision boundary,  differentiating the decision boundary itself over different social groups, or wrapping a fair classifier on top of a black-box base classifier. Such approaches may be applied to legacy software, including non-AI/ML algorithms, that cannot be replaced by in-processing approaches or changed by post-processing approaches.</p>
</div>
<div class="section" id="standards-and-guidelines">
<h4>Standards and guidelines<a class="headerlink" href="#standards-and-guidelines" title="Permalink to this headline">¶</a></h4>
<p>Several initiatives have started to audit, standardize and certify algorithmic fairness, such as the <a class="reference external" href="https://ico.org.uk/about-the-ico/ico-and-stakeholder-consultations/ico-consultation-on-the-draft-ai-auditing-framework-guidance-for-organisations">ICO Draft on AI Auditing
Framework</a>,
the draft <a class="reference external" href="https://standards.ieee.org/project/7003.html">IEEE P7003™ Standard on Algorithmic Bias
Considerations</a>, the <a class="reference external" href="https://standards.ieee.org/industry-connections/ecpais.html">IEEE
Ethics Certification Program for Autonomous and Intelligent
Systems</a>,
and the <a class="reference external" href="https://www.iso.org/standard/77607.html">ISO/IEC TR 24027:2021 Bias in AI systems and AI aided decision
making</a> (see also the entry on
<a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/auditing"><span class="doc">Auditing AI</span></a>). Regarding the issue of equality data collection, the European Union
High Level Group on Non-discrimination, Equality and Diversity has set
up “<a class="reference external" href="https://ec.europa.eu/info/sites/default/files/en-guidelines-improving-collection-and-use-of-equality-data.pdf">Guidelines on improving the collection and use of equality
data</a>”,
and the European Union Agency for Fundamental Rights (FRA) maintains <a href="https://fra.europa.eu/en/promising-practices-list" target=_blank>a
list</a> of promising practices for equality data collection.</p>
<p>Very few scientific works attempt at investigating the practical applicability of fairness in AI
<span id="id9">[<a class="reference internal" href="#id2436">12</a>, <a class="reference internal" href="#id2435">13</a>]</span>.
This issue is challenging, and likely to require domain-specific
approaches <span id="id10">[<a class="reference internal" href="#id2434">14</a>]</span>. On the educational side,
however, there are hundreds of university courses on technology ethics
<span id="id11">[<a class="reference internal" href="#id2437">15</a>]</span>, many of which cover fairness in AI.</p>
</div>
<div class="section" id="software-frameworks-supporting-dimension">
<h4>Software frameworks supporting dimension<a class="headerlink" href="#software-frameworks-supporting-dimension" title="Permalink to this headline">¶</a></h4>
<!--The landscape of software libraries and tools is very large. Existing
proposals cover almost every step of the data-friven AI development
process (data collection, data processing, model development, model
deployment, model monitoring), every type of AI models (classification,
regression, clustering, ranking, community detection, influence
maximization, distribution/allocation of goods), and every type of data
(tabular, text, images, videos). Reviews and critical discussions of
gaps for a few fairness toolkits can be found in
{cite}`lee2021landscape,DBLP:journals/corr/abs-2112-05700`.-->
<p>The landscape of software libraries and tools is very large. Existing proposals cover almost every step of the data-friven AI development process (data collection, data processing, model development, model deployment, model monitoring), every type of AI models (classification, regression, clustering, ranking, community detection, influence maximization, distribution/allocation of goods), and every type of data (tabular, text, images, videos).</p>
<p>Reviews and critical discussions of gaps for a few fairness toolkits can be found in~<span id="id12">[<a class="reference internal" href="TAILOR.html#id2578">1</a>, <a class="reference internal" href="#id2438">17</a>]</span>.</p>
</div>
<div class="section" id="main-keywords">
<h4>Main keywords<a class="headerlink" href="#main-keywords" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/auditing"><span class="doc">Auditing AI</span></a>: <strong>Auditing AI</strong> aims to identify and address possible risks and impacts while ensuring robust and trustworthy (see <a class="reference internal" href="TAILOR.html#document-Accountability/L2.Accountability"><span class="doc">Accountability</span></a>).</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/bias"><span class="doc">Bias</span></a>: <strong>Bias</strong> refers to an inclination towards or against a particular individual, group, or sub-groups. AI models may inherit biases from training data or introduce new forms of bias.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/bias_factors"><span class="doc">Bias Conducive Factors</span></a>: <strong>Bias conducive factors</strong> are aspects of individuals and institutions that lead to biases in data-driven models by influencing data and tech development. This entry presents a selection of bias conducive factors in algorithmic hiring.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/bias_lmm"><span class="doc">Bias and Fairness in LLMs</span></a>: within the Natural Language Processing (NLP) field, bias manifests in several forms, possibly leading to harms and unfairness. We review here intrinsic, lantent bias; extrinsic harms, and data selection bias in Large Language Models (LLMs).</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/equity"><span class="doc">Discrimination &amp; Equity</span></a>: Forms of bias that count as discrimination against social groups or individuals should be avoided, both from legal and ethical perspectives. Discrimination can be direct or indirect, intentional or unintentional.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/fairness"><span class="doc">Fairness notions and metrics</span></a>: The term <strong>fairness</strong> is defined as the quality or state of being fair; or a lack of favoritism towards one side. The notions of fairness, and quantitative measures of them (fairness metrics), can be distinguished based on the focus on individuals, groups and sub-groups.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/fair_ML"><span class="doc">Fair Machine Learning</span></a>: <strong>Fair Machine Learning</strong> models take into account the issues of bias and fairness. Approaches can be categorized as pre-processig, which transform the input data, as in-processing, which modify the learning algorithm, and post-processing, which alter models’ internals or their decisions.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/discrimination"><span class="doc">Grounds of Discrimination</span></a>: international and national laws prohibit <strong>discriminating on some explicitly defined grounds</strong>, such as race, sex, religion, etc. They can be considered in isolation, or interacting, giving rise to multiple discrimination and intersectional discrimination.
-<a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/intersectionality"><span class="doc">Intersectionality</span></a>: <strong>Intersectionality</strong> focuses on a specific type of bias due to the combination of sensitive factors. An individual might not be discriminated against based on race or based on gender only, but she might be discriminated against because of a combination of both. Black women are particularly prone to this type of discrimination.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/justice"><span class="doc">Justice</span></a>: <strong>Justice</strong> encompasses three different perspectives: (1) <em>fairness</em> understood as the fair treatment of people, (2) <em>rightness</em> as the quality of being fair or reasonable, and (3) a legal system, the scheme or system of law. Justice can be distinguished between <em>substantive</em> and <em>procedural</em>.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/segregation"><span class="doc">Segregation</span></a>: <strong>Social segregation</strong> refers to the separation of groups on the grounds of personal or cultural traits. Separation can be physical (e.g., in schools or neighborhoods) or virtual (e.g., in social networks).</p></li>
</ul>
</div>
<div class="section" id="bibliography">
<h4>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h4>
<p id="id13"><dl class="citation">
<dt class="label" id="id2536"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Deirdre K Mulligan, Joshua A Kroll, Nitin Kohli, and Richmond Y Wong. This thing called fairness: disciplinary confusion realizing a value in technology. <em>Proceedings of the ACM on Human-Computer Interaction</em>, 3(CSCW):1–36, 2019.</p>
</dd>
<dt class="label" id="id2429"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Martha Minow. Equality vs. Equity. <em>American Journal of Law and Equality</em>, 1:167–193, 2021.</p>
</dd>
<dt class="label" id="id2456"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Jeffrey Lehman, Shirelle Phelps, and others. <em>West's encyclopedia of American law</em>. Thomson/Gale, 2004.</p>
</dd>
<dt class="label" id="id2433"><span class="brackets"><a class="fn-backref" href="#id5">4</a></span></dt>
<dd><p>Eirini Ntoutsi, Pavlos Fafalios, Ujwal Gadiraju, Vasileios Iosifidis, Wolfgang Nejdl, Maria-Esther Vidal, Salvatore Ruggieri, Franco Turini, Symeon Papadopoulos, Emmanouil Krasanakis, Ioannis Kompatsiaris, Katharina Kinder-Kurlanda, Claudia Wagner, Fariba Karimi, Miriam Fernández, Harith Alani, Bettina Berendt, Tina Kruegel, Christian Heinze, Klaus Broelemann, Gjergji Kasneci, Thanassis Tiropanis, and Steffen Staab. Bias in data-driven artificial intelligence systems - an introductory survey. <em>WIREs Data Mining Knowl. Discov.</em>, 2020.</p>
</dd>
<dt class="label" id="id2669"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>Jose M. Alvarez, Alejandra Bringas Colmenarejo, Alaa Elobaid, Simone Fabbrizzi, Miriam Fahimi, Antonio Ferrara, Siamak Ghodsi, Carlos Mougan, Ioanna Papageorgiou, Paula Reyero, Mayra Russo, Kristen M. Scott, Laura State, Xuan Zhao, and Salvatore Ruggieri. Policy advice and best practices on bias and fairness in AI. <em>Ethics Inf. Technol.</em>, 26(2):31, 2024.</p>
</dd>
<dt class="label" id="id2463"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p>Joshua A. Kroll, Joanna Huey, Solon Barocas, Edward W. Felten, Joel R. Reidenberg, David G. Robinson, and Harlan Yu. Accountable algorithms. <em>U. of Penn. Law Review</em>, 165:633–705, 2017.</p>
</dd>
<dt class="label" id="id2428"><span class="brackets"><a class="fn-backref" href="#id7">7</a></span></dt>
<dd><p>Ben Hutchinson and Margaret Mitchell. 50 years of test (un)fairness: lessons for machine learning. In <em>FAT</em>, 49–58. ACM, 2019.</p>
</dd>
<dt class="label" id="id2427"><span class="brackets"><a class="fn-backref" href="#id7">8</a></span></dt>
<dd><p>Reuben Binns. Fairness in machine learning: lessons from political philosophy. In <em>FAT</em>, volume 81 of Proceedings of Machine Learning Research, 149–159. PMLR, 2018.</p>
</dd>
<dt class="label" id="id2439"><span class="brackets"><a class="fn-backref" href="#id7">9</a></span></dt>
<dd><p>Andrea Romei and Salvatore Ruggieri. A multidisciplinary survey on discrimination analysis. <em>Knowl. Eng. Rev.</em>, 29(5):582–638, 2014.</p>
</dd>
<dt class="label" id="id2425"><span class="brackets"><a class="fn-backref" href="#id8">10</a></span></dt>
<dd><p>Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. <em>ACM Comput. Surv.</em>, 54(6):115:1–115:35, 2021.</p>
</dd>
<dt class="label" id="id2493"><span class="brackets"><a class="fn-backref" href="#id8">11</a></span></dt>
<dd><p>Indre Zliobaite. Measuring discrimination in algorithmic decision making. <em>Data Min. Knowl. Discov.</em>, 31(4):1060–1089, 2017.</p>
</dd>
<dt class="label" id="id2436"><span class="brackets"><a class="fn-backref" href="#id9">12</a></span></dt>
<dd><p>Karima Makhlouf, Sami Zhioua, and Catuscia Palamidessi. On the applicability of machine learning fairness notions. <em>SIGKDD Explor.</em>, 23(1):14–23, 2021.</p>
</dd>
<dt class="label" id="id2435"><span class="brackets"><a class="fn-backref" href="#id9">13</a></span></dt>
<dd><p>Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Allison Woodruff, Christine Luu, Pierre Kreitmann, Jonathan Bischof, and Ed H. Chi. Putting fairness principles into practice: challenges, metrics, and improvements. In <em>AIES</em>, 453–459. ACM, 2019.</p>
</dd>
<dt class="label" id="id2434"><span class="brackets"><a class="fn-backref" href="#id10">14</a></span></dt>
<dd><p>Michelle Seng Ah Lee and Luciano Floridi. Algorithmic fairness in mortgage lending: from absolute conditions to relational trade-offs. <em>Minds Mach.</em>, 31(1):165–191, 2021.</p>
</dd>
<dt class="label" id="id2437"><span class="brackets"><a class="fn-backref" href="#id11">15</a></span></dt>
<dd><p>Casey Fiesler, Natalie Garrett, and Nathan Beard. What do we teach when we teach tech ethics?: A syllabi analysis. In <em>SIGCSE</em>, 289–295. ACM, 2020.</p>
</dd>
<dt class="label" id="id2575"><span class="brackets"><a class="fn-backref" href="#id12">16</a></span></dt>
<dd><p>Michelle Seng Ah Lee and Jatinder Singh. The landscape and gaps in open source fairness toolkits. In <em>CHI</em>, 699:1–699:13. ACM, 2021.</p>
</dd>
<dt class="label" id="id2438"><span class="brackets"><a class="fn-backref" href="#id12">17</a></span></dt>
<dd><p>Brianna Richardson and Juan E. Gilbert. A framework for fairness: A systematic review of existing fair AI solutions. <em>CoRR</em>, 2021.</p>
</dd>
<dt class="label" id="id2462"><span class="brackets"><a class="fn-backref" href="#id2923">18</a></span></dt>
<dd><p>Salvatore Ruggieri. Algorithmic fairness. In <em>Elgar Encyclopedia of Law and Data Science</em>. Edward Elgar Publishing Limited, 2022.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Salvatore Ruggieri.</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="readapt"><span class="brackets"><a class="fn-backref" href="#id4">1</a></span></dt>
<dd><p>This Section was readapted from <span id="id2923">[<a class="reference internal" href="#id2462">18</a>]</span>.</p>
</dd>
</dl>
</div>
<div class="toctree-wrapper compound">
<span id="document-Diversity_Non-Discrimination_and_Fairness/auditing"></span><div class="tex2jax_ignore mathjax_ignore section" id="auditing-ai">
<h4>Auditing AI<a class="headerlink" href="#auditing-ai" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p><strong>Auditing AI</strong> aims to identify and address possible risks and impacts while ensuring robust and trustworthy <a class="reference internal" href="TAILOR.html#document-Accountability/L2.Accountability"><span class="doc">Accountability</span></a>.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>One of the measures to ensure that AI is used responsibly is the initiation of auditing practices as they facilitate to verify if the system works as intended.</p>
<p>Audits can be conducted either in-house or by external parties. The former requires internal evaluation regarding whether systems are fit, the human elements involved with the system are appropriate and monitored, and the technical elements of the system are in perfect condition and function correctly <span id="id1">[<a class="reference internal" href="#id2517">2</a>, <a class="reference internal" href="#id2518">3</a>]</span>.
The latter involves both regulators and third-parties verifying compliance <span id="id2">[<a class="reference internal" href="#id2519">4</a>]</span>.
Interestingly, critical external audits encompasses “disparate methods of journalist, technicians, and social scientist who have examined the consequences of already-deployed algorithmic systems and who have no formal relationship which the institutions designing or integrating the audited systems” <span id="id3">[<a class="reference internal" href="#id2511">5</a>]</span>.
Well-known examples of these practices such as the Propublica’s examination of Northpoint recivism prediction API <span id="id4">[<a class="reference internal" href="#id2512">6</a>]</span> or the Gender Shade Project <span id="id5">[<a class="reference internal" href="TAILOR.html#id2691">1</a>]</span>,
have played a crucial role pointing out harmful application of algorithm systems to draw the attention of the society and require companies an active role setting out governance and accountability mechanism.</p>
<p>As a result of these social demands, internal governance mechanisms
<span id="id6">[<a class="reference internal" href="#id2511">5</a>]</span> have been introduced from within the own companies that design and deployed the algorithmic systems. The goal is to propose technical and organisational procedures, among which are detailed frameworks for algorithm auditing <span id="id7">[<a class="reference internal" href="#id2513">8</a>]</span>, able to identify and address possible risk and impacts while ensuring robust and trustful accountability. In essence, precise and well-documented audits facilitate later scrutiny offering records on the reasons for the audit to be initiated, the procedures that were followed as well as the conclusions that were reached and, if carried out, the remedies or measures that were adopted.</p>
<p>To this regard, more and more voices consider audits as indispensable accountability mechanism to ensure the compliance of AI systems along their life-cycle with the different applicable legislation, concerning in particular privacy and data protection law <span id="id8">[<a class="reference internal" href="#id2516">9</a>]</span>.
Moreover, AI auditing can benefit from extensive literature in more mature disciplines, such as audit studies in social sciences <span id="id9">[<a class="reference internal" href="#id2584">10</a>]</span> and empirical economics
<span id="id10">[<a class="reference internal" href="#id2452">1</a>]</span>. Audits facilitate private entities the
provision of documentation when requested by public bodies, favouring a
systematic governance <span id="id11">[<a class="reference internal" href="#id2515">11</a>]</span> of AI systems through a
general transparency and enforcement regime. This joint effort between
public and private institutions would, in turn, result in collaborative
governance scheme <span id="id12">[<a class="reference internal" href="#id2515">11</a>]</span>.</p>
<p>The recent <a class="reference internal" href="TAILOR.html#document-main/Ethical_Legal_Framework/AI_ACT"><span class="doc std std-doc">EU Artificial Intelligence Act</span></a> can be seen as a proposal to
establish a Europe-wide ecosystem for conducting AI auditing <span id="id13">[<a class="reference internal" href="#id2580">12</a>]</span> and in line with that idea more and more
research is done on auditing procedures for algorithms (for reviews see <span id="id14">[<a class="reference internal" href="#id2589">13</a>, <a class="reference internal" href="#id2590">14</a>]</span>). For example,
<span id="id15">[<a class="reference internal" href="#id2513">8</a>]</span> propose a framework for internal AI auditing which includes both ethical aspects (a social impact assessment and ethical risk analysis chart) and technical audits (such as adversarial testing and a Failure Modes and Effect Analysis). Such audits are often supported by technical documentation, such as the Datasheets for Datasets proposal <span id="id16">[<a class="reference internal" href="#id2583">15</a>]</span> to maintain information on datasets used to train AI systems. Such documentation can both help to ensure that AI systems are deployed for tasks in line with the data they
were trained on and help to spot ethical risks stemming from the data <span id="id17">[<a class="reference internal" href="#id2585">16</a>]</span>, such as <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/bias"><span class="doc std std-doc">biases</span></a>.</p>
<p>Ethical risks can also be the sole focus of AI audits, as in ethics-based auditing (proposed for AI in <span id="id18">[<a class="reference internal" href="#id2581">17</a>]</span>). While still in development, several options are emerging where: “functionality audits focus on the rationale behind decisions; code audits entail reviewing the source code of an algorithm; and impact audits investigate the types, severity, and prevalence of effects of an algorithm’s outputs.” <span id="id19">[<a class="reference internal" href="#id2493">18</a>]</span> For these audits in particular determining what is measured can be a challenge, as it is difficult to define clear metrics on which ethical aspects of AI systems can be evaluated.
<em>Fairness metrics</em> (cf. the entry on <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/fairness"><span class="doc">Fairness notions and metrics</span></a>) can certainly help here, but as discussed there is a difficulty in the selection of the right metric and even then there are limitations and trade-offs with other metrics. In addition, for the integration of AI ethics in ESG (Environmental, Social and Governance) reporting towards investors <span id="id20">[<a class="reference internal" href="#id2587">19</a>]</span>
such fairness metrics need not give sufficient insights into whether algorithms are used responsibly at an organisational level.
Existing ESG criteria for organizational audits may help here, as well as work on KPIs for Responsible Research and Innovation <span id="id21">[<a class="reference internal" href="#id2591">20</a>]</span>. Despite all this work on metrics, it is however still an open question to what extent ethics can be captured in numbers the way other aspects of audits are, with some arguing that it is impossible to develop benchmarks for how ethical an AI system is <span id="id22">[<a class="reference internal" href="#id2586">21</a>]</span>. Instead, they argue, the focus should be on values and value trade-offs.</p>
<p>Z-Inspection, another auditing framework proposed based on the European High Level Expert Group’s Guidelines for Trustworthy AI <span id="id23">[<a class="reference internal" href="#id2594">22</a>]</span>, takes values as its starting point <span id="id24">[<a class="reference internal" href="#id2592">23</a>]</span>.
As can also be seen in a case study for the framework involving an algorithm that recognizes cardiac arrests in emergency calls <span id="id25">[<a class="reference internal" href="#id2593">24</a>]</span> this framework proceeds from a wide
identification of stakeholders and their values to the analysis of (socio-)technical scenario’s to reach an identification and (potentially) resolution of ethical, technical and legal issues of an AI
system. Ultimately this still depends on the translation of values into metrics, and so the main challenge of developing such metrics stands regardless of one’s auditing approach.</p>
<p>Standards represent a natural framework for the proceduralization of audits. Certification by neutral third party states compliance to certain standards as the result of auditing. Several draft proposals are being prepared which include (at least implicitely) elements for conducting audits, such as the following:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.iso.org/standard/77607.html">ISO/IEC TR 24027:2021</a> - Artificial intelligence (AI) — Bias in AI systems and AI aided decision making.</p></li>
<li><p><a class="reference external" href="https://standards.ieee.org/ieee/7010/7718/">IEEE 7010-2020</a>:
Recommended Practice for Assessing the Impact of Autonomous and
Intelligent Systems on Human Well-being.</p></li>
<li><p><a class="reference external" href="https://standards.ieee.org/ieee/2863/10142/">IEEE P2863</a> -
Recommended Practice for Organizational Governance of Artificial
Intelligence.</p></li>
<li><p><a class="reference external" href="https://engagestandards.ieee.org/ieeecertifaied.html">IEEE CertifAIEd</a> –
Ontological Specification for Ethical Accountability</p></li>
<li><p><a class="reference external" href="https://engagestandards.ieee.org/ieeecertifaied.html">IEEE CertifAIEd</a> –
Ontological Specification for Ethical Algorithmic Bias.</p></li>
<li><p><a class="reference external" href="https://www.nist.gov/itl/ai-risk-management-framework">NIST AI Risk Management Framework</a>.</p></li>
</ul>
<p>However, there is not yet a formal professional standard to guide
auditors of AI systems, yet some <a href="https://ec.europa.eu/futurium/en/system/files/ged/auditing-artificial-intelligence.pdf" target=_blank>guidelines</a> exist.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id26"><dl class="citation">
<dt class="label" id="id2452"><span class="brackets"><a class="fn-backref" href="#id10">1</a></span></dt>
<dd><p>Andrea Romei and Salvatore Ruggieri. A multidisciplinary survey on discrimination analysis. <em>Knowl. Eng. Rev.</em>, 29(5):582–638, 2014.</p>
</dd>
<dt class="label" id="id2517"><span class="brackets"><a class="fn-backref" href="#id1">2</a></span></dt>
<dd><p>Ada Lovelace and UK DataKind. Examining the black box: tools for assessing algorithmic systems. Technical Report, Technical report, AdaLovelace Institute, 2020. URL: <a class="reference external" href="https://www.adalovelaceinstitute.org/report/examining-the-black-box-tools-for-assessing-algorithmic-systems/">https://www.adalovelaceinstitute.org/report/examining-the-black-box-tools-for-assessing-algorithmic-systems/</a>.</p>
</dd>
<dt class="label" id="id2518"><span class="brackets"><a class="fn-backref" href="#id1">3</a></span></dt>
<dd><p>Emre Kazim, Danielle Mendes Thame Denny, and Adriano Koshiyama. Ai auditing and impact assessment: according to the uk information commissioner’s office. <em>AI and Ethics</em>, 1(3):301–310, 2021.</p>
</dd>
<dt class="label" id="id2519"><span class="brackets"><a class="fn-backref" href="#id2">4</a></span></dt>
<dd><p>Jennifer Cobbe, Michelle Seng Ah Lee, and Jatinder Singh. Reviewable automated decision-making: a framework for accountable algorithmic systems. In <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, 598–609. 2021.</p>
</dd>
<dt class="label" id="id2511"><span class="brackets">5</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id6">2</a>)</span></dt>
<dd><p>Jacob Metcalf, Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, and Madeleine Clare Elish. Algorithmic impact assessments and accountability: the co-construction of impacts. In <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, 735–746. 2021.</p>
</dd>
<dt class="label" id="id2512"><span class="brackets"><a class="fn-backref" href="#id4">6</a></span></dt>
<dd><p>Jeff Larson, Surya Mattu, Lauren Kirchner, and Julia Angwin. How we analyzed the compas recidivism algorithm. <em>ProPublica (5 2016)</em>, 9(1):3–3, 2016.</p>
</dd>
<dt class="label" id="id2703"><span class="brackets"><a class="fn-backref" href="#id5">7</a></span></dt>
<dd><p>Joy Adowaa Buolamwini. <em>Gender shades: Intersectional phenotypic and demographic evaluation of face datasets and gender classifiers</em>. PhD thesis, Massachusetts Institute of Technology, 2017.</p>
</dd>
<dt class="label" id="id2513"><span class="brackets">8</span><span class="fn-backref">(<a href="#id7">1</a>,<a href="#id15">2</a>)</span></dt>
<dd><p>Inioluwa Deborah Raji, Andrew Smart, Rebecca N. White, Margaret Mitchell, Timnit Gebru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, and Parker Barnes. Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing. In <em>FAT*</em>, 33–44. ACM, 2020.</p>
</dd>
<dt class="label" id="id2516"><span class="brackets"><a class="fn-backref" href="#id8">9</a></span></dt>
<dd><p>Bryan Casey, Ashkon Farhangi, and Roland Vogl. Rethinking explainable machines: the gdpr's' right to explanation'debate and the rise of algorithmic audits in enterprise. <em>Berkeley Tech. LJ</em>, 34:143, 2019.</p>
</dd>
<dt class="label" id="id2584"><span class="brackets"><a class="fn-backref" href="#id9">10</a></span></dt>
<dd><p>Briana Vecchione, Karen Levy, and Solon Barocas. Algorithmic auditing and social justice: lessons from the history of audit studies. In <em>EAAMO</em>, 19:1–19:9. ACM, 2021.</p>
</dd>
<dt class="label" id="id2515"><span class="brackets">11</span><span class="fn-backref">(<a href="#id11">1</a>,<a href="#id12">2</a>)</span></dt>
<dd><p>Margot E Kaminski and Gianclaudio Malgieri. Algorithmic impact assessments under the GDPR: producing multi-layered explanations. <em>International Data Privacy Law</em>, pages 19–28, 2020.</p>
</dd>
<dt class="label" id="id2580"><span class="brackets"><a class="fn-backref" href="#id13">12</a></span></dt>
<dd><p>Jakob Mökander, Maria Axente, Federico Casolari, and Luciano Floridi. Conformity assessments and post-market monitoring: a guide to the role of auditing in the proposed european ai regulation. <em>Minds and Machines</em>, pages 1–28, 2021.</p>
</dd>
<dt class="label" id="id2589"><span class="brackets"><a class="fn-backref" href="#id14">13</a></span></dt>
<dd><p>Adriano Koshiyama, Emre Kazim, Philip Treleaven, Pete Rai, Lukasz Szpruch, Giles Pavey, Ghazi Ahamat, Franziska Leutner, Randy Goebel, Andrew Knight, and others. Towards algorithm auditing: a survey on managing legal, ethical and technological risks of AI, ML and associated algorithms. <em>SSRN Electronic Journal</em>, 2021.</p>
</dd>
<dt class="label" id="id2590"><span class="brackets"><a class="fn-backref" href="#id14">14</a></span></dt>
<dd><p>Danaë Metaxa, Joon Sung Park, Ronald E Robertson, Karrie Karahalios, Christo Wilson, Jeff Hancock, Christian Sandvig, and others. Auditing algorithms: understanding algorithmic systems from the outside in. <em>Foundations and Trends® in Human–Computer Interaction</em>, 14(4):272–344, 2021.</p>
</dd>
<dt class="label" id="id2583"><span class="brackets"><a class="fn-backref" href="#id16">15</a></span></dt>
<dd><p>Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. Datasheets for datasets. <em>Communications of the ACM</em>, 64(12):86–92, 2021.</p>
</dd>
<dt class="label" id="id2585"><span class="brackets"><a class="fn-backref" href="#id17">16</a></span></dt>
<dd><p>Karen L Boyd. Datasheets for datasets help ml engineers notice and understand ethical issues in training data. <em>Proceedings of the ACM on Human-Computer Interaction</em>, 5(CSCW2):1–27, 2021.</p>
</dd>
<dt class="label" id="id2581"><span class="brackets"><a class="fn-backref" href="#id18">17</a></span></dt>
<dd><p>Jakob Mökander and Luciano Floridi. Ethics-based auditing to develop trustworthy ai. <em>Minds and Machines</em>, 31(2):323–327, 2021.</p>
</dd>
<dt class="label" id="id2493"><span class="brackets"><a class="fn-backref" href="#id19">18</a></span></dt>
<dd><p>Jakob Mökander, Jessica Morley, Mariarosaria Taddeo, and Luciano Floridi. Ethics-based auditing of automated decision-making systems: nature, scope, and limitations. <em>Science and engineering ethics</em>, 27(4):1–30, 2021.</p>
</dd>
<dt class="label" id="id2587"><span class="brackets"><a class="fn-backref" href="#id20">19</a></span></dt>
<dd><p>Matti Minkkinen, Anniina Niukkanen, and Matti Mäntymäki. What about investors? ESG analyses as tools for ethics-based AI auditing. <em>AI &amp; SOCIETY</em>, pages 1–15, 2022.</p>
</dd>
<dt class="label" id="id2591"><span class="brackets"><a class="fn-backref" href="#id21">20</a></span></dt>
<dd><p>Zenlin Kwee, Emad Yaghmaei, and Steven Flipse. Responsible research and innovation in practice an exploratory assessment of key performance indicators (kpis) in a nanomedicine project. <em>Journal of Responsible Technology</em>, 5:100008, 2021.</p>
</dd>
<dt class="label" id="id2586"><span class="brackets"><a class="fn-backref" href="#id22">21</a></span></dt>
<dd><p>Travis LaCroix and Alexandra Sasha Luccioni. Metaethical perspectives on'benchmarking'ai ethics. <em>arXiv preprint arXiv:2204.05151</em>, 2022. URL: <a class="reference external" href="https://arxiv.org/abs/2204.05151">https://arxiv.org/abs/2204.05151</a>.</p>
</dd>
<dt class="label" id="id2594"><span class="brackets"><a class="fn-backref" href="#id23">22</a></span></dt>
<dd><p>Nathalie Smuha. The EU approach to ethics guidelines for trustworthy Artificial Intelligence. In <em>Computer Law Review International</em>, volume 20, 97–106. 2019.</p>
</dd>
<dt class="label" id="id2592"><span class="brackets"><a class="fn-backref" href="#id24">23</a></span></dt>
<dd><p>Roberto V Zicari, John Brodersen, James Brusseau, Boris Düdder, Timo Eichhorn, Todor Ivanov, Georgios Kararigas, Pedro Kringen, Melissa McCullough, Florian Möslein, and others. Z-inspection®: a process to assess trustworthy AI. <em>IEEE Transactions on Technology and Society</em>, 2(2):83–97, 2021.</p>
</dd>
<dt class="label" id="id2593"><span class="brackets"><a class="fn-backref" href="#id25">24</a></span></dt>
<dd><p>Roberto V Zicari, James Brusseau, Stig Nikolaj Blomberg, Helle Collatz Christensen, Megan Coffee, Marianna B Ganapini, Sara Gerke, Thomas Krendl Gilbert, Eleanore Hickman, Elisabeth Hildt, and others. On assessing trustworthy ai in healthcare. machine learning as a supportive tool to recognize cardiac arrest in emergency calls. <em>Frontiers in Human Dynamics</em>, pages 30, 2021.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Alejandra Bringas Colmenarejo, Stefan Buijsman, and Salvatore Ruggieri.</p>
</div></blockquote>
</div>
</div>
<span id="document-Diversity_Non-Discrimination_and_Fairness/bias"></span><div class="tex2jax_ignore mathjax_ignore section" id="bias">
<h4>Bias<a class="headerlink" href="#bias" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p><strong>Bias</strong> refers to an inclination towards or against a particular individual, group,
or sub-groups. AI models may inherit biases from training data or introduce
new forms of bias.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>The success of Machine Learning (ML) systems in visual recognition, online advertising, and recommendation systems have inspired its use in applications such as employee hiring, legal systems, social systems, and voice interfaces such as Alexa, Siri, and the like. Along with the proliferation of these domains, a significant concern regarding the trustworthiness of decisions has risen due to various biases (or systematic errors) which may produce skewed results in the automated decision making. The word ‘bias’ has an established normative explanation in legal language, where it refers to “judgement based on preconceived notions or prejudices, as opposed to the impartial evaluation of facts” <span id="id1">[<a class="reference internal" href="#id2592">4</a>]</span>. In a more generalized version, bias refers to an inclination towards or against a particular individual, group, or sub-groups. The real world is often described as biased in this sense, and since machine learning techniques simply imitate observations of the world, it should come as no surprise that the resulting systems also capture the same bias <span id="id2">[<a class="reference internal" href="#id2432">1</a>, <a class="reference internal" href="TAILOR.html#id2697">2</a>]</span>.</p>
<p>Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) <span id="id3">[<a class="reference internal" href="#id2531">5</a>]</span> software for recidivism prediction, used by the U.S department courts to decide whether to release a person or keep them in prison, has discontinued its use after a careful investigation conducted by the U.S officers as they concluded that the software is biased against African-Americans. Also, the use of predictive policing <span id="id4">[<a class="reference internal" href="#id2532">6</a>]</span> software has been ceased due to the presence of racial biases. Amazon’s employment hiring <span id="id5">[<a class="reference internal" href="#id2533">7</a>]</span> application realized that it is biased against women. Content personalization and ad ranking systems have been accused of filter bubbles and racial and gender profiling. Bidirectional Encoder Representations from Transformers (BERT), has shown signs of gender biases in google search as it is observed that the gender-neutral terms (such as receptionist, doctor, nurse etc.) acquire stereotype and bias due to the context in which they are present in the corpus <span id="id6">[<a class="reference internal" href="#id2593">8</a>]</span>. In image domain, a latest gender classification report from the National Institute for Standards and Technology (NIST) pointed out that image classification algorithms performed worse for female-labeled faces than male-labeled faces , exhibit gender biases <span id="id7">[<a class="reference internal" href="#id2594">9</a>]</span>. A bias can exist in different forms and shapes based on the domain and context of application. The main reasons for the origin of these biases are manifold. An outline of bias-inducing stages in the ML pipeline is detailed in <span id="id8">[<a class="reference internal" href="#id2424">3</a>]</span>. Based on this study, bias definitions can be induced in data, algorithms, and user interaction feedback loops.</p>
<p>ML systems are primarly based on  data-driven approaches; therefore, the outcome of ML-based decision-making processes depends on the input data and the interpretation of that data. This decision-making process involves numerous data analyses, such as uncovering patterns in the data, finding correlations and trends, missing data imputations, and data pre-processing. The performance of ML models depends on the data used to train these models and the analysis performed on the training data. It is noted that the primary source of biases is from the data and its processing- involves what data was used for training, how it was collected, and how the data was generated and pre-processed. A general definition of dataset bias is that the data is not representative of the population of study <span id="id9">[<a class="reference internal" href="#id2534">10</a>]</span>. Nevertheless, in a broader sense, it also occurs when the data does not contain features for specific applications we are interested in. Additionally, human interactions with the data produce bias against a specific group or individual <span id="id10">[<a class="reference internal" href="#id2534">10</a>]</span>. Various forms of dataset biases have been identified in ML systems. Sample/selection biases emerge due to the non-random sampling of groups and sub-groups. Exclusion bias arises at the data pre-processing stage when valuable data are omitted thought to be unnecessary. When the data used for training a model is different from the data collected from the real world, for example, the training data is collected using a fixed camera in image training, but the production data is collected using different cameras, a measurement bias can be occurred. Recall bias is a kind of measurement bias, and it occurs when similar data are inconsistently labeled. Observer bias occurs when we observe data based on what we want to see or expect to see. Association biases are resultant of the spurious correlations between features in the data.</p>
<p>Furthermore, algorithmic biases are systematic errors in computer systems or models that cause certain privileges in outcomes concerning a particular group or a person. These biases can emerge in various ways. Foremost among these are the design of the algorithm or the way it uses the datasets to be coded, collected, selected, and processed. Algorithmic errors may lead to biased outcomes even though the data used for training are unbiased.
A clear example is pre-existing bias, arising as the result of underlying social and institutional ideologies <span id="id11">[<a class="reference internal" href="#id2641">11</a>]</span>. Another algorithmic bias is caused by technical biases manifested due the technical limitations of code, its computational resources, its design, and the constraints on the system. Technical biases are more frequent when we rely more on the algorithm in other domains or unanticipated contexts. Moreover, and related to the algorithm internals, correlation biases materialise when algorithms assume conclusions from the correlations in data attributes without knowing the specific purpose of those attributes. Finally, another known bias – in between the identified ones – are feedback loop biases. These arise when there is a recursion error in the mechanism in which information is processed into the data-model-experience pipeline.</p>
<p>The skewed outcomes from the biased data or (and) biased algorithms affect user decisions which may result in a more biased data for future ML systems. For example, consider a search engine which ranks queries. The end users interact mostly with the top ranked results, rather than going down the list, that can affect popularity and user interest of the upcoming decisions, due to the biased interactions.</p>
<p>As a long term vision to create responsible ML systems, identifying and mitigating biases throughout the ML development life cycle should be given paramount importance. In a broader sense, the different ways the bias could be mitigated are:</p>
<ol class="simple">
<li><p>Identify and define potential sources.</p></li>
<li><p>Set up guidelines and rules for data collection as well as a model use.</p></li>
<li><p>Define accurate representative data for training.</p></li>
<li><p>Properly document and share how the entire data collection process has been done.</p></li>
<li><p>Incorporate ways to measure and mitigate biases as part of the standard evaluation procedure.</p></li>
</ol>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id12"><dl class="citation">
<dt class="label" id="id2432"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>Eirini Ntoutsi, Pavlos Fafalios, Ujwal Gadiraju, Vasileios Iosifidis, Wolfgang Nejdl, Maria-Esther Vidal, Salvatore Ruggieri, Franco Turini, Symeon Papadopoulos, Emmanouil Krasanakis, Ioannis Kompatsiaris, Katharina Kinder-Kurlanda, Claudia Wagner, Fariba Karimi, Miriam Fernández, Harith Alani, Bettina Berendt, Tina Kruegel, Christian Heinze, Klaus Broelemann, Gjergji Kasneci, Thanassis Tiropanis, and Steffen Staab. Bias in data-driven artificial intelligence systems - an introductory survey. <em>WIREs Data Mining Knowl. Discov.</em>, 2020.</p>
</dd>
<dt class="label" id="id2668"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Jose M. Alvarez, Alejandra Bringas Colmenarejo, Alaa Elobaid, Simone Fabbrizzi, Miriam Fahimi, Antonio Ferrara, Siamak Ghodsi, Carlos Mougan, Ioanna Papageorgiou, Paula Reyero, Mayra Russo, Kristen M. Scott, Laura State, Xuan Zhao, and Salvatore Ruggieri. Policy advice and best practices on bias and fairness in AI. <em>Ethics Inf. Technol.</em>, 26(2):31, 2024.</p>
</dd>
<dt class="label" id="id2424"><span class="brackets"><a class="fn-backref" href="#id8">3</a></span></dt>
<dd><p>Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. <em>ACM Comput. Surv.</em>, 54(6):115:1–115:35, 2021.</p>
</dd>
<dt class="label" id="id2592"><span class="brackets"><a class="fn-backref" href="#id1">4</a></span></dt>
<dd><p>A Campolo, M Sanfilippo, M Whittaker, and K Crawford. Ai now 2017 symposium and workshop. <em>AI Now Institute at New York University</em>, 2018.</p>
</dd>
<dt class="label" id="id2531"><span class="brackets"><a class="fn-backref" href="#id3">5</a></span></dt>
<dd><p>Tim Brennan and William Dieterich. Correctional offender management profiles for alternative sanctions (compas). <em>Handbook of Recidivism Risk/Needs Assessment Tools (2018)</em>, 2018.</p>
</dd>
<dt class="label" id="id2532"><span class="brackets"><a class="fn-backref" href="#id4">6</a></span></dt>
<dd><p>Kiana Alikhademi, Emma Drobina, Diandra Prioleau, Brianna Richardson, Duncan Purves, and Juan E Gilbert. A review of predictive policing from the perspective of fairness. <em>Artificial Intelligence and Law</em>, pages 1–17, 2021.</p>
</dd>
<dt class="label" id="id2533"><span class="brackets"><a class="fn-backref" href="#id5">7</a></span></dt>
<dd><p>Akhil Alfons Kodiyan. An overview of ethical issues in using ai systems in hiring with a case study of amazon’s ai based hiring tool. <em>Researchgate Preprint</em>, 2019.</p>
</dd>
<dt class="label" id="id2593"><span class="brackets"><a class="fn-backref" href="#id6">8</a></span></dt>
<dd><p>Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: measuring stereotypical bias in pretrained language models. In <em>ACL/IJCNLP (1)</em>, 5356–5371. Association for Computational Linguistics, 2021.</p>
</dd>
<dt class="label" id="id2594"><span class="brackets"><a class="fn-backref" href="#id7">9</a></span></dt>
<dd><p>Patrick J Grother, Patrick J Grother, and Mei Ngan. <em>Face recognition vendor test (frvt)</em>. US Department of Commerce, National Institute of Standards and Technology, 2014.</p>
</dd>
<dt class="label" id="id2534"><span class="brackets">10</span><span class="fn-backref">(<a href="#id9">1</a>,<a href="#id10">2</a>)</span></dt>
<dd><p>Tatiana Tommasi, Novi Patricia, Barbara Caputo, and Tinne Tuytelaars. A deeper look at dataset bias. In <em>Domain adaptation in computer vision applications</em>, pages 37–55. Springer, 2017.</p>
</dd>
<dt class="label" id="id2641"><span class="brackets"><a class="fn-backref" href="#id11">11</a></span></dt>
<dd><p>Roel Dobbe, Sarah Dean, Thomas Krendl Gilbert, and Nitin Kohli. A broader view on bias in automated decision-making: reflecting on epistemology and dynamics. <em>CoRR</em>, 2018. <a class="reference external" href="https://arxiv.org/abs/1807.00553">arXiv:1807.00553</a>.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Resmi Ramachandranpillai, Fredrik Heintz, Miguel Couceiro, and Gabriel Gonzalez-Castañé.</p>
</div></blockquote>
</div>
</div>
<span id="document-Diversity_Non-Discrimination_and_Fairness/bias_factors"></span><div class="tex2jax_ignore mathjax_ignore section" id="bias-conducive-factors">
<h4>Bias Conducive Factors<a class="headerlink" href="#bias-conducive-factors" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p><strong>Bias conducive factors</strong> are aspects of individuals and institutions that lead to biases in data-driven models by influencing data and tech development. This entry presents a selection of bias conducive factors in algorithmic hiring.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>We present a selection of bias conducive factors distinguishing between 1) overarching factors, 2) institutional biases, 3) individual preferences, and 4) technology blindspots.</p>
<div class="section" id="overarching-factors">
<h6>Overarching factors<a class="headerlink" href="#overarching-factors" title="Permalink to this headline">¶</a></h6>
<p>Overarching factors are fundamental biases interacting with all other factors and leading to worse algorithmic outcomes for disadvantaged groups.</p>
<p><em>Stereotypes</em>. Stereotypes, shaped by culture, socialization, and experience, are commonly held beliefs about groups based on shared characteristics, impacting their perceived suitability for certain roles. Acquired early and often activated unconsciously <span id="id1">[<a class="reference internal" href="#id2650">1</a>, <a class="reference internal" href="#id2651">2</a>]</span>, stereotypes persistently influence individuals’ lives, both descriptively and prescriptively <span id="id2">[<a class="reference internal" href="#id2652">3</a>]</span>. Consequently, they mold expectations regarding personal and others’ qualities, priorities, and needs, particularly in the context of work <span id="id3">[<a class="reference internal" href="#id2653">4</a>, <a class="reference internal" href="#id2654">5</a>]</span>. For instance, men are typically associated with <em>agency</em> (leadership and goal achievement), while women are associated with <em>communion</em> (warmth and caregiving), leading to widespread effects on gender roles and expectations in employment <span id="id4">[<a class="reference internal" href="#id2655">6</a>, <a class="reference internal" href="#id2656">7</a>]</span>.</p>
<p><em>Proxies</em>. Sensitive attributes are normally not used directly as input features to algorithms. Despite this fact, sensitive attribute information is still available to data driven methods through variables that are strongly correlated with sensitive attributes, referred to as proxies. In algorithmic hiring, for example, video interviews and resumes encode information on gender and race  <span id="id5">[<a class="reference internal" href="#id2657">8</a>, <a class="reference internal" href="#id2658">9</a>]</span>.</p>
</div>
<div class="section" id="institutional-biases">
<h6>Institutional biases<a class="headerlink" href="#institutional-biases" title="Permalink to this headline">¶</a></h6>
<p>Institutional biases stem from practices, habits, and norms of institutions.</p>
<p><em>Horizontal segregation</em>. Horizontal segregation, a significant aspect of job allocation, influences hiring decisions and future workforce dynamics. Past experience is a key determinant of job suitability <span id="id6">[<a class="reference internal" href="#id2659">10</a>]</span>. This segregation involves disparities in employment rates among industries due to factors like gender and race <span id="id7">[<a class="reference internal" href="#id2660">11</a>, <a class="reference internal" href="#id2661">12</a>]</span> (see also the entry <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/segregation"><span class="doc">Segregation</span></a>). Gender biases, rooted in enduring stereotypes about traits like agency and communion, contribute to pronounced gender imbalances across various regions. Field experiments consistently reveal discrimination against individuals in industries dominated by the opposite gender <span id="id8">[<a class="reference internal" href="#id2662">13</a>, <a class="reference internal" href="#id2663">14</a>]</span>.</p>
<p><em>Vertical segregation</em>. Vertical segregation refers to disparities in career advancement toward leadership roles, traditionally examined through a gender lens <span id="id9">[<a class="reference internal" href="#id2664">15</a>, <a class="reference internal" href="#id2665">16</a>]</span>. However, recent research extends this concept to non-binary individuals, racial minorities, and intersectional identities <span id="id10">[<a class="reference internal" href="#id2666">17</a>, <a class="reference internal" href="#id2667">18</a>]</span>. These disparities, when formalized into data, perpetuate wage gaps <span id="id11">[<a class="reference internal" href="#id2668">19</a>]</span> and the underrepresentation of diverse groups in high-ranking positions.</p>
</div>
<div class="section" id="individual-preferences">
<h6>Individual preferences<a class="headerlink" href="#individual-preferences" title="Permalink to this headline">¶</a></h6>
<p>Individual preferences shaping generalized patterns for protected groups are presented in this section. Caveat: Categorizing bias in this manner doesn’t assign individual responsibility or justify discrimination. Instead, it underscores how apparent individual choices often stem from broader recurring patterns linked to protected attributes.</p>
<p><em>Job satisfaction</em>. Job satisfaction plays a key role in job commitment <span id="id12">[<a class="reference internal" href="#id2669">20</a>, <a class="reference internal" href="#id2670">21</a>]</span>. However, historically marginalized groups—like transgender, nonbinary, women, Black, and disabled workers—are more prone to workplace discrimination and harassment <span id="id13">[<a class="reference internal" href="#id2670">21</a>, <a class="reference internal" href="#id2671">22</a>]</span>. This issue can be seen in data sets showing lower job tenure for these groups, potentially leading to biases in algorithms designed to maximize tenure to cut hiring costs and retain talent.</p>
<p><em>Differences in salary negotiation</em>. Differences in salary negotiation between men and women are well-documented, including variations in their propensity and approach <span id="id14">[<a class="reference internal" href="#id2673">23</a>, <a class="reference internal" href="#id2672">24</a>]</span>. Explanations range from differences in risk aversion to perceived chances of success <span id="id15">[<a class="reference internal" href="#id2672">24</a>, <a class="reference internal" href="#id2674">25</a>]</span>. Despite appearing as an individual outcome for female candidates, unsuccessful salary negotiations also reflect an unfair status quo that affects group expectations <span id="id16">[<a class="reference internal" href="#id2672">24</a>]</span>.</p>
</div>
<div class="section" id="technological-blindspots">
<h6>Technological blindspots<a class="headerlink" href="#technological-blindspots" title="Permalink to this headline">¶</a></h6>
<p>Technological blindspots are introduced by biased components unreflectively integrated into larger algorithmic pipelines.</p>
<p><em>Accessibility challenges and ableist norms</em>. Accessibility challenges and ableist norms can deter disabled individuals from applying for jobs and lead to biased evaluations against them <span id="id17">[<a class="reference internal" href="#id2675">26</a>, <a class="reference internal" href="#id2677">27</a>]</span>. Asynchronous video interviews can disadvantage candidates with speech impairments, who may give shorter answers, or those with visual impairments, due to misinterpretation of eye contact by algorithmic recruitment systems. This can result in these candidates being rated less favorably <span id="id18">[<a class="reference internal" href="#id2678">28</a>]</span>.</p>
<p><em>Uneven performance</em>. The uneven performance of language processing and computer vision tools regarding gender, race, and other sensitive attributes has been well-documented <span id="id19">[<a class="reference internal" href="#id2679">29</a>, <a class="reference internal" href="TAILOR.html#id2692">2</a>]</span>. Integrating these off-the-shelf algorithms into hiring processes can lead to worse performance for minority candidates due to biased feature extraction, negatively impacting other algorithms based on these features.</p>
</div>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id20"><dl class="citation">
<dt class="label" id="id2650"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Jilana Jaxon, Ryan F Lei, Reut Shachnai, Eleanor K Chestnut, and Andrei Cimpian. The acquisition of gender stereotypes about intellectual ability: intersections with race. <em>Journal of Social Issues</em>, 75(4):1192–1215, 2019.</p>
</dd>
<dt class="label" id="id2651"><span class="brackets"><a class="fn-backref" href="#id1">2</a></span></dt>
<dd><p>Gordon B Moskowitz, Jeff Stone, and Amanda Childs. Implicit stereotyping and medical decisions: unconscious stereotype activation in practitioners' thoughts about african americans. <em>American J. of Public Health</em>, 102(5):996–1001, 2012.</p>
</dd>
<dt class="label" id="id2652"><span class="brackets"><a class="fn-backref" href="#id2">3</a></span></dt>
<dd><p>Naomi Ellemers. Gender stereotypes. <em>Annual Review of Psychology</em>, 69(1):275–298, 2018.</p>
</dd>
<dt class="label" id="id2653"><span class="brackets"><a class="fn-backref" href="#id3">4</a></span></dt>
<dd><p>Donna Bobbitt-Zeher. Gender discrimination at work: connecting gender stereotypes, institutional policies, and gender composition of workplace. <em>Gender &amp; Society</em>, 25(6):764–786, 2011.</p>
</dd>
<dt class="label" id="id2654"><span class="brackets"><a class="fn-backref" href="#id3">5</a></span></dt>
<dd><p>United Nations Development Programme. Breaking down gender biases: shifting social norms towards gender equality. 2023. URL: <a class="reference external" href="https://hdr.undp.org/system/files/documents/hdp-document/gsni202302pdf.pdf">https://hdr.undp.org/system/files/documents/hdp-document/gsni202302pdf.pdf</a>.</p>
</dd>
<dt class="label" id="id2655"><span class="brackets"><a class="fn-backref" href="#id4">6</a></span></dt>
<dd><p>Alice H Eagly, Christa Nater, David I Miller, Michèle Kaufmann, and Sabine Sczesny. Gender stereotypes have changed: a cross-temporal meta-analysis of US public opinion polls from 1946 to 2018. <em>American Psychologist</em>, 75(3):301–315, 2019.</p>
</dd>
<dt class="label" id="id2656"><span class="brackets"><a class="fn-backref" href="#id4">7</a></span></dt>
<dd><p>Madeline E Heilman. Gender stereotypes and workplace bias. <em>Research in organizational Behavior</em>, 32:113–135, 2012.</p>
</dd>
<dt class="label" id="id2657"><span class="brackets"><a class="fn-backref" href="#id5">8</a></span></dt>
<dd><p>Ketki V. Deshpande, Shimei Pan, and James R. Foulds. Mitigating demographic bias in AI-based resume filtering. In <em>UMAP (Adjunct Publication)</em>, 268–275. ACM, 2020.</p>
</dd>
<dt class="label" id="id2658"><span class="brackets"><a class="fn-backref" href="#id5">9</a></span></dt>
<dd><p>Maria De-Arteaga, Alexey Romanov, Hanna M. Wallach, Jennifer T. Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Cem Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. Bias in bios: A case study of semantic representation bias in a high-stakes setting. In <em>FAT</em>, 120–128. ACM, 2019.</p>
</dd>
<dt class="label" id="id2659"><span class="brackets"><a class="fn-backref" href="#id6">10</a></span></dt>
<dd><p>Joseph Fuller, Manjari Raman, Eva Sage-Gavin, and Kristen Hines. Hidden workers: untapped talent. Technical Report, Harvard Business School, 2021.</p>
</dd>
<dt class="label" id="id2660"><span class="brackets"><a class="fn-backref" href="#id7">11</a></span></dt>
<dd><p>Lotte Bloksgaard. Masculinities, femininities and work–the horizontal gender segregation in the danish labour market. <em>Nordic J. of Working Life Studies</em>, 1(2):5–21, 2011.</p>
</dd>
<dt class="label" id="id2661"><span class="brackets"><a class="fn-backref" href="#id7">12</a></span></dt>
<dd><p>Rebbeca Tesfai and Kevin JA Thomas. Dimensions of inequality: black immigrants’ occupational segregation in the United States. <em>Sociology of Race and Ethnicity</em>, 6(1):1–21, 2020.</p>
</dd>
<dt class="label" id="id2662"><span class="brackets"><a class="fn-backref" href="#id8">13</a></span></dt>
<dd><p>Peter A Riach and Judith Rich. Field experiments of discrimination in the market place. <em>The Economic Journal</em>, 112(483):F480–F518, 2002.</p>
</dd>
<dt class="label" id="id2663"><span class="brackets"><a class="fn-backref" href="#id8">14</a></span></dt>
<dd><p>Judith Rich. <em>What do field experiments of discrimination in markets tell us? A meta analysis of studies conducted since 2000</em>. IZA Discussion paper, 2014.</p>
</dd>
<dt class="label" id="id2664"><span class="brackets"><a class="fn-backref" href="#id9">15</a></span></dt>
<dd><p>David A Cotter, Joan M Hermsen, Seth Ovadia, and Reeve Vanneman. The glass ceiling effect. <em>Social forces</em>, 80(2):655–681, 2001.</p>
</dd>
<dt class="label" id="id2665"><span class="brackets"><a class="fn-backref" href="#id9">16</a></span></dt>
<dd><p><strong>missing publisher in eige2020gender</strong></p>
</dd>
<dt class="label" id="id2666"><span class="brackets"><a class="fn-backref" href="#id10">17</a></span></dt>
<dd><p>Skylar Davidson. Gender inequality: nonbinary transgender people in the workplace. <em>Cogent Social Sciences</em>, 2(1):1236511, 2016.</p>
</dd>
<dt class="label" id="id2667"><span class="brackets"><a class="fn-backref" href="#id10">18</a></span></dt>
<dd><p>Deepak Hegde, Alexander Ljungqvist, and Manav Raj. Race, glass ceilings, and lower pay for equal work. Technical Report 21-09, Swedish House of Finance, 2022.</p>
</dd>
<dt class="label" id="id2668"><span class="brackets"><a class="fn-backref" href="#id11">19</a></span></dt>
<dd><p>Clara Rus, Jeffrey Luppes, Harrie Oosterhuis, and Gido H. Schoenmacker. Closing the gender wage gap: adversarial fairness in job recommendation. In <em>HR&#64;RecSys</em>, volume 3218 of CEUR Workshop Proceedings. CEUR-WS.org, 2022.</p>
</dd>
<dt class="label" id="id2669"><span class="brackets"><a class="fn-backref" href="#id12">20</a></span></dt>
<dd><p>Arthur G Bedeian, Gerald R Ferris, and K Michele Kacmar. Age, tenure, and job satisfaction: a tale of two perspectives. <em>J. of Vocational Behavior</em>, 40(1):33–48, 1992.</p>
</dd>
<dt class="label" id="id2670"><span class="brackets">21</span><span class="fn-backref">(<a href="#id12">1</a>,<a href="#id13">2</a>)</span></dt>
<dd><p>Michael A Shields and Stephen Wheatley Price. Racial harassment, job satisfaction and intentions to quit: Evidence from the british nursing profession. <em>Economica</em>, 69(274):295–326, 2002.</p>
</dd>
<dt class="label" id="id2671"><span class="brackets"><a class="fn-backref" href="#id13">22</a></span></dt>
<dd><p>Kimberly T Schneider, Suzanne Swan, and Louise F Fitzgerald. Job-related and psychological effects of sexual harassment in the workplace: Empirical evidence from two organizations. <em>J. of Applied Psychology</em>, 82(3):401, 1997.</p>
</dd>
<dt class="label" id="id2673"><span class="brackets"><a class="fn-backref" href="#id14">23</a></span></dt>
<dd><p>Andreas Leibbrandt and John A List. Do women avoid salary negotiations? Evidence from a large-scale natural field experiment. <em>Management Science</em>, 61(9):2016–2024, 2015.</p>
</dd>
<dt class="label" id="id2672"><span class="brackets">24</span><span class="fn-backref">(<a href="#id14">1</a>,<a href="#id15">2</a>,<a href="#id16">3</a>)</span></dt>
<dd><p>Kelsey Gray, Angela Neville, Amy H Kaji, Mary Wolfe, Kristine Calhoun, Farin Amersi, Timothy Donahue, Tracy Arnell, Benjamin Jarman, Kenji Inaba, and others. Career goals, salary expectations, and salary negotiation among male and female general surgery residents. <em>JAMA Surgery</em>, 154(11):1023–1029, 2019.</p>
</dd>
<dt class="label" id="id2674"><span class="brackets"><a class="fn-backref" href="#id15">25</a></span></dt>
<dd><p>Iñigo Hernandez-Arenaz and Nagore Iriberri. A review of gender differences in negotiation. <em>Oxford Research Encyclopedia of Economics and Finance</em>, 2019.</p>
</dd>
<dt class="label" id="id2675"><span class="brackets"><a class="fn-backref" href="#id17">26</a></span></dt>
<dd><p>Frederike Scholz. Taken for granted: ableist norms embedded in the design of online recruitment practices. <em>The Palgrave Handbook of Disability at Work</em>, pages 451–469, 2020.</p>
</dd>
<dt class="label" id="id2677"><span class="brackets"><a class="fn-backref" href="#id17">27</a></span></dt>
<dd><p>Nicholas Tilmes. Disability, fairness, and algorithmic bias in AI recruitment. <em>Ethics Inf. Technol.</em>, 24(2):21, 2022.</p>
</dd>
<dt class="label" id="id2678"><span class="brackets"><a class="fn-backref" href="#id18">28</a></span></dt>
<dd><p><strong>missing institution in orcaa2020description</strong></p>
</dd>
<dt class="label" id="id2679"><span class="brackets"><a class="fn-backref" href="#id19">29</a></span></dt>
<dd><p>Su Lin Blodgett, Lisa Green, and Brendan T. O'Connor. Demographic dialectal variation in social media: A case study of african-american english. In <em>EMNLP</em>, 1119–1130. The Association for Computational Linguistics, 2016.</p>
</dd>
<dt class="label" id="id2698"><span class="brackets"><a class="fn-backref" href="#id19">30</a></span></dt>
<dd><p>Joy Buolamwini and Timnit Gebru. Gender shades: intersectional accuracy disparities in commercial gender classification. In <em>FAT</em>, volume 81 of Proceedings of Machine Learning Research, 77–91. PMLR, 2018.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from~\cite[Section 3]{fabris2024fairness} by Alessandro Fabris}\[1ex]</p>
</div></blockquote>
</div>
</div>
<span id="document-Diversity_Non-Discrimination_and_Fairness/bias_lmm"></span><div class="tex2jax_ignore mathjax_ignore section" id="bias-and-fairness-in-llms">
<h4>Bias and Fairness in LLMs<a class="headerlink" href="#bias-and-fairness-in-llms" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p>Within the Natural Language Processing (NLP) field, bias manifests in several forms, possibly leading to harms and unfairness. We review here intrinsic, lantent bias; extrinsic harms, and data selection bias in Large Language Models (LLMs).</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>Large Language Models (LLMs) are ubiquitous in NLP and are often used as a base step for fine-tuning models on downstream tasks.
The unparalleled ability of LLMs to generalize from vast corpora is affected by an inherent reinforcement of biases (see also the entry <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/bias"><span class="doc">Bias</span></a>). Since biases are communicated and embedded in language, they manifest in texts, both learnt and produced by LLMs.
As foundation models, they are often employed in human-centric scenarios where their outputs may have undesired effects on historically marginalized groups of people, including discriminatory consequences.</p>
<p>In <span id="id1">[<a class="reference internal" href="#id2429">1</a>]</span>, authors introduce the concepts of <strong>intrinsic, latent biases</strong>, i.e., “properties of the foundation model that can lead to harm in downstream systems”, and <strong>extrinsic harms</strong>, i.e., “specific harms from the downstream applications that are created by adapting a foundation model”. Representational bias pertains to the intrinsic harms and manifests itself as misrepresentation, under-representation, and over-representation. From the extrinsic level, harms manifest as the generation of abusive content, and marked performance disparities among different demographic groups.</p>
<p>These issues warn that LLMs concretely impact society, posing a severe risk and limitation to the well-being of under-represented minorities, ultimately amplifying pre-existing social stereotypes, possible marginalization, and explicit harms <span id="id2">[<a class="reference internal" href="#id2678">2</a>, <a class="reference internal" href="#id2679">3</a>]</span>.
Current research has highlighted cases emblematic of harms arising from LLMs.
For instance, studies have shown that word embeddings can encode and perpetuate gender bias by echoing and strengthening societal stereotypes <span id="id3">[<a class="reference internal" href="TAILOR.html#id88">10</a>, <a class="reference internal" href="#id2681">5</a>]</span>.
These biases are not merely encoded within language models’ representations but are also perpetuated to downstream tasks <span id="id4">[<a class="reference internal" href="#id2427">6</a>, <a class="reference internal" href="#id2428">7</a>]</span>, where they can manifest in an uneven treatment of different demographic groups.
For example, automatic translation systems have been found to reproduce damaging gender and racial biases, especially towards gendered pronoun languages <span id="id5">[<a class="reference internal" href="#id2683">8</a>]</span>.
Similarly, gender bias can be propagated in coreference resolution if models are trained on biased text <span id="id6">[<a class="reference internal" href="#id2682">9</a>]</span>.
Moreover, it was found that human annotators have a tendency to label social media posts written in Afro-American English as hateful more often than other messages. This harmful pattern could potentially result in the development of a biased system that reproduces and amplifies these same discriminatory attitudes <span id="id7">[<a class="reference internal" href="#id2684">10</a>]</span>.
Recent studies have also documented the anti-Muslim sentiment exhibited by GPT-3, which generated toxic and abusive text when interrogated with prompts containing references to Islam and Muslims <span id="id8">[<a class="reference internal" href="#id2685">11</a>]</span>.
Summarizing, sensitive axes targeted by biases in LLMs are gender, age, sexual orientation, physical appearance, disability, nationality, ethnicity and race, socioeconomic status, religion, culture and intersectional identities. It is crucial to acknowledge how biases directed towards certain groups are yet underexplored and unaddressed by the research community <span id="id9">[<a class="reference internal" href="#id2686">12</a>]</span>.</p>
<p>Although biases manifest differently in the various NLP tasks, they can be assessed through formal fairness notions measuring through a score the LLM output w.r.t. a social group. Fairness is evaluated using a range of metrics <span id="id10">[<a class="reference internal" href="#id2687">13</a>, <a class="reference internal" href="#id2688">14</a>]</span>, which often present conflicting perspectives <span id="id11">[<a class="reference internal" href="#id2689">15</a>]</span> (see also the entry <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/fairness"><span class="doc">Fairness notions and metrics</span></a>). Moreover, defining fairness in the NLP context is challenging, and existing works are often inaccurate, inconsistent, and contradictory in formalizing bias <span id="id12">[<a class="reference internal" href="#id2690">16</a>]</span>.
Nonetheless, starting from carefully auditing models’ output is mandatory to mitigate and avoid stigmatization and discrimination, given the sensitive contexts in which LLMs are deployed <span id="id13">[<a class="reference internal" href="#id2691">17</a>]</span>.</p>
<p><em>Data selection bias</em> is defined as mistakes originating from texts sampled, selected and preprocessed to train LLMs, or introduced within the subsequent fine-tuning stage. These systematic errors arise from (i) skewed composition and distribution of knowledge domain and textual genre, (ii) range of time of the data gathered, (iii) demographic groups that create (and therefore represent) the data vs the affected application stakeholders, (iv) focusing mainly on high-resources languages (cultures) amplifying the gap w.r.t. low-resource languages (cultures), feeding a vicious feedback loop <span id="id14">[<a class="reference internal" href="#id2686">12</a>]</span>.
To address this issue, several strategies are suggested, e.g.,~starting from a sound conceptualization step, that should guide the choice of the most appropriate assessment measures and mitigation strategies that, among others, could consist of leveraging smaller, more curated datasets, together with enhancing the diversity of the cultures and languages to which the model has been exposed <span id="id15">[<a class="reference internal" href="#id2686">12</a>]</span>.</p>
<p>Investigating the other steps of the LLM life-cycle, potential entry points for bias injection are the model itself (e.g., equal treatment of different social groups when computing the loss), the evaluation step (e.g., leveraging on aggregate measures) and the deployment phase (e.g., unintended uses of the model) <span id="id16">[<a class="reference internal" href="#id2692">18</a>]</span>.
Tracing back the source of these biases is extremely challenging, and attempts to mitigate them are often not successful nor effective. Indeed, in-depth data training investigation and counteracting bias by design are not doable in practice due to the huge data quantities, unfeasible to fully verify and assess, and the resources needed to develop LLMs from scratch. A recommended direction consists of collectively designing new, more accurate, holistic evaluation benchmarks, encompassing and testing different ethical desiderata <span id="id17">[<a class="reference internal" href="#id2686">12</a>]</span>.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id18"><dl class="citation">
<dt class="label" id="id2429"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, and et al. On the opportunities and risks of foundation models. <em>CoRR</em>, 2021.</p>
</dd>
<dt class="label" id="id2678"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Harini Suresh and John V. Guttag. A framework for understanding unintended consequences of machine learning. <em>CoRR</em>, 2019.</p>
</dd>
<dt class="label" id="id2679"><span class="brackets"><a class="fn-backref" href="#id2">3</a></span></dt>
<dd><p>Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and mitigating unintended bias in text classification. In <em>AIES</em>, 67–73. ACM, 2018.</p>
</dd>
<dt class="label" id="id2680"><span class="brackets"><a class="fn-backref" href="#id3">4</a></span></dt>
<dd><p>Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In <em>Advances in neural information processing systems</em>, 4349–4357. 2016.</p>
</dd>
<dt class="label" id="id2681"><span class="brackets"><a class="fn-backref" href="#id3">5</a></span></dt>
<dd><p>Malvina Nissim, Rik van Noord, and Rob van der Goot. Fair is better than sensational: man is to doctor as woman is to doctor. <em>Comput. Linguistics</em>, 46(2):487–497, 2020.</p>
</dd>
<dt class="label" id="id2427"><span class="brackets"><a class="fn-backref" href="#id4">6</a></span></dt>
<dd><p>Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna M. Wallach. Stereotyping Norwegian salmon: an inventory of pitfalls in fairness benchmark datasets. In <em>ACL/IJCNLP (1)</em>, 1004–1015. Association for Computational Linguistics, 2021.</p>
</dd>
<dt class="label" id="id2428"><span class="brackets"><a class="fn-backref" href="#id4">7</a></span></dt>
<dd><p>Karolina Stanczak and Isabelle Augenstein. A survey on gender bias in natural language processing. <em>CoRR</em>, 2021. URL: <a class="reference external" href="https://arxiv.org/abs/2112.14168">https://arxiv.org/abs/2112.14168</a>, <a class="reference external" href="https://arxiv.org/abs/2112.14168">arXiv:2112.14168</a>.</p>
</dd>
<dt class="label" id="id2683"><span class="brackets"><a class="fn-backref" href="#id5">8</a></span></dt>
<dd><p>Beatrice Savoldi, Marco Gaido, Luisa Bentivogli, Matteo Negri, and Marco Turchi. Gender bias in machine translation. <em>Trans. Assoc. Comput. Linguistics</em>, 9:845–874, 2021.</p>
</dd>
<dt class="label" id="id2682"><span class="brackets"><a class="fn-backref" href="#id6">9</a></span></dt>
<dd><p>Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in coreference resolution: evaluation and debiasing methods. In <em>NAACL-HLT (2)</em>, 15–20. Association for Computational Linguistics, 2018.</p>
</dd>
<dt class="label" id="id2684"><span class="brackets"><a class="fn-backref" href="#id7">10</a></span></dt>
<dd><p>Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A. Smith. The risk of racial bias in hate speech detection. In <em>ACL (1)</em>, 1668–1678. Association for Computational Linguistics, 2019.</p>
</dd>
<dt class="label" id="id2685"><span class="brackets"><a class="fn-backref" href="#id8">11</a></span></dt>
<dd><p>Abubakar Abid, Maheen Farooqi, and James Zou. Persistent anti-muslim bias in large language models. In <em>AIES</em>, 298–306. ACM, 2021.</p>
</dd>
<dt class="label" id="id2686"><span class="brackets">12</span><span class="fn-backref">(<a href="#id9">1</a>,<a href="#id14">2</a>,<a href="#id15">3</a>,<a href="#id17">4</a>)</span></dt>
<dd><p>Roberto Navigli, Simone Conia, and Björn Ross. Biases in large language models: origins, inventory, and discussion. <em>ACM J. Data Inf. Qual.</em>, 15(2):10:1–10:21, 2023. URL: <a class="reference external" href="https://doi.org/10.1145/3597307">https://doi.org/10.1145/3597307</a>, <a class="reference external" href="https://doi.org/10.1145/3597307">doi:10.1145/3597307</a>.</p>
</dd>
<dt class="label" id="id2687"><span class="brackets"><a class="fn-backref" href="#id10">13</a></span></dt>
<dd><p>Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In <em>NIPS</em>, 3315–3323. 2016.</p>
</dd>
<dt class="label" id="id2688"><span class="brackets"><a class="fn-backref" href="#id10">14</a></span></dt>
<dd><p>Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard S. Zemel. Fairness through awareness. In <em>ITCS</em>, 214–226. ACM, 2012.</p>
</dd>
<dt class="label" id="id2689"><span class="brackets"><a class="fn-backref" href="#id11">15</a></span></dt>
<dd><p>Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In <em>ITCS</em>, volume 67 of LIPIcs, 43:1–43:23. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2017.</p>
</dd>
<dt class="label" id="id2690"><span class="brackets"><a class="fn-backref" href="#id12">16</a></span></dt>
<dd><p>Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna M. Wallach. Language (technology) is power: A critical survey of “bias&quot; in NLP. In <em>ACL</em>, 5454–5476. Association for Computational Linguistics, 2020.</p>
</dd>
<dt class="label" id="id2691"><span class="brackets"><a class="fn-backref" href="#id13">17</a></span></dt>
<dd><p>Debora Nozza, Federico Bianchi, and Dirk Hovy. Pipelines for social bias testing of large language models. In <em>Workshop on Challenges &amp; Perspectives in Creating Large Language Models</em>. virtual+Dublin, 2022. ACL.</p>
</dd>
<dt class="label" id="id2692"><span class="brackets"><a class="fn-backref" href="#id16">18</a></span></dt>
<dd><p>Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md. Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed. Bias and fairness in large language models: A survey. <em>CoRR</em>, 2023. URL: <a class="reference external" href="https://doi.org/10.48550/arXiv.2309.00770">https://doi.org/10.48550/arXiv.2309.00770</a>, <a class="reference external" href="https://arxiv.org/abs/2309.00770">arXiv:2309.00770</a>, <a class="reference external" href="https://doi.org/10.48550/ARXIV.2309.00770">doi:10.48550/ARXIV.2309.00770</a>.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was partially readapted from~\cite{DBLP:journals/corr/Marchiori23, DBLP:journals/corr/Setzu24} by Marta Marchiori Manerba}</p>
</div></blockquote>
</div>
</div>
<span id="document-Diversity_Non-Discrimination_and_Fairness/equity"></span><div class="tex2jax_ignore mathjax_ignore section" id="discrimination-equity">
<h4>Discrimination &amp; Equity<a class="headerlink" href="#discrimination-equity" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p>Forms of bias that count as discrimination against social groups or individuals should be avoided, both from legal and ethical perspectives. Discrimination can be direct or indirect, intentional or unintentional.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>Not all forms of <em>bias</em> (also known as <em>statistical discrimination</em>) are problematic. Here we use the normative sense of <em>discrimination</em>, where all forms of bias that count as discrimination are considered problematic and should be avoided. This discrimination can be direct (where a protected feature is intentionally used in the decision making procedure).
In such a case explainability tools such as feature importance methods can help to detect whether a model’s decisions are based on the feature, in addition to the other <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/fairness"><span class="doc std std-doc">fairness metrics</span></a>. Often, however, protected features are purposely not included among the input variables, and so no direct discrimination will take place.
Instead, indirect discrimination (where there is direct discrimination on features that strongly correlate with a protected feature, in such a
way that users with a socially salient value of the feature – e.g. women – are worse off <span id="id1">[<a class="reference internal" href="#id2525">1</a>]</span>) is the most common type of discrimination in machine learning systems. For ways to detect these cases of indirect discrimination, see the <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/fairness"><span class="doc std std-doc">fairness metrics</span></a> entry.</p>
<p>This type of indirect discrimination is often unintentional. Philosophical accounts thus disagree about the degree of intentionality that is required for bias to count as discrimination: mental state accounts <span id="id2">[<a class="reference internal" href="#id2526">2</a>]</span> require systematic animosity or preferences towards a certain group. Such animosity need not be present among the designers of the system, though it may be part of the reason for the societal biases that filter through into the data. Other accounts <span id="id3">[<a class="reference internal" href="#id2525">1</a>, <a class="reference internal" href="TAILOR.html#id2548">3</a>]</span> opt for weaker notions of intentionality, where it is sufficient to enable the feature/group membership to play a role in the decision making procedure. This clearly allows for the (frequent) scenario where an algorithm has disparate impacts on groups even when this was not the result of preferences/animosities of the developers. Yet even then not all types of bias are considered normatively problematic: a statistical bias that negatively impacts smokers is not clearly a case of discrimination. So when is a bias a case of discrimination?</p>
<p>An influential point of departure is the idea that biases should not be on features outside of people’s control <span id="id4">[<a class="reference internal" href="#id2528">4</a>, <a class="reference internal" href="#id2527">5</a>]</span>. This might explain why the paradigmatic cases of discrimination is when there is disparate treatment based on gender, race, or ethnicity (see the entry <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/discrimination"><span class="doc">Grounds of Discrimination</span></a>), as we cannot choose these. However, there are more features beyond our control, as illustrated by the “other people’s choice principle” <span id="id5">[<a class="reference internal" href="#id2528">4</a>]</span>: statistical patterns resulting from other people’s choices are not in our control either, and thus may lead to discrimination. Consequently, charging higher premiums to a buyer of a red car because on average drivers of red cars cause more accidents may be seen as problematic. It violates the other people’s choice principle, as a buyer has no control over the driving of other car owners. On the other hand, charging higher premiums to smokers does not violate this principle, since smoking is a direct cause of higher health risks. In practice, however, it is difficult to draw a clear border, as e.g. socio-economic status impacts people’s choices. Instead, some authors <span id="id6">[<a class="reference internal" href="TAILOR.html#id2492">2</a>]</span> suggest to consider notions of <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/justice"><span class="doc">Justice</span></a> as guiding the distribution of benefits and burdens resulting from the use of AI. For example, luck egalitarianism would consider it discriminatory to uphold biases which reflect factors of luck. Still, while the exact confines of normatively problematic discrimination are difficult to define, it is clear that gender, race, etc. are protected features and that such discrimination needs to be detected and tackled.</p>
<p>Following the egalitarian principles of <span id="id7">[<a class="reference internal" href="#id2446">7</a>, <a class="reference internal" href="#id2445">8</a>]</span>, some authors address fairness from a multi-agent perspective <span id="id8">[<a class="reference internal" href="#id2444">9</a>, <a class="reference internal" href="#id2443">10</a>]</span> in automated decision making. Taking a welfare perspective <span id="id9">[<a class="reference internal" href="#id2424">11</a>]</span> propose a family of welfare based measures that can be integrated together with other fairness and performance constraints. Following the same tracks,
<span id="id10">[<a class="reference internal" href="#id2428">12</a>]</span> considered the temporal/sequential dimension and addressed fairness in the context of Markov decision processes and reinforcement learning. Such multi-objective and welfare approaches not only enforce human intervention and social criteria  to prevent unfair outcomes for some users or stakeholders, but could be adapted to ensure equity and fair trade-off between privilege and unprivileged groups.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id11"><dl class="citation">
<dt class="label" id="id2525"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id3">2</a>)</span></dt>
<dd><p>Kasper Lippert-Rasmussen. <em>Born free and equal?: A philosophical inquiry into the nature of discrimination</em>. Oxford University Press, 2013.</p>
</dd>
<dt class="label" id="id2526"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Thomas M Scanlon. Moral dimensions. In <em>Moral Dimensions</em>. Harvard University Press, 2009.</p>
</dd>
<dt class="label" id="id2518"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Michele Loi and Markus Christen. Choosing how to discriminate: navigating ethical trade-offs in fair algorithmic design for the insurance sector. <em>Philosophy &amp; Technology</em>, 34(4):967–992, 2021.</p>
</dd>
<dt class="label" id="id2528"><span class="brackets">4</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id5">2</a>)</span></dt>
<dd><p>Kasper Lippert-Rasmussen. Nothing personal: on statistical discrimination. <em>Journal of Political Philosophy</em>, 15(4):385–403, 2007.</p>
</dd>
<dt class="label" id="id2527"><span class="brackets"><a class="fn-backref" href="#id4">5</a></span></dt>
<dd><p>Daniel E Palmer. Insurance, risk assessment and fairness: an ethical analysis. In <em>Insurance ethics for a more ethical world</em>. Emerald Group Publishing Limited, 2007.</p>
</dd>
<dt class="label" id="id2463"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p>Reuben Binns, Max Van Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, and Nigel Shadbolt. 'it's reducing a human being to a percentage': perceptions of justice in algorithmic decisions. In <em>CHI</em>, 377. ACM, 2018.</p>
</dd>
<dt class="label" id="id2446"><span class="brackets"><a class="fn-backref" href="#id7">7</a></span></dt>
<dd><p>John Rawls. <em>A theory of justice</em>. Harvard university press, 2009.</p>
</dd>
<dt class="label" id="id2445"><span class="brackets"><a class="fn-backref" href="#id7">8</a></span></dt>
<dd><p>Hervé Moulin. <em>Fair division and collective welfare</em>. MIT press, 2004.</p>
</dd>
<dt class="label" id="id2444"><span class="brackets"><a class="fn-backref" href="#id8">9</a></span></dt>
<dd><p>Steven de Jong, Karl Tuyls, and Katja Verbeeck. Fairness in multi-agent systems. <em>Knowl. Eng. Rev.</em>, 23(2):153–180, 2008.</p>
</dd>
<dt class="label" id="id2443"><span class="brackets"><a class="fn-backref" href="#id8">10</a></span></dt>
<dd><p>Jianye Hao and Ho-fung Leung. <em>Fairness in Cooperative Multiagent Systems</em>, pages 27–70. Springer Berlin Heidelberg, Berlin, Heidelberg, 2016.</p>
</dd>
<dt class="label" id="id2424"><span class="brackets"><a class="fn-backref" href="#id9">11</a></span></dt>
<dd><p>Hoda Heidari, Claudio Ferrari, Krishna P. Gummadi, and Andreas Krause. Fairness behind a veil of ignorance: A welfare analysis for automated decision making. In <em>NeurIPS</em>, 1273–1283. 2018.</p>
</dd>
<dt class="label" id="id2428"><span class="brackets"><a class="fn-backref" href="#id10">12</a></span></dt>
<dd><p>Shahin Jabbari, Matthew Joseph, Michael J. Kearns, Jamie Morgenstern, and Aaron Roth. Fairness in reinforcement learning. In <em>ICML</em>, volume 70 of Proceedings of Machine Learning Research, 1617–1626. PMLR, 2017.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Stefan Buijsman and Miguel Couceiro.</p>
</div></blockquote>
</div>
</div>
<span id="document-Diversity_Non-Discrimination_and_Fairness/fairness"></span><div class="tex2jax_ignore mathjax_ignore section" id="fairness-notions-and-metrics">
<h4>Fairness notions and metrics<a class="headerlink" href="#fairness-notions-and-metrics" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p>The term <strong>fairness</strong> is defined as the quality or state of being fair; or a lack of favoritism towards one side. The notions of fairness, and quantitative measures of
them (fairness metrics), can be distinguished based on the focus on individuals, groups and sub-groups.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>The term fairness is defined as the quality or state of being fair; or a lack of favoritism towards one side. However, like <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/bias"><span class="doc">Bias</span></a>, fairness can mean different concepts to different peoples, different contexts, and different disciplines. The definition of fairness in various disciplines is detailed in <span id="id1">[<a class="reference internal" href="#id2564">1</a>]</span>. An unfair model produces results that are skewed towards particular individuals or groups. The primary sources of this unfairness are the presence of biases. There are two important categories of biases which play crucial role in fairness; (i) technical bias and (ii) social bias. Technical biases can be traced back to the sources, but social biases are very difficult to fix as these are a matter of politics, perspectives, and shifts in prejudices and preconceptions that can take years to change <span id="id2">[<a class="reference internal" href="#id2628">4</a>]</span>. Most of the state-of-the-art techniques tackle technical errors, but it cannot resolve the root causes of bias <span id="id3">[<a class="reference internal" href="#id2697">2</a>]</span>. Based on this observation, Sandra et al. <span id="id4">[<a class="reference internal" href="#id2628">4</a>]</span> proposed three responses concerning algorithmic bias and resulting social inequality. The first is not an active choice as it allows the system to get worse and do nothing to fix biases. Second, incorporate techniques to fix technical errors and maintain a status quo to ensure that the system do not make it worse. Much works in fairness focused on this option, called “bias preserving fairness”, maintains a status quo as a baseline, aligns with the formal equality of EU non-discrimination law. Finally, “bias transforming fairness”, the third response focuses on the substantive equality of EU non-discrimination which can only be achieved by accounting for historical (social) inequalities. As argued in <span id="id5">[<a class="reference internal" href="#id2628">4</a>]</span>, users (developers,deployers etc.) should give preference to “bias transforming” fairness metrics, when a fairness metric is used to make substantive decisions about people in contexts where significant disparity has been previously observed.</p>
<p>The notions of fairness fall under individuals, groups and sub-groups. Individual fairness ensures that similar individuals should be treated similarly. It accounts for the distance measures to evaluate the similarity of individuals <span id="id6">[<a class="reference internal" href="#id2565">5</a>, <a class="reference internal" href="#id2566">6</a>]</span>. On the other hand,
group fairness compares quantities at the group level primarily identified by protective features such as gender, ethnicity, etc. <span id="id7">[<a class="reference internal" href="#id2570">7</a>, <a class="reference internal" href="#id2571">8</a>]</span>. Sub-group fairness is more rigid than group fairness as this ensures fairness concerning one or more structured sub-groups defined by sensitive features, interpolates between individual and group fairness notions <span id="id8">[<a class="reference internal" href="#id2727">9</a>]</span>.
According to <span id="id9">[<a class="reference internal" href="#id2631">10</a>]</span>, it is impossible to satisfy all of the above notions, leading to conflicts between fairness definitions. Therefore, one suggestion could be to select appropriate fairness criteria and use those based on the application and deployment. Another concern has risen in <span id="id10">[<a class="reference internal" href="#id2572">11</a>]</span>, temporal aspects of fairness notions may harm the sensitive groups over time if not updated.</p>
<p><strong>Some widely used fairness metrics:</strong> In order to recall some widely used fairness metrics we need to introduce some notation.
Let <span class="math notranslate nohighlight">\(V\)</span>, <span class="math notranslate nohighlight">\(A\)</span>, and <span class="math notranslate nohighlight">\(X\)</span> be three random variables representing, respectively, the total set of features, the sensitive features, and the remaining features describing an individual such that <span class="math notranslate nohighlight">\(V=(X,A)\)</span> and <span class="math notranslate nohighlight">\(P(V=v_i)\)</span> represents the probability of drawing an individual with a vector of values <span class="math notranslate nohighlight">\(v_i\)</span> from the population.
For simplicity, we focus on the case where <span class="math notranslate nohighlight">\(A\)</span> is a binary random variable where <span class="math notranslate nohighlight">\(A=0\)</span> designates the protected group, while <span class="math notranslate nohighlight">\(A=1\)</span> designates the non-protected group. Let <span class="math notranslate nohighlight">\(Y\)</span> represent the actual outcome and <span class="math notranslate nohighlight">\(\hat{Y}\)</span> represent the outcome returned by the prediction algorithm. Without loss of generality, assume that <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\hat{Y}\)</span> are binary random variables where <span class="math notranslate nohighlight">\(Y=1\)</span> designates a positive instance, while <span class="math notranslate nohighlight">\(Y=0\)</span> a negative one. Typically, the predicted outcome <span class="math notranslate nohighlight">\(\hat{Y}\)</span> is derived from a score represented by a random variable <span class="math notranslate nohighlight">\(S\)</span> where <span class="math notranslate nohighlight">\(P[S = s]\)</span> is the probability that the score value is equal to <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p><strong>Statistical parity</strong> <span id="id11">[<a class="reference internal" href="#id2624">12</a>]</span> is one of the most commonly accepted notions of fairness. It requires the prediction to be statistically independent of the sensitive feature <span class="math notranslate nohighlight">\((\hat{Y}  \perp A)\)</span>.
In other words, the predicted acceptance rates for both protected and unprotected groups should be equal. Statistical parity implies that<br> <span class="math notranslate nohighlight">\(\displaystyle \frac{TP+FP}{TP+FP+FN+TN}\)</span> <a class="footnote-reference brackets" href="#statistical" id="id12">1</a> <br> is equal for both groups. A classifier Ŷ satisfies statistical parity if:<br>  <span class="math notranslate nohighlight">\(\label{eq:sp} P[\hat{Y} \mid A = 0] = P[\hat{Y} \mid A = 1].\)</span> <br>
<br></p>
<p><strong>Conditional statistical parity</strong> <span id="id13">[<a class="reference internal" href="#id2626">13</a>]</span> is a
variant of statistical parity obtained by controlling on a set of
resolving features<a class="footnote-reference brackets" href="#parity" id="id14">2</a>. The resolving features (we refer to them as <span class="math notranslate nohighlight">\(R\)</span>)
among <span class="math notranslate nohighlight">\(X\)</span> are correlated with the sensitive feature <span class="math notranslate nohighlight">\(A\)</span> and give some
factual information about the label at the same time leading to a
<em>legitimate</em> discrimination. Conditional statistical parity holds if: <br>
<span class="math notranslate nohighlight">\(\label{eq:csp}
P[\hat{Y}=1 \mid R=r,A = 0] = P[\hat{Y}=1 \mid R=r,A = 1] \quad \forall r \in range(R).\)</span>
<br></p>
<p><strong>Equalized odds</strong> <span id="id15">[<a class="reference internal" href="#id2627">14</a>]</span> considers both the predicted and
the actual outcomes. The prediction is conditionally independent from
the protected feature, given the actual outcome
<span class="math notranslate nohighlight">\((\hat{Y} \perp A \mid Y)\)</span>. In other words, equalized odds requires that
both sub-populations to have the same true positive rate
<span class="math notranslate nohighlight">\(TPR = \frac{TP}{TP+FN}\)</span> and false positive rate
<span class="math notranslate nohighlight">\(FPR = \frac{FP}{FP+TN}\)</span>: <br>
<span class="math notranslate nohighlight">\(\label{eq:eqOdds}
P[\hat{Y} = 1 \mid Y=y,\; A=0] = P[\hat{Y}=1 \mid Y= y,\; A=1]  \quad \forall{ y \in \{0,1\}}.\)</span></p>
<p>Because equalized odds requirement is rarely satisfied in practice, two
variants can be obtained by relaxing its equation. The first one is
called <strong>equal opportunity</strong> <span id="id16">[<a class="reference internal" href="#id2627">14</a>]</span> and is obtained by
requiring only TPR equality among groups: <br>
<span class="math notranslate nohighlight">\(\label{eq:eqOpp}
P[\hat{Y}=1 \mid Y=1,A = 0] = P[\hat{Y}=1\mid Y=1,A = 1].\)</span> <br>
As <span class="math notranslate nohighlight">\(TPR\)</span>
does not take into consideration <span class="math notranslate nohighlight">\(FP\)</span>, equal opportunity is completely
insensitive to the number of false positives.</p>
<p>The second relaxed variant of equalized odds is called <strong>predictive
equality</strong> <span id="id17">[<a class="reference internal" href="#id2626">13</a>]</span> which requires only the FPR to be
equal in both groups: <br>
<span class="math notranslate nohighlight">\(\label{eq:predEq}
P[\hat{Y}=1 \mid Y=0,A = 0] = P[\hat{Y}=1\mid Y=0,A = 1].\)</span> <br>
Since <span class="math notranslate nohighlight">\(FPR\)</span> is independent from <span class="math notranslate nohighlight">\(FN\)</span>, predictive equality is completely
insensitive to false negatives.
<br></p>
<p><strong>Conditional use accuracy equality</strong> <span id="id18">[<a class="reference internal" href="#id2629">15</a>]</span> is achieved
when all population groups have equal positive predictive value
<span class="math notranslate nohighlight">\(PPV=\frac{TP}{TP+FP}\)</span> and negative predictive value
<span class="math notranslate nohighlight">\(NPV=\frac{TN}{FN+TN}\)</span>. In other words, the probability of subjects with
positive predictive value to truly belong to the positive class and the
probability of subjects with negative predictive value to truly belong
to the negative class should be the same. By contrast to equalized odds,
one is conditioning on the algorithm’s predicted outcome not the actual
outcome. In other words, the emphasis is on the precision of prediction
rather than its recall: <br>
<span class="math notranslate nohighlight">\(\label{eq:condUseAcc}
P[Y=y\mid \hat{Y}=y ,A = 0] = P[Y=y\mid \hat{Y}=y,A = 1] \quad \forall{ y \in \{0,1\}}.\)</span>
<br></p>
<p><strong>Predictive parity</strong> <span id="id19">[<a class="reference internal" href="#id2630">16</a>]</span> is a relaxation of
conditional use accuracy equality requiring only equal <span class="math notranslate nohighlight">\(PPV\)</span> among
groups: $<span class="math notranslate nohighlight">\(\label{eq:predPar}
P[Y=1 \mid \hat{Y} =1,A = 0] = P[Y=1\mid \hat{Y} =1,A = 1]\)</span>$ Like
predictive equality, predictive parity is insensitive to false
negatives.
<br></p>
<p><strong>Overall accuracy equality</strong> <span id="id20">[<a class="reference internal" href="#id2629">15</a>]</span> is achieved when
overall accuracy for both groups is the same. This implies that</p>
<div class="math notranslate nohighlight">
\[\label{eq:accuracy}
\frac{TP+TN}{TP+FN+FP+TN}\]</div>
<p>is equal for both groups:</p>
<div class="math notranslate nohighlight">
\[\label{eq:ovAcc}
P[\hat{Y} = Y | A = 0] = P[\hat{Y} = Y | A = 1]\]</div>
<br>
<p><strong>Treatment equality</strong> <span id="id21">[<a class="reference internal" href="#id2629">15</a>]</span> is achieved when the ratio of
FPs and FNs is the same for both protected and unprotected groups: <br>
<span class="math notranslate nohighlight">\(\label{eq:treatEq}
\frac{FN}{FP}\)</span><sub>A=0</sub> <span class="math notranslate nohighlight">\(= \frac {FN}{FP}\)</span><sub>A=1</sub>
<br></p>
<p><strong>Total fairness</strong> <span id="id22">[<a class="reference internal" href="#id2629">15</a>]</span> holds when all aforementioned
fairness notions are satisfied simultaneously, that is, statistical
parity, equalized odds, conditional use accuracy equality (hence,
overall accuracy equality), and treatment equality. Total fairness is a
very strong notion which is very difficult to hold in practice.
<br></p>
<p><strong>Balance</strong> <span id="id23">[<a class="reference internal" href="#id2631">10</a>]</span> uses the score (<span class="math notranslate nohighlight">\(S\)</span>) from which the outcome
<span class="math notranslate nohighlight">\(Y\)</span> is typically derived through thresholding. <br>
<em>Balance for positive
class</em> focuses on the applicants who constitute positive instances and
is satisfied if the average score <span class="math notranslate nohighlight">\(S\)</span> received by those applicants is
the same for both groups: <br>
<span class="math notranslate nohighlight">\(\label{eq:balPosclass}
E[S \mid Y =1,A = 0)] = E[S \mid Y =1,A = 1].\)</span> <br>
<em>Balance of negative
class</em> focuses instead on the negative class: <br>
<span class="math notranslate nohighlight">\(\label{eq:balNegclass}
E[S \mid Y =0,A = 0] = E[S \mid Y =0,A = 1].\)</span>
<br></p>
<p><strong>Calibration</strong> <span id="id24">[<a class="reference internal" href="#id2630">16</a>]</span> holds if, for each predicted
probability score <span class="math notranslate nohighlight">\(S=s\)</span>, individuals in all groups have the same
probability to actually belong to the positive class: <br>
<span class="math notranslate nohighlight">\(\label{eq:calib}
P[Y =1 \mid S =s,A = 0] = P[Y =1 \mid S =s,A = 1] \quad \forall s \in [0,1].\)</span>
<br></p>
<p><strong>Well-calibration</strong> <span id="id25">[<a class="reference internal" href="#id2631">10</a>]</span> is a stronger variant of
calibration. It requires that (1) calibration is satisfied, (2) the
score is interpreted as the probability to truly belong to the positive
class, and (3) for each score <span class="math notranslate nohighlight">\(S=s\)</span>, the probability to truly belong to
the positive class is equal to that particular score: <br>
<span class="math notranslate nohighlight">\(\label{eq:wellCalib}
P[Y =1 \mid S =s,A = 0] = P[Y =1 \mid S =s,A = 1] = s  \quad  \forall \; {s \in [0,1]}.\)</span>
<br></p>
<p><strong>Fairness through awareness</strong> <span id="id26">[<a class="reference internal" href="#id2624">12</a>]</span> implies that similar
individuals should have similar predictions. Let <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> be two
individuals represented by their attributes values vectors <span class="math notranslate nohighlight">\(v_i\)</span> and
<span class="math notranslate nohighlight">\(v_j\)</span>. Let <span class="math notranslate nohighlight">\(d(v_i,v_j)\)</span> represent the similarity distance between
individuals <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. Let <span class="math notranslate nohighlight">\(M(v_i)\)</span> represent the probability
distribution over the outcomes of the prediction. For example, if the
outcome is binary (<span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>), <span class="math notranslate nohighlight">\(M(v_i)\)</span> might be <span class="math notranslate nohighlight">\([0.2,0.8]\)</span> which
means that for individual <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(P[\hat{Y}=0]) = 0.2\)</span> and
<span class="math notranslate nohighlight">\(P[\hat{Y}=1] = 0.8\)</span>. Let <span class="math notranslate nohighlight">\(d_M\)</span> be a distance metric between
probability distributions. Fairness through awareness is achieved iff,
for any pair of individuals <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>: <br>
<span class="math notranslate nohighlight">\(d_M(M(v_i), M(v_j))  \leq d(v_i, v_j)\)</span> <br>
In practice, fairness through
awareness assumes that the similarity metric is known for each pair of
individuals <span id="id27">[<a class="reference internal" href="#id2632">17</a>]</span>. That is, a challenging aspect of this
approach is the difficulty to determine what is an appropriate metric
function to measure the similarity between two individuals. Typically,
this requires careful human intervention from professionals with domain
expertise <span id="id28">[<a class="reference internal" href="#id2667">18</a>]</span>.
<br></p>
<p><strong>Process fairness</strong>  <span id="id29">[<a class="reference internal" href="#id2665">19</a>]</span> (or procedural fairness) can
be described as a set of subjective fairness notions that are centered
on the process that leads to outcomes. These notions are not focused on
the fairness of the outcomes, instead they quantify the fraction of
users that consider fair the use of a particular set of features. They
are subjective as they depend on user judgments which may be obtained by
subjective reasoning.</p>
<p>A natural approach to improve process fairness is to remove all
sensitive (protected or salient) features before training classifiers.
This simple approach connects process fairness to <em>fairness through
unawareness</em>. However, there is a trade-off to manage since dropping out
sensitive features may impact negatively classification
performance <span id="id30">[<a class="reference internal" href="#id2666">20</a>]</span>.
<br></p>
<p><strong>Nonstatistical fairness metrics:</strong> Recently, further metrics have been
proposed and that differ from the previous in that they do not fully
rely on statistical considerations, and take into account domain
knowledge, that is not directly observable from data, require expert
input, or reason about hypothetical situations. As they fall out of the
scope of this chapter, we will not further dwell into these and simply
mention a few to the interested reader: <em>total effect</em>
<span id="id31">[<a class="reference internal" href="#id2668">21</a>]</span> (that is the “causal” version of statistical
parity and measures the effect of changing the value of an attribute,
taking into account a given causal graph), <em>effect of treatment of the
treated</em> <span id="id32">[<a class="reference internal" href="#id2668">21</a>]</span> (that relies on counterfactuals with
respect to sensitive features and measures the difference between the
probabilities of instances and their counterfactuals), and
<em>counterfactual fairness</em> <span id="id33">[<a class="reference internal" href="#id2667">18</a>]</span> (which is a
fine-grained variant of the previous but with respect to the set all
features).
<br></p>
<p><strong>Discussion:</strong> As the above fairness metrics often conflict, and it is
not possible to be fair according to all of these definitions, it is a
challenge to choose the relevant metric to focus on. While still very
much an open research area, some suggestions on how one can deal with
conflicts between fairness metrics can be found in
<span id="id34">[<a class="reference internal" href="#id2548">3</a>, <a class="reference internal" href="#id2549">22</a>]</span>. Indeed, fairness metrics
frequently conflict with other metrics such as accuracy and privacy.
<span id="id35">[<a class="reference internal" href="#id2550">23</a>]</span> show that in a credit scoring case enforcing
fairness metrics can lead to significant drops in accuracy and, thus,
maximum profit. This is unavoidable: improvements on fairness often
result in lower accuracy, and research on the Pareto frontier for this
trade-off is now emerging <span id="id36">[<a class="reference internal" href="#id2552">24</a>, <a class="reference internal" href="#id2551">25</a>]</span>.
Similarly, there is a trade-off between fairness and privacy, as
fairness metrics typically require sensitive information in order to be
used. As a result, fairness affects privacy (and vice versa), for
example in facial recognition <span id="id37">[<a class="reference internal" href="#id2553">26</a>]</span> and medical applications
<span id="id38">[<a class="reference internal" href="#id2554">27</a>]</span>.</p>
<p>Finally, there is a connection between <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/Diversity_Non-Discrimination_and_Fairness"><span class="doc std std-doc">fairness</span></a> and <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/justice"><span class="doc">Justice</span></a>, seen in
for example Rawls’ work on Justice as Fairness <span id="id39">[<a class="reference internal" href="#id2547">28</a>]</span>. And
indeed, a range of theories of (distributive) justice describe how
benefits and burdens should be distributed (cf. the entry on <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/justice"><span class="doc">Justice</span></a>). As such,
they can be seen as guiding the outcomes of algorithms even if they
describe what these distributions should be in society as a whole. Yet,
as <span id="id40">[<a class="reference internal" href="#id2542">29</a>]</span> argue at length, there is little overlap
between theories of distributive justice and fairness metrics.
Non-comparative notions of justice are not captured by fairness metrics,
nor are notions such as Rawls’ difference principle, on which the right
distribution is the one where the worst off have the highest absolute
level of benefits. Fairness metrics have focused more on
<a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/discrimination"><span class="doc std std-doc">discrimination</span></a> than on <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/justice"><span class="doc std std-doc">justice</span></a>.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id41"><dl class="citation">
<dt class="label" id="id2564"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Deirdre K Mulligan, Joshua A Kroll, Nitin Kohli, and Richmond Y Wong. This thing called fairness: disciplinary confusion realizing a value in technology. <em>Proceedings of the ACM on Human-Computer Interaction</em>, 3(CSCW):1–36, 2019.</p>
</dd>
<dt class="label" id="id2697"><span class="brackets"><a class="fn-backref" href="#id3">2</a></span></dt>
<dd><p>Jose M. Alvarez, Alejandra Bringas Colmenarejo, Alaa Elobaid, Simone Fabbrizzi, Miriam Fahimi, Antonio Ferrara, Siamak Ghodsi, Carlos Mougan, Ioanna Papageorgiou, Paula Reyero, Mayra Russo, Kristen M. Scott, Laura State, Xuan Zhao, and Salvatore Ruggieri. Policy advice and best practices on bias and fairness in AI. <em>Ethics Inf. Technol.</em>, 26(2):31, 2024.</p>
</dd>
<dt class="label" id="id2548"><span class="brackets"><a class="fn-backref" href="#id34">3</a></span></dt>
<dd><p>Michele Loi and Markus Christen. Choosing how to discriminate: navigating ethical trade-offs in fair algorithmic design for the insurance sector. <em>Philosophy &amp; Technology</em>, 34(4):967–992, 2021.</p>
</dd>
<dt class="label" id="id2628"><span class="brackets">4</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id4">2</a>,<a href="#id5">3</a>)</span></dt>
<dd><p>Sandra Wachter, Brent Mittelstadt, and Chris Russell. Bias preservation in machine learning: the legality of fairness metrics under eu non-discrimination law. <em>W. Va. L. Rev.</em>, 123:735, 2020.</p>
</dd>
<dt class="label" id="id2565"><span class="brackets"><a class="fn-backref" href="#id6">5</a></span></dt>
<dd><p>Philips George John, Deepak Vijaykeerthy, and Diptikalyan Saha. Verifying individual fairness in machine learning models. In <em>UAI</em>, volume 124 of Proceedings of Machine Learning Research, 749–758. AUAI Press, 2020.</p>
</dd>
<dt class="label" id="id2566"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p>Asia J. Biega, Krishna P. Gummadi, and Gerhard Weikum. Equity of attention: amortizing individual fairness in rankings. In <em>SIGIR</em>, 405–414. ACM, 2018.</p>
</dd>
<dt class="label" id="id2570"><span class="brackets"><a class="fn-backref" href="#id7">7</a></span></dt>
<dd><p>Yu Cheng, Zhihao Jiang, Kamesh Munagala, and Kangning Wang. Group fairness in committee selection. <em>ACM Transactions on Economics and Computation (TEAC)</em>, 8(4):1–18, 2020.</p>
</dd>
<dt class="label" id="id2571"><span class="brackets"><a class="fn-backref" href="#id7">8</a></span></dt>
<dd><p>Vincent Conitzer, Rupert Freeman, Nisarg Shah, and Jennifer Wortman Vaughan. Group fairness for the allocation of indivisible goods. In <em>AAAI</em>, 1853–1860. AAAI Press, 2019.</p>
</dd>
<dt class="label" id="id2727"><span class="brackets"><a class="fn-backref" href="#id8">9</a></span></dt>
<dd><p>Michael J. Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: auditing and learning for subgroup fairness. In <em>ICML</em>, volume 80 of Proceedings of Machine Learning Research, 2569–2577. PMLR, 2018.</p>
</dd>
<dt class="label" id="id2631"><span class="brackets">10</span><span class="fn-backref">(<a href="#id9">1</a>,<a href="#id23">2</a>,<a href="#id25">3</a>)</span></dt>
<dd><p>Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In <em>ITCS</em>, volume 67 of LIPIcs, 43:1–43:23. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2017.</p>
</dd>
<dt class="label" id="id2572"><span class="brackets"><a class="fn-backref" href="#id10">11</a></span></dt>
<dd><p>Lydia T. Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machine learning. In <em>IJCAI</em>, 6196–6200. ijcai.org, 2019.</p>
</dd>
<dt class="label" id="id2624"><span class="brackets">12</span><span class="fn-backref">(<a href="#id11">1</a>,<a href="#id26">2</a>)</span></dt>
<dd><p>Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In <em>Proceedings of the 3rd innovations in theoretical computer science conference</em>, 214–226. 2012.</p>
</dd>
<dt class="label" id="id2626"><span class="brackets">13</span><span class="fn-backref">(<a href="#id13">1</a>,<a href="#id17">2</a>)</span></dt>
<dd><p>Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost of fairness. In <em>KDD</em>, 797–806. ACM, 2017.</p>
</dd>
<dt class="label" id="id2627"><span class="brackets">14</span><span class="fn-backref">(<a href="#id15">1</a>,<a href="#id16">2</a>)</span></dt>
<dd><p>Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In <em>NIPS</em>, 3315–3323. 2016.</p>
</dd>
<dt class="label" id="id2629"><span class="brackets">15</span><span class="fn-backref">(<a href="#id18">1</a>,<a href="#id20">2</a>,<a href="#id21">3</a>,<a href="#id22">4</a>)</span></dt>
<dd><p>Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal justice risk assessments: the state of the art. <em>Sociological Methods &amp; Research</em>, 50:3–44, 2018.</p>
</dd>
<dt class="label" id="id2630"><span class="brackets">16</span><span class="fn-backref">(<a href="#id19">1</a>,<a href="#id24">2</a>)</span></dt>
<dd><p>Alexandra Chouldechova. Fair prediction with disparate impact: a study of bias in recidivism prediction instruments. <em>Big data</em>, 5(2):153–163, 2017.</p>
</dd>
<dt class="label" id="id2632"><span class="brackets"><a class="fn-backref" href="#id27">17</a></span></dt>
<dd><p>Michael P. Kim, Omer Reingold, and Guy N. Rothblum. Fairness through computationally-bounded awareness. In <em>NeurIPS</em>, 4847–4857. 2018.</p>
</dd>
<dt class="label" id="id2667"><span class="brackets">18</span><span class="fn-backref">(<a href="#id28">1</a>,<a href="#id33">2</a>)</span></dt>
<dd><p>Matt J. Kusner, Joshua R. Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In <em>NIPS</em>, 4066–4076. 2017.</p>
</dd>
<dt class="label" id="id2665"><span class="brackets"><a class="fn-backref" href="#id29">19</a></span></dt>
<dd><p>Nina Grgic-Hlaca, Muhammad Bilal Zafar, Krishna P. Gummadi, and Adrian Weller. Beyond distributive fairness in algorithmic decision making: feature selection for procedurally fair learning. In <em>AAAI</em>, 51–60. AAAI Press, 2018.</p>
</dd>
<dt class="label" id="id2666"><span class="brackets"><a class="fn-backref" href="#id30">20</a></span></dt>
<dd><p>Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fairness beyond disparate treatment &amp; disparate impact: learning classification without disparate mistreatment. In <em>WWW</em>, 1171–1180. ACM, 2017.</p>
</dd>
<dt class="label" id="id2668"><span class="brackets">21</span><span class="fn-backref">(<a href="#id31">1</a>,<a href="#id32">2</a>)</span></dt>
<dd><p>Judea Pearl. <em>Causality</em>. Cambridge university press, 2009.</p>
</dd>
<dt class="label" id="id2549"><span class="brackets"><a class="fn-backref" href="#id34">22</a></span></dt>
<dd><p>Michelle Seng Ah Lee, Luciano Floridi, and Jatinder Singh. Formalising trade-offs beyond algorithmic fairness: lessons from ethical philosophy and welfare economics. <em>AI Ethics</em>, 1(4):529–544, 2021.</p>
</dd>
<dt class="label" id="id2550"><span class="brackets"><a class="fn-backref" href="#id35">23</a></span></dt>
<dd><p>Nikita Kozodoi, Johannes Jacob, and Stefan Lessmann. Fairness in credit scoring: assessment, implementation and profit implications. <em>European Journal of Operational Research</em>, 297(3):1083–1094, 2022.</p>
</dd>
<dt class="label" id="id2552"><span class="brackets"><a class="fn-backref" href="#id36">24</a></span></dt>
<dd><p>Susan Wei and Marc Niethammer. The fairness-accuracy pareto front. <em>Statistical Analysis and Data Mining: The ASA Data Science Journal</em>, 2020.</p>
</dd>
<dt class="label" id="id2551"><span class="brackets"><a class="fn-backref" href="#id36">25</a></span></dt>
<dd><p>Annie Liang, Jay Lu, and Xiaosheng Mu. Algorithmic design: fairness versus accuracy. <em>arXiv preprint arXiv:2112.09975</em>, 2021. URL: <a class="reference external" href="https://arxiv.org/abs/2112.09975">https://arxiv.org/abs/2112.09975</a>.</p>
</dd>
<dt class="label" id="id2553"><span class="brackets"><a class="fn-backref" href="#id37">26</a></span></dt>
<dd><p>Alice Xiang. Being'seen'vs.'mis-seen': tensions between privacy and fairness in computer vision. <em>Harvard Journal of Law &amp; Technology, Forthcoming</em>, 2022.</p>
</dd>
<dt class="label" id="id2554"><span class="brackets"><a class="fn-backref" href="#id38">27</a></span></dt>
<dd><p>Andrew Chester, Yun Sing Koh, Jörg Wicker, Quan Sun, and Junjae Lee. Balancing utility and fairness against privacy in medical data. In <em>2020 IEEE Symposium Series on Computational Intelligence (SSCI)</em>, 1226–1233. IEEE, 2020.</p>
</dd>
<dt class="label" id="id2547"><span class="brackets"><a class="fn-backref" href="#id39">28</a></span></dt>
<dd><p>John Rawls. <em>Justice as fairness: A restatement</em>. Harvard University Press, 2001.</p>
</dd>
<dt class="label" id="id2542"><span class="brackets"><a class="fn-backref" href="#id40">29</a></span></dt>
<dd><p>Matthias Kuppler, Christoph Kern, Ruben L Bach, and Frauke Kreuter. Distributive justice and fairness metrics in automated decision-making: how much overlap is there? <em>arXiv preprint arXiv:2105.01441</em>, 2021. URL: <a class="reference external" href="https://arxiv.org/abs/2105.01441">https://arxiv.org/abs/2105.01441</a>.</p>
</dd>
<dt class="label" id="id2569"><span class="brackets"><a class="fn-backref" href="#id2951">30</a></span></dt>
<dd><p>Faisal Kamiran, Indre Zliobaite, and Toon Calders. Quantifying explainable discrimination and removing illegal discrimination in automated decision making. <em>Knowl. Inf. Syst.</em>, 35(3):613–644, 2013.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Resmi Ramachandranpillai, Fredrik Heintz, Stefan Buijsman, Miguel Couceiro, Guilherme Alves, Karima Makhlouf, and Sami Zhioua.</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="statistical"><span class="brackets"><a class="fn-backref" href="#id12">1</a></span></dt>
<dd><p><span class="math notranslate nohighlight">\(TP,FP,FN,\)</span> and <span class="math notranslate nohighlight">\(TN\)</span> stand for: true positives, false positives, false negatives, and true negatives, respectively.</p>
</dd>
<dt class="label" id="parity"><span class="brackets"><a class="fn-backref" href="#id14">2</a></span></dt>
<dd><p>Called explanatory features in <span id="id2951">[<a class="reference internal" href="#id2569">30</a>]</span>.</p>
</dd>
</dl>
</div>
</div>
<span id="document-Diversity_Non-Discrimination_and_Fairness/fair_ML"></span><div class="tex2jax_ignore mathjax_ignore section" id="fair-machine-learning">
<h4>Fair Machine Learning<a class="headerlink" href="#fair-machine-learning" title="Permalink to this headline">¶</a></h4>
<!-- TODO: fairlean.py is recognized as a link... -->
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p><strong>Fair Machine Learning models</strong> take into account the issues of bias and fairness. Approaches can be categorized as pre-processig, which transform the
input data, as in-processing, which modify the learning algorithm, and post-processing, which alter models’ internals or their decisions.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>Fairness can be promoted in three different ways in ML as surveyed in <span id="id1">[<a class="reference internal" href="#id2549">3</a>]</span>. This survey provides a clear categorization of methods under pre-process, in-process and post-process approaches.</p>
<p><em>Pre-process approaches</em> are the most flexible ones that transforms the data so that the underlying bias is removed. One advantage of using pre-process techniques is that it is the most inspectable method, as it is the earliest opportunity to mitigate biases and measure how it affects the outcome compared to the other two approaches in fair machine learning <span id="id2">[<a class="reference internal" href="#id2548">4</a>]</span>. Suppression or  Fairness Through Unawareness is a baseline method that accounts for removing the sensitive features and proxy sensitive features from the dataset <span id="id3">[<a class="reference internal" href="#id2593">5</a>]</span>. A recent study <span id="id4">[<a class="reference internal" href="#id2594">6</a>]</span> proved that removing sensitive information does not guarantee fair outcomes.
Massaging the dataset (relabeling) method can act in two ways:</p>
<ol class="simple">
<li><p>Identify unfair outcomes and correct them by changing the label to what ought to have happened.</p></li>
<li><p>Identify sensitive classes and relabel them so that the outcome is fair.
Reweighting approach has several positive aspects compared to suppression and relabeling. It works by postulating that a fair dataset would have no conditional dependence on the outcome of any of the sensitive attributes. That means it corrects the past unfair outcomes by giving more weightage to correct cases and less weightage to incorrect cases. Learning fair representations approach fairness fundamentally differently by aiming for a middle ground between-group fairness and individual fairness <span id="id5">[<a class="reference internal" href="#id2550">7</a>]</span>. It turns the pre-process problem into a combined optimization problem that finds trade-offs between-group fairness, individual fairness, and accuracy.</p></li>
</ol>
<p><em>In-process approaches</em> modify the learning algorithms to remove biases during model training by either incorporating fairness into the optimization equation or imposing a constraint as regularization <span id="id6">[<a class="reference internal" href="#id2551">8</a>, <a class="reference internal" href="#id2552">9</a>]</span>. The main categories of in-processing approaches are adversarial debiasing and prejudice removal. The former involves an adversary to predict the sensitive attributes from a downstream task (classification or regression), and thus the model learns a representation independent of sensitive features. Learning fair representations can be done by adding noise to the predictive power using the regulation. Adversarial reweighted learning <span id="id7">[<a class="reference internal" href="#id2553">10</a>]</span> uses non-sensitive features and labels to measure unfairness and co-train the adversarial reweighting approach to improving learning. On the other hand, the prejudice remover approach has various techniques to mitigate biases during training.
Some of the standard methods are:</p>
<ol class="simple">
<li><p>Heuristic-based: Use Roony rules <span id="id8">[<a class="reference internal" href="#id2554">11</a>]</span>, which effectively rank problems.</p></li>
<li><p>Algorithmic Changes: These can be made in every single step of calibration, such as input, output, and model structure <span id="id9">[<a class="reference internal" href="#id2548">4</a>, <a class="reference internal" href="#id2555">12</a>, <a class="reference internal" href="#id2556">13</a>, <a class="reference internal" href="#id2557">14</a>, <a class="reference internal" href="#id2558">15</a>, <a class="reference internal" href="#id2559">16</a>, <a class="reference internal" href="#id2560">17</a>, <a class="reference internal" href="#id2561">18</a>, <a class="reference internal" href="#id2562">19</a>]</span>.</p></li>
<li><p>Using pre-trained models: It involves combining available pre-trained models and transferring them to reduce bias <span id="id10">[<a class="reference internal" href="#id2563">20</a>]</span>.</p></li>
<li><p>Counterfactual and Causal Reasoning: This considers a model to be group or individual fair if its prediction in the real world is similar to the counterfactual world, where individuals belong to a different protected group. Causal reasoning can be used to caution against those counterfactual explanations <span id="id11">[<a class="reference internal" href="#id2564">21</a>, <a class="reference internal" href="#id2565">22</a>]</span>. A primary concern on the use and misuse of counterfactual fairness has been studied in<span id="id12">[<a class="reference internal" href="#id2566">23</a>]</span>.</p></li>
</ol>
<p>Finally, <em>post-process approaches</em> are the most versatile approaches if the model is already in the production stage and it does not require retraining the model. Another advantage of using post-processing is that the fairness (individual and group) of any downstream tasks can be easily satisfied concerning the domain and application of the model <span id="id13">[<a class="reference internal" href="#id2567">24</a>]</span>. Also, post-processing is agnostic to the input data, which makes it easier to implement. However, post-processing procedures may present weaker results when compare to pre-processing ones <span id="id14">[<a class="reference internal" href="TAILOR.html#id2550">23</a>, <a class="reference internal" href="#id2644">25</a>]</span>.</p>
<p><strong>Assessment tools</strong>: Tools can assist practitioners or organizations in documenting the measures, providing guidance, helping formalize processes, and empowering automated decisions. There are various types of tools to identify and mitigate the biases. Out of which, technical/quantitative tools and qualitative tools are primarily used in real-world applications by engineers and data scientists.Technical/quantitative tools focus on data or AI pipeline through technical solutions. One major drawback is that it may miss essential fairness considerations; for example, it cannot be employed to mitigate bias in the COMPAS algorithm as the nuances could not be adequately captured. It lacks methods to understand and mitigate biases but perpetuates a misleading notion that “Fair ML” is not a complex task to achieve. Some of the standard solutions in this category are:</p>
<ol class="simple">
<li><p>IBM’s AI Fairness 360 Toolkit: It is a python toolkit through the lens of technical solutions under fairness metrics.</p></li>
<li><p>Google’s What-If Tool explores the model’s performance on a dataset through hypothetical situations. It allows users to explore different definitions of fairness constraints under various feature intersections.</p></li>
<li><p>Microsoft’s <a class="reference external" href="http://fairlean.py">fairlean.py</a>: It is a python package consisting of mitigation algorithms and metrics for model assessment.</p></li>
</ol>
<p>On the other hand, Quantitative techniques can delve into the nuances of fairness. They can enable teams to explore the societal implications, analyses fairness harms and tradeoffs, and propose plans to find the potential sources of bias and ways to mitigate them. Two of the most prominent qualitative techniques are:</p>
<ol class="simple">
<li><p>Co-designed AI fairness checklist (2020): This checklist is designed by a group of Microsoft researchers and academicians, 49 individuals from 12 technical organizations. It covers the items included in different stages of the AI pipeline, including envision, define, prototype, build, launch, and evolve, and is customizable according to the deployment.</p></li>
<li><p>Fairness Analytic (2019): This analytic tool is developed by Mulligan et al. to promote fairness at the earlier stages of product development. It enables teams to understand biases from a specific application perspective to analyze and document their effects.</p></li>
</ol>
<p>While these tools exist to analyze the potential harms, it is the responsibility of users to understand the after-effects of which tools they are using, and which types of biases can mitigate. A detailed review of landscape and gaps in fairness tool kits is given in <span id="id15">[<a class="reference internal" href="#id2578">1</a>]</span>.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id16"><dl class="citation">
<dt class="label" id="id2578"><span class="brackets"><a class="fn-backref" href="#id15">1</a></span></dt>
<dd><p>Michelle Seng Ah Lee and Jatinder Singh. The landscape and gaps in open source fairness toolkits. In <em>CHI</em>, 699:1–699:13. ACM, 2021.</p>
</dd>
<dt class="label" id="id2525"><span class="brackets"><a class="fn-backref" href="#id14">2</a></span></dt>
<dd><p>Nikita Kozodoi, Johannes Jacob, and Stefan Lessmann. Fairness in credit scoring: assessment, implementation and profit implications. <em>European Journal of Operational Research</em>, 297(3):1083–1094, 2022.</p>
</dd>
<dt class="label" id="id2549"><span class="brackets"><a class="fn-backref" href="#id1">3</a></span></dt>
<dd><p>Simon Caton and Christian Haas. Fairness in machine learning: a survey. <em>arXiv preprint arXiv:2010.04053</em>, 2020. URL: <a class="reference external" href="https://arxiv.org/abs/2010.04053">https://arxiv.org/abs/2010.04053</a>.</p>
</dd>
<dt class="label" id="id2548"><span class="brackets">4</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id9">2</a>)</span></dt>
<dd><p>Brian d'Alessandro, Cathy O'Neil, and Tom LaGatta. Conscientious classification: a data scientist's guide to discrimination-aware classification. <em>Big data</em>, 5(2):120–134, 2017.</p>
</dd>
<dt class="label" id="id2593"><span class="brackets"><a class="fn-backref" href="#id3">5</a></span></dt>
<dd><p>Pratik Gajane and Mykola Pechenizkiy. On formalizing fairness in prediction with machine learning. <em>arXiv preprint arXiv:1710.03184</em>, 2017. URL: <a class="reference external" href="https://arxiv.org/abs/1710.03184">https://arxiv.org/abs/1710.03184</a>.</p>
</dd>
<dt class="label" id="id2594"><span class="brackets"><a class="fn-backref" href="#id4">6</a></span></dt>
<dd><p>Boris Ruf and Marcin Detyniecki. Active fairness instead of unawareness. <em>arXiv preprint arXiv:2009.06251</em>, 2020. URL: <a class="reference external" href="https://arxiv.org/abs/2009.06251">https://arxiv.org/abs/2009.06251</a>.</p>
</dd>
<dt class="label" id="id2550"><span class="brackets"><a class="fn-backref" href="#id5">7</a></span></dt>
<dd><p>Richard S. Zemel, Yu Wu, Kevin Swersky, Toniann Pitassi, and Cynthia Dwork. Learning fair representations. In <em>ICML (3)</em>, volume 28 of JMLR Workshop and Conference Proceedings, 325–333. JMLR.org, 2013.</p>
</dd>
<dt class="label" id="id2551"><span class="brackets"><a class="fn-backref" href="#id6">8</a></span></dt>
<dd><p>Rachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh, Kush R. Varshney, and Yunfeng Zhang. AI fairness 360: an extensible toolkit for detecting and mitigating algorithmic bias. <em>IBM J. Res. Dev.</em>, 63(4/5):4:1–4:15, 2019.</p>
</dd>
<dt class="label" id="id2552"><span class="brackets"><a class="fn-backref" href="#id6">9</a></span></dt>
<dd><p>Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael J. Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth. A convex framework for fair regression. <em>CoRR</em>, 2017.</p>
</dd>
<dt class="label" id="id2553"><span class="brackets"><a class="fn-backref" href="#id7">10</a></span></dt>
<dd><p>Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and Ed H. Chi. Fairness without demographics through adversarially reweighted learning. In <em>NeurIPS</em>. 2020.</p>
</dd>
<dt class="label" id="id2554"><span class="brackets"><a class="fn-backref" href="#id8">11</a></span></dt>
<dd><p>Caitlin Kuhlman, MaryAnn Van Valkenburg, and Elke A. Rundensteiner. FARE: diagnostics for fair ranking using pairwise error metrics. In <em>WWW</em>, 2936–2942. ACM, 2019.</p>
</dd>
<dt class="label" id="id2555"><span class="brackets"><a class="fn-backref" href="#id9">12</a></span></dt>
<dd><p>Benjamin Fish, Jeremy Kun, and Ádám Dániel Lelkes. A confidence-based approach for balancing fairness and accuracy. In <em>SDM</em>, 144–152. SIAM, 2016.</p>
</dd>
<dt class="label" id="id2556"><span class="brackets"><a class="fn-backref" href="#id9">13</a></span></dt>
<dd><p>Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt, Zhe Zhao, Lichan Hong, Ed H. Chi, and Cristos Goodrow. Fairness in recommendation ranking through pairwise comparisons. In <em>KDD</em>, 2212–2220. ACM, 2019.</p>
</dd>
<dt class="label" id="id2557"><span class="brackets"><a class="fn-backref" href="#id9">14</a></span></dt>
<dd><p>Dylan Slack, Sorelle A. Friedler, and Emile Givental. Fairness warnings and fair-MAML: learning fairly with minimal data. In <em>FAT*</em>, 200–209. ACM, 2020.</p>
</dd>
<dt class="label" id="id2558"><span class="brackets"><a class="fn-backref" href="#id9">15</a></span></dt>
<dd><p>Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Allison Woodruff, Christine Luu, Pierre Kreitmann, Jonathan Bischof, and Ed H. Chi. Putting fairness principles into practice: challenges, metrics, and improvements. In <em>AIES</em>, 453–459. ACM, 2019.</p>
</dd>
<dt class="label" id="id2559"><span class="brackets"><a class="fn-backref" href="#id9">16</a></span></dt>
<dd><p>Jialu Wang, Yang Liu, and Caleb C. Levy. Fair classification with group-dependent label noise. In <em>FAccT</em>, 526–536. ACM, 2021.</p>
</dd>
<dt class="label" id="id2560"><span class="brackets"><a class="fn-backref" href="#id9">17</a></span></dt>
<dd><p>Cynthia Dwork, Nicole Immorlica, Adam Tauman Kalai, and Mark D. M. Leiserson. Decoupled classifiers for group-fair and efficient machine learning. In <em>FAT</em>, volume 81 of Proceedings of Machine Learning Research, 119–133. PMLR, 2018.</p>
</dd>
<dt class="label" id="id2561"><span class="brackets"><a class="fn-backref" href="#id9">18</a></span></dt>
<dd><p>Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard S. Zemel. The variational fair autoencoder. In <em>ICLR</em>. 2016.</p>
</dd>
<dt class="label" id="id2562"><span class="brackets"><a class="fn-backref" href="#id9">19</a></span></dt>
<dd><p>Anay Mehrotra and L. Elisa Celis. Mitigating bias in set selection with noisy protected attributes. In <em>FAccT</em>, 237–248. ACM, 2021.</p>
</dd>
<dt class="label" id="id2563"><span class="brackets"><a class="fn-backref" href="#id10">20</a></span></dt>
<dd><p>David Madras, Elliot Creager, Toniann Pitassi, and Richard S. Zemel. Learning adversarially fair and transferable representations. In <em>ICML</em>, volume 80 of Proceedings of Machine Learning Research, 3381–3390. PMLR, 2018.</p>
</dd>
<dt class="label" id="id2564"><span class="brackets"><a class="fn-backref" href="#id11">21</a></span></dt>
<dd><p>Joshua R Loftus, Chris Russell, Matt J Kusner, and Ricardo Silva. Causal reasoning for algorithmic fairness. <em>arXiv preprint arXiv:1805.05859</em>, 2018. URL: <a class="reference external" href="https://arxiv.org/abs/1805.05859">https://arxiv.org/abs/1805.05859</a>.</p>
</dd>
<dt class="label" id="id2565"><span class="brackets"><a class="fn-backref" href="#id11">22</a></span></dt>
<dd><p>Razieh Nabi, Daniel Malinsky, and Ilya Shpitser. Learning optimal fair policies. In <em>ICML</em>, volume 97 of Proceedings of Machine Learning Research, 4674–4682. PMLR, 2019.</p>
</dd>
<dt class="label" id="id2566"><span class="brackets"><a class="fn-backref" href="#id12">23</a></span></dt>
<dd><p>Atoosa Kasirzadeh and Andrew Smart. The use and misuse of counterfactuals in ethical machine learning. In <em>FAccT</em>, 228–236. ACM, 2021.</p>
</dd>
<dt class="label" id="id2567"><span class="brackets"><a class="fn-backref" href="#id13">24</a></span></dt>
<dd><p>Pranay Kr. Lohia, Karthikeyan Natesan Ramamurthy, Manish Bhide, Diptikalyan Saha, Kush R. Varshney, and Ruchir Puri. Bias mitigation post-processing for individual and group fairness. In <em>ICASSP</em>, 2847–2851. IEEE, 2019.</p>
</dd>
<dt class="label" id="id2644"><span class="brackets"><a class="fn-backref" href="#id14">25</a></span></dt>
<dd><p>Dana Pessach and Erez Shmueli. A review on fairness in machine learning. <em>ACM Computing Surveys (CSUR)</em>, 55(3):1–44, 2022.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Resmi Ramachandran Pillai, Fredrik Heintz, Miguel Couceiro, and Guilherme Alves.</p>
</div></blockquote>
</div>
</div>
<span id="document-Diversity_Non-Discrimination_and_Fairness/discrimination"></span><div class="tex2jax_ignore mathjax_ignore section" id="grounds-of-discrimination">
<h4>Grounds of Discrimination<a class="headerlink" href="#grounds-of-discrimination" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p>International and national laws prohibit <strong>discriminating on some explicitly defined grounds</strong>, such as race, sex, religion, etc. They can be considered in isolation, or interacting, giving rise to multiple discrimination and intersectional discrimination.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>The Universal Declaration of Human Rights prohibit discrimination in several grounds <a class="footnote-reference brackets" href="#declaration" id="id1">1</a>;</p>
<ol class="simple">
<li><p>race,</p></li>
<li><p>skin colour,</p></li>
<li><p>sex,</p></li>
<li><p>language,</p></li>
<li><p>religion,</p></li>
<li><p>political or other opinion,</p></li>
<li><p>national or social origin,</p></li>
<li><p>property, or</p></li>
<li><p>birth <span id="id2">[<a class="reference internal" href="#id2500">2</a>]</span>, <br>
although the list is not exhaustive.</p></li>
</ol>
<p>By directly addressing these grounds, the Declaration highlights the problematic of considering decisions or regulations on them while leaves the door open to a more extensive view by prohibiting as well discrimination based on <em>other grounds</em>. By doing so, the Declaration implies that any difference in treatment or exercise with respect to the rights encompassed in the Declaration would have legal implications. Therefore, grounds of discrimination should not be considered a closed and fixed list but an enumeration opened to debate and reflection as the circumstances and context require. For example, the African Charter on Human and People’s Rights prohibits discrimination in grounds of fortune, rather than property <span id="id3">[<a class="reference internal" href="#id2501">3</a>]</span> whereas the American Convention on Human Rights includes economic status <span id="id4">[<a class="reference internal" href="#id2507">4</a>]</span> and the Charter of Fundamental Rights of the European Union (E.U.) adds the association with a national minority <span id="id5">[<a class="reference internal" href="TAILOR.html#id2496">4</a>]</span>.</p>
<p>To this regard, <em>grounds of discrimination</em> encompass three different motives on which decisions and policies should not be based (see also the entry <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/equity"><span class="doc">Discrimination &amp; Equity</span></a>); (1) grounds innate to the individual such as race, gender, age, disability, (2) grounds intrinsic to the individual freedom and autonomy that is political belief or religion, and (3) grounds highly founded on stereotypes or stigma and which are usually irrelevant for social, economic, or politic interactions, as sexual orientation or ethnicity) <span id="id6">[<a class="reference internal" href="#id2508">6</a>]</span>. The use of any of these grounds is often perceived as a lack of impartiality influenced by negative and prejudiced reasons and emotions towards certain members of the society. Prohibiting discrimination on these grounds aims to ensure that the distribution of social goods and services do not respond to subjective and irrational feelings, whether that turns out to be an advantage or a disadvantage for the individual or group concerned.</p>
<p><em>Sex</em> refers to a person’s biological status, categorized as male, female, or intersex; <em>gender</em> refers to the attitudes and behaviors that a culture associates with a person’s sex, categorized as
masculine, feminine and transgender (gender identity different from sex assigned at birth or non-binary); <em>sexual orientation</em> refers to the sex of those to whom one is sexually and romantically attracted, categorized as homosexual, heterosexual, and bisexual.
See <span id="id7">[<a class="reference internal" href="#id2559">7</a>]</span> for a psychological discussion of the differences between the terms, <span id="id8">[<a class="reference internal" href="#id2471">8</a>]</span> for a discussion with reference to the United States (U.S.) anti-discrimination law, and <span id="id9">[<a class="reference internal" href="#id2506">9</a>]</span> for a comparative analysis on anti-discrimination European Law. A country profile report on the legal rights of lesbian, gay, bisexual and transgender and (LGBT) people is published yearly<a class="footnote-reference brackets" href="#lgbt" id="id10">2</a> by Human Rights Watch.  Human-Computer Interaction research is also addressing the extent to which AI systems “express gender and sexuality explicitly and through relation of experience and emotions, mimicking the human language on which they are trained” <span id="id11">[<a class="reference internal" href="#id2462">10</a>]</span>.</p>
<p><em>Race</em> is a social construct to categorize people into groups. The term is controversial, and with little consensus on its actual meaning.
<span id="id12">[<a class="reference internal" href="#id2470">11</a>]</span> summarizes biological and social concepts of race, and discuss U.S. categorizations of races used for data collection, e.g., in census data.</p>
<p><em>Ethnicity</em> refers to self-identifying groups based on beliefs concerning shared culture, ancestry and history. The distinction between <em>race</em> and <em>ethnic</em> grounds is, nonetheless, a provocative issue primarily in Europe where after the Second War World the notion of <em>race</em> become some sort of a <em>taboo</em>. By consequence, the lacking of words, academic work, and policies addressing <em>race (un)justice</em> has also resulted on a downplay of race grounds of discrimination and the indistinct use of <em>ethnic origin</em> as <em>race</em> with mislead intentions <span id="id13">[<a class="reference internal" href="#id2520">12</a>]</span>.</p>
<p>Legislations and research studies have evolved with a different focus on vulnerable groups, sometimes restricting themselves to specific settings, including redit and insurance, sale, rental, and financing of housing, personnel selection and wages, access to public accommodation, and education. For instance, discrimination against Afro-Americans is dealt with to a large extent by studies from the U.S., whilst discrimination against Roma people has been mainly considered by E.U. studies.</p>
<p>Although the aforementioned grounds for discrimination are typically considered separately, the interaction of multiple forms of discrimination has been receiving increasing attention <span id="id14">[<a class="reference internal" href="#id2473">13</a>, <a class="reference internal" href="#id2472">14</a>]</span>. An elderly disabled woman for example, could be discriminated against for being above a certain age, because she is a woman, because she is disabled, or any combination of these. <em>Multiple</em> discrimination comes into play when a person is discriminated against on the basis of different characteristics <em>at different times</em>: each type of
discrimination works independently, according to distinct experiences, and multiple discrimination refers to their cumulative impact. When different grounds operate \emph{at the same time}, then this is known as <em>compound</em> or <span class="xref myst">intersectional</span> discrimination. Compound discrimination (sometimes called <em>additive multiple discrimination</em>)
occurs when each ground <em>adds</em> to discrimination on other grounds, for example migrant women experiencing both under-employment (such as migrants compared to local residents) and lower pay (such as female workers compared to male workers).
Intersectional discrimination occurs when concurrent acts of discrimination result in a specific and distinct form of discrimination <span id="id15">[<a class="reference internal" href="#id2463">15</a>]</span>. For example, <span id="id16">[<a class="reference internal" href="#id2474">16</a>]</span> reports the case of Afro-American women stereotypes which when taken in isolation cover neither women nor Afro-Americans.</p>
<p>Grounds of discrimination are key inputs in the design of fair AI systems: fairness metrics, for instance, rely on comparing models’ performances across protected and unprotected groups. We refer to the entries on <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/fairness"><span class="doc std std-doc">fairness</span></a> and <span class="xref myst">fair ML</span> for details.</p>
<p>Here, we concentrate on the problem of faithfully representing grounds of discrimination in data, by distinguishing the coding of human identity in raw data (<em>datafication</em>) and the representativeness of grounds of discrimination in data (<em>representation bias</em>).</p>
<p>For instance, if gender is coded with a binary feature (male/female), then any further discrimination analysis is limited to contrasting only such two groups, excluding non-binary people. There is then the need for a more elaborate representation of human identity in raw data, e.g, using ontologies for concept reasoning <span id="id17">[<a class="reference internal" href="#id2505">17</a>]</span>. Moreover, the categories used to encode grounds of discrimination may embed forms of structural discrimination, which is hidden when  features  are considered in isolation, but made apparent when connected with other features in a knowledge graph <span id="id18">[<a class="reference internal" href="#id2504">18</a>]</span>. The issue of <em>source criticism</em> <span id="id19">[<a class="reference internal" href="#id2651">19</a>]</span>, which is central historical and humanistic disciplines, is still in its infancy in the area of big data and AI. Source criticism attains at the provenance, authenticity, and completeness of data collected, especially in social media platforms. For instance, the mechanisms of social software, such as the option given to users to identify their gender as binary, result into functional biases <span id="id20">[<a class="reference internal" href="#id2652">20</a>]</span> of the data collected.</p>
<p>Beyond the complexity of datafication, the  representiveness of grounds of discrimination in datasets <span id="id21">[<a class="reference internal" href="#id2502">21</a>]</span> also affects discrimination and diversity analyses, and the fairness of AI models trained over those datasets (see also the entry on <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/bias"><span class="doc">Bias</span></a>).</p>
<p>Most of the grounds of discrimination fall in the category of sensitive personal data whose collection and processing is prohibited under several privacy and data protection laws, unless certain exceptions apply. For example, the grounds of race, ethnic origin, sexual orientation, political stances, religious beliefs, and trade union membership are considered special categories of personal data under the European General Data Protection Regulation <span id="id22">[<a class="reference internal" href="TAILOR.html#id8">1</a>]</span>. Likewise, the California Privacy Rights Act (CPRA) will include as sensitive attributes, among other, consumer’s racial or ethnic origin, religious or philosophical beliefs <span id="id23">[<a class="reference internal" href="#id2510">22</a>]</span>, while the Virginia Consumer Data Protection Act (VCDPA) will add to those attributes, the ground of mental or physical health, sexual orientation, or citizenship or immigration status <span id="id24">[<a class="reference internal" href="#id2511">23</a>]</span>. From a regulatory perspective, the restriction towards the collection and processing of sensitive personal data intends to minimize the possibilities of algorithmic systems to discriminate people based on intrinsic or innate attributes of the individual. However, some criticism have arisen towards this perspective as more voices defend the need to use sensitive attributes to ensure the non-discriminatory nature of algorithmic models <span id="id25">[<a class="reference internal" href="#id2513">24</a>]</span>. The European Proposal for Regulating Artificial Intelligence (Artificial Intelligence Act)  seems to have reflect on this position as it will introduce a exception allowing, to the extent that it is strictly necessary for the purposes of ensuring bias monitoring, detection and correction in relation to the high-risk AI systems, the processing of special categories of data <span id="id26">[<a class="reference internal" href="#id2514">25</a>]</span>.</p>
<p>Discrimination grounds in datasets can be be the output of an inference. For instance, gender may be explicitly given (e.g., in a registration form) with consent to a specific usage (e.g., personalization), or it can be inferred using supervised learning <span id="id27">[<a class="reference internal" href="#id2653">26</a>]</span>. A growing number of AI approaches can infer people’s personality traits <span id="id28">[<a class="reference internal" href="#id2655">27</a>]</span>, to be used e.g., for personalization and recommendation purposes. To some extent, even in cases where the system is blinded to protected attributes, inferences can lead to discriminatory results as the system finds correlations directly related to grounds of discriminatory. Inferences can, therefore, be quite problematic because they can reinforce the historical disadvantage and inequalities suffered by certain members of the society <span id="id29">[<a class="reference internal" href="#id2515">28</a>]</span>. As the current legal protections rest on the restricted access to data reveling the belonging of an individual to a protected group or prohibition of use of such data to motivate a decision, the access and use of such information indirectly creates a threat to individuals’ rights <span id="id30">[<a class="reference internal" href="#id2516">29</a>]</span>. For this reason, the correctness of such inferences can be crucial on attributed grounds of discrimination and, consequently, on decisions and fairness analyses. Despite inferences offer new possibilities for biased and invasive decision-making, the legal status of inferred personal, both with respect to data protection and anti-discrimination laws, is quite debated <span id="id31">[<a class="reference internal" href="#id2654">30</a>]</span>.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id32"><dl class="citation">
<dt class="label" id="id37"><span class="brackets"><a class="fn-backref" href="#id22">1</a></span></dt>
<dd><p>European Parliament &amp; Council. General data protection regulation. 2016. L119, 4/5/2016, p. 1–88.</p>
</dd>
<dt class="label" id="id2500"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>UN General Assembly and others. Universal declaration of human rights. <em>UN General Assembly</em>, 302(2):14–25, 1948.</p>
</dd>
<dt class="label" id="id2501"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>The African Union. African Charter on Human and Peoples' Rights. 1981.</p>
</dd>
<dt class="label" id="id2507"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Organization of American States. American Convention on Human Rights Pact of San Jose, Costa Rica (B-32). 1969.</p>
</dd>
<dt class="label" id="id2488"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>&quot;European Parliament and the Council&quot;. Charter of fundamental rights of the european union. 2007.</p>
</dd>
<dt class="label" id="id2508"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p>Janneke Gerards. The discrimination grounds of article 14 of the european convention on human rights. <em>Human Rights Law Review</em>, 13(1):99–124, 2013.</p>
</dd>
<dt class="label" id="id2559"><span class="brackets"><a class="fn-backref" href="#id7">7</a></span></dt>
<dd><p>American Psychological Association. Guidelines for psychological practice with lesbian, gay, and bisexual clients. 2011. URL: <a class="reference external" href="http://www.apa.org/pi/lgbt/resources/guidelines.aspx">http://www.apa.org/pi/lgbt/resources/guidelines.aspx</a>.</p>
</dd>
<dt class="label" id="id2471"><span class="brackets"><a class="fn-backref" href="#id8">8</a></span></dt>
<dd><p>Mary Anne C. Case. Disaggregating gender from sex and sexual orientation: The effeminate man in the law and feminist jurisprudence. <em>The Yale Law Journal</em>, 105(1):1–105, 1995.</p>
</dd>
<dt class="label" id="id2506"><span class="brackets"><a class="fn-backref" href="#id9">9</a></span></dt>
<dd><p>FRA. Protection against discrimination on grounds of sexual orientation, gender identity and sex characteristics in the EU-comparative legal analysis. 2015.</p>
</dd>
<dt class="label" id="id2462"><span class="brackets"><a class="fn-backref" href="#id11">10</a></span></dt>
<dd><p>Justin Edwards, Leigh Clark, and Allison Perrone. Lgbtq-ai? exploring expressions of gender and sexual orientation in chatbots. In <em>CUI</em>, 2:1–2:4. ACM, 2021.</p>
</dd>
<dt class="label" id="id2470"><span class="brackets"><a class="fn-backref" href="#id12">11</a></span></dt>
<dd><p>Rebecca M. Blank, Marilyn Dabady, and Constance F. Citro, editors. <em>Measuring Racial Discrimination - Panel on Methods for Assessing Discrimination</em>. National Academies Press, 2004.</p>
</dd>
<dt class="label" id="id2520"><span class="brackets"><a class="fn-backref" href="#id13">12</a></span></dt>
<dd><p>Nicolas Kayser-Bril. Europeans can’t talk about racist ai systems. they lack the words. 2021. URL: <a class="reference external" href="https://algorithmwatch.org/en/europeans-cant-talk-about-racist-ai-systems-they-lack-the-words/">https://algorithmwatch.org/en/europeans-cant-talk-about-racist-ai-systems-they-lack-the-words/</a>.</p>
</dd>
<dt class="label" id="id2473"><span class="brackets"><a class="fn-backref" href="#id14">13</a></span></dt>
<dd><p>European Commission. Tackling multiple discrimination: Practices, policies and laws. 2007. Directorate General for Employment, Social Affairs and Equal Opportunities, Unit G.4. URL: <a class="reference external" href="http://ec.europa.eu/social/main.jsp?catId=738\&amp;pubId=51">http://ec.europa.eu/social/main.jsp?catId=738\&amp;pubId=51</a>.</p>
</dd>
<dt class="label" id="id2472"><span class="brackets"><a class="fn-backref" href="#id14">14</a></span></dt>
<dd><p>ENAR. European network against racism, Fact sheet 44: the legal implications of multiple discrimination. 2011. URL: <a class="reference external" href="https://www.enar-eu.org/wp-content/uploads/fs44_-_the_legal_implications_of_multiple_discrimination_final_en.pdf">https://www.enar-eu.org/wp-content/uploads/fs44_-_the_legal_implications_of_multiple_discrimination_final_en.pdf</a>.</p>
</dd>
<dt class="label" id="id2463"><span class="brackets"><a class="fn-backref" href="#id15">15</a></span></dt>
<dd><p>European Commission, Directorate-General for Justice and Consumers and Sandra Fredman. <em>Intersectional discrimination in EU gender equality and non-discrimination law</em>. Publications Office, 2016.</p>
</dd>
<dt class="label" id="id2474"><span class="brackets"><a class="fn-backref" href="#id16">16</a></span></dt>
<dd><p>T. Makkonen. Compound and intersectional discrimination: bringing the experiences of the most marginalized to the fore. 2002. Unpublished manuscript, Institute for Human Rights, Abo Alademi University.</p>
</dd>
<dt class="label" id="id2505"><span class="brackets"><a class="fn-backref" href="#id17">17</a></span></dt>
<dd><p>Clair A. Kronk and Judith W. Dexheimer. Development of the gender, sex, and sexual orientation ontology: evaluation and workflow. <em>J. Am. Medical Informatics Assoc.</em>, 27(7):1110–1115, 2020.</p>
</dd>
<dt class="label" id="id2504"><span class="brackets"><a class="fn-backref" href="#id18">18</a></span></dt>
<dd><p>Christopher L. Dancy and P. Khalil Saucier. AI and blackness: towards moving beyond bias and representation. <em>CoRR</em>, 2021.</p>
</dd>
<dt class="label" id="id2651"><span class="brackets"><a class="fn-backref" href="#id19">19</a></span></dt>
<dd><p>Gertraud Koch and Katharina Kinder-Kurlanda. Source criticism of data platform logics on the internet. <em>Historical Social Research</em>, 45(3):270–287, 2020.</p>
</dd>
<dt class="label" id="id2652"><span class="brackets"><a class="fn-backref" href="#id20">20</a></span></dt>
<dd><p>Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre Kiciman. Social data: biases, methodological pitfalls, and ethical boundaries. <em>Frontiers Big Data</em>, 2:13, 2019.</p>
</dd>
<dt class="label" id="id2502"><span class="brackets"><a class="fn-backref" href="#id21">21</a></span></dt>
<dd><p>Nima Shahbazi, Yin Lin, Abolfazl Asudeh, and H. V. Jagadish. A survey on techniques for identifying and resolving representation bias in data. <em>CoRR</em>, 2022.</p>
</dd>
<dt class="label" id="id2510"><span class="brackets"><a class="fn-backref" href="#id23">22</a></span></dt>
<dd><p>California Statu Legislature and the Council. California consumer privacy act of 2018 [1798.100 - 1798.199.100]. 2018.</p>
</dd>
<dt class="label" id="id2511"><span class="brackets"><a class="fn-backref" href="#id24">23</a></span></dt>
<dd><p>Virginia Senate. Virginia consumer data protection act. Effective January 1, 2023.</p>
</dd>
<dt class="label" id="id2513"><span class="brackets"><a class="fn-backref" href="#id25">24</a></span></dt>
<dd><p>Indrė Žliobaitė and Bart Custers. Using sensitive personal data may be necessary for avoiding discrimination in data-driven decision models. <em>Artificial Intelligence and Law</em>, 24(2):183–201, 2016.</p>
</dd>
<dt class="label" id="id2514"><span class="brackets"><a class="fn-backref" href="#id26">25</a></span></dt>
<dd><p>European Parliament and the Council. Regulation of the european parliament and of the council laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain union legislative acts. 2021.</p>
</dd>
<dt class="label" id="id2653"><span class="brackets"><a class="fn-backref" href="#id27">26</a></span></dt>
<dd><p>Lucía Santamaría and Helena Mihaljevic. Comparison and benchmark of name-to-gender inference services. <em>PeerJ Comput. Sci.</em>, 4:e156, 2018.</p>
</dd>
<dt class="label" id="id2655"><span class="brackets"><a class="fn-backref" href="#id28">27</a></span></dt>
<dd><p>Alessandro Vinciarelli and Gelareh Mohammadi. A survey of personality computing. <em>IEEE Trans. Affect. Comput.</em>, 5(3):273–291, 2014.</p>
</dd>
<dt class="label" id="id2515"><span class="brackets"><a class="fn-backref" href="#id29">28</a></span></dt>
<dd><p>Raphaele Xenidis. Tuning eu equality law to algorithmic discrimination: three pathways to resilience. <em>Maastricht Journal of European and Comparative Law</em>, 27(6):736–758, 2020.</p>
</dd>
<dt class="label" id="id2516"><span class="brackets"><a class="fn-backref" href="#id30">29</a></span></dt>
<dd><p>Solon Barocas. Data mining and the discourse on discrimination. In <em>Data Ethics Workshop, Conference on Knowledge Discovery and Data Mining</em>, 1–4. 2014.</p>
</dd>
<dt class="label" id="id2654"><span class="brackets"><a class="fn-backref" href="#id31">30</a></span></dt>
<dd><p>Sandra Wachter and Brent Mittelstadt. A right to reasonable inferences: re-thinking data protection law in the age of big data and AI. <em>Columbia Business Law Review</em>, 2019(2):494–620, 2019.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Alejandra Bringas Colmenarejo and Salvatore Ruggieri.</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="declaration"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Protected group, protected grounds and prohibited grounds are also used as synonymous of grounds of discrimination.</p>
</dd>
<dt class="label" id="lgbt"><span class="brackets"><a class="fn-backref" href="#id10">2</a></span></dt>
<dd><p>For 2021, see <a href="https://www.hrw.org/video-photos/interactive/2022/05/19/2022-country-profiles-sexual-orientation-and-gender" target=_blank>the Human Right Watch World Report</a>.</p>
</dd>
</dl>
</div>
</div>
<span id="document-Diversity_Non-Discrimination_and_Fairness/intersectionality"></span><div class="tex2jax_ignore mathjax_ignore section" id="intersectionality">
<h4>Intersectionality<a class="headerlink" href="#intersectionality" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p><strong>Intersectionality</strong> focuses on a specific type of bias due to the combination of sensitive factors. An individual might not be discriminated against based on race or based on gender only, but she might be discriminated against because of a combination of both. Black women are particularly prone to this type of discrimination.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>Historically, the topic of intersectionality has been influenced by several studies in the late twentieth century about violence against dark skinned women <span id="id1">[<a class="reference internal" href="#id2689">4</a>]</span>. On one hand, feminist efforts focused on politicizing experiences of women and on the other hand, antiracist efforts focused on politicizing experiences of people of color. However, both types of activists have frequently proceeded as though the issues and experiences they each detail occur on mutually exclusive terrains.
Consequently, this intersectional identity as both women and with dark skin within a discourse targeting to respond to one or the other, women of color are marginalized within both. That is, the intersection of racism and sexism in black women’s lives cannot be captured by looking at the race or gender dimensions separately.</p>
<p>In 2018, the Gender Shades project <span id="id2">[<a class="reference internal" href="#id2691">1</a>, <a class="reference internal" href="#id2692">2</a>, <a class="reference internal" href="#id2690">5</a>]</span> described the first intersectional evaluation of face recognition systems. Instead of evaluating accuracy
by gender or skin type alone, Buolamwini et al.~examined four intersectional subgroups: dark-skinned females, dark-skinned males, light-skinned females, and light-skinned males. The three evaluated commercial gender classifiers, namely, Microsoft Cognitive Services Face <span id="id3">[<a class="reference internal" href="#id2693">6</a>]</span>, IBM Watson Visual Recognition <span id="id4">[<a class="reference internal" href="#id2694">7</a>]</span>, and Face++ (a computer vision company headquartered in China <span id="id5">[<a class="reference internal" href="#id2695">8</a>]</span>), had the lowest accuracy on dark-skinned females. More precisely, they had the highest error rates for all evaluated gender classifiers ranging from <span class="math notranslate nohighlight">\(20\)</span> to <span class="math notranslate nohighlight">\(35\%\)</span>.</p>
<div class="figure align-center" id="fig-interaction">
<a class="reference internal image-reference" href="_images/Interaction.png"><img alt="_images/Interaction.png" src="_images/Interaction.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 25 </span><span class="caption-text">Interaction Bias, where <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are sensitive variables and <span class="math notranslate nohighlight">\(Y\)</span> is an outcome.</span><a class="headerlink" href="#fig-interaction" title="Permalink to this image">¶</a></p>
</div>
<p>Studying the problem of discrimination from the lenses of causality, intersectionality can be captured by the concept of interaction <span id="id6">[<a class="reference internal" href="#id2696">9</a>]</span>. Interaction is observed when two causes of the outcome interact with each other, making the joint effect smaller or greater than the sum of individual effects. Interaction is graphically illustrated in Figure <a class="reference internal" href="#fig-interaction"><span class="std std-numref">25</span></a>, where <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are sensitive attributes and <span class="math notranslate nohighlight">\(Y\)</span> is the outcome<a class="footnote-reference brackets" href="#binary" id="id7">1</a>. Note that regular DAGs are not able to express interaction. Figure <a class="reference internal" href="#fig-interaction"><span class="std std-numref">25</span></a> employs the graphical representation proposed by <span id="id8">[<a class="reference internal" href="#id2598">10</a>]</span>. The arrows pointing to arrows instead of nodes account for the interaction term.</p>
<p>The interaction term coincides with the portion of the effect due to intersectionality and can be expressed as follows:<br>
<span class="math notranslate nohighlight">\(\textit{Interaction}(A,B) = P(Y_1|a_1, b_1) - P(Y_1|a_0, b_1) - P(Y_1|a_1, b_0) + P(Y_1|a_0, b_0)\)</span><br></p>
<p>In practice, intersectionality presents some challenges. The first challenge is related to the complexity of data collection and analysis. In other words, collecting and analyzing data on intersecting factors can be challenging due to limited resources, methodological constraints, and privacy concerns. Second, groups that are not (yet) defined in anti-discrimination law may exist and may need protection <span id="id9">[<a class="reference internal" href="#id2697">11</a>]</span>.</p>
<p>More generally, there are three practical concerns along the machine learning pipeline related to intersectionality in a fairness context <span id="id10">[<a class="reference internal" href="#id2698">12</a>]</span>. The first problem is related to which factors to consider when training models. It is advised to evaluate the most granular intersecting factors in the dataset while integrating domain knowledge with experiments to understand which are best to include when training models. The second problem concerns the challenge of handling increasingly small groups of individuals. Due to additional ethical considerations, it is advisable to refrain from directly applying conventional machine learning techniques to imbalanced data. Instead, exploring inherent structures among groups with shared identities is encouraged.
The third problem is related to evaluating a large number of sub-groups. In the context of binary attributes, fairness assessment often involves comparing group differences using performance metrics derived from the confusion matrix. When dealing with more than two groups, evaluation metrics have a similar structure, albeit more generalized.
They are typically expressed in relation to the maximum disparity (or ratio) of a performance metric either between one group and all others <span id="id11">[<a class="reference internal" href="TAILOR.html#id2727">9</a>, <a class="reference internal" href="#id2699">13</a>, <a class="reference internal" href="#id2701">14</a>]</span> or between two specific groups <span id="id12">[<a class="reference internal" href="#id2702">15</a>, <a class="reference internal" href="#id2703">16</a>]</span>.
Hence, methods are proposed for conducting pairwise comparisons more judiciously and introducing supplementary metrics to capture broader algorithmic trends that existing metrics may overlook <span id="id13">[<a class="reference internal" href="#id2698">12</a>]</span>.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id14"><dl class="citation">
<dt class="label" id="id2691"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>Joy Adowaa Buolamwini. <em>Gender shades: Intersectional phenotypic and demographic evaluation of face datasets and gender classifiers</em>. PhD thesis, Massachusetts Institute of Technology, 2017.</p>
</dd>
<dt class="label" id="id2692"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Joy Buolamwini and Timnit Gebru. Gender shades: intersectional accuracy disparities in commercial gender classification. In <em>FAT</em>, volume 81 of Proceedings of Machine Learning Research, 77–91. PMLR, 2018.</p>
</dd>
<dt class="label" id="id2700"><span class="brackets"><a class="fn-backref" href="#id11">3</a></span></dt>
<dd><p>Michael J. Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: auditing and learning for subgroup fairness. In <em>ICML</em>, volume 80 of Proceedings of Machine Learning Research, 2569–2577. PMLR, 2018.</p>
</dd>
<dt class="label" id="id2689"><span class="brackets"><a class="fn-backref" href="#id1">4</a></span></dt>
<dd><p>Kimberle Crenshaw. Mapping the margins: intersectionality, identity politics, and violence against women of color. <em>Stan. L. Rev.</em>, 43:1241, 1990.</p>
</dd>
<dt class="label" id="id2690"><span class="brackets"><a class="fn-backref" href="#id2">5</a></span></dt>
<dd><p>Joy Buolamwini, Timnit Gebru, Helen Raynham, Deborah Raji, and Ethan Zuckerman. Gender Shades Project. <span><a class="reference external" href="#"></a></span>https://www.media.mit.edu/projects/gender-shades, 2018.</p>
</dd>
<dt class="label" id="id2693"><span class="brackets"><a class="fn-backref" href="#id3">6</a></span></dt>
<dd><p>Microsoft. Face API. <span><a class="reference external" href="#"></a></span>https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/identity-api-reference.</p>
</dd>
<dt class="label" id="id2694"><span class="brackets"><a class="fn-backref" href="#id4">7</a></span></dt>
<dd><p>IBM. Watson Visual Recognition. <span><a class="reference external" href="#"></a></span>https://mediacenter.ibm.com/media/IBM+Watson+Visual+Recognition/0_jbsmp6lq.</p>
</dd>
<dt class="label" id="id2695"><span class="brackets"><a class="fn-backref" href="#id5">8</a></span></dt>
<dd><p>Face++. AI Open Platform. <span><a class="reference external" href="#"></a></span>https://www.faceplusplus.com/face-detection/.</p>
</dd>
<dt class="label" id="id2696"><span class="brackets"><a class="fn-backref" href="#id6">9</a></span></dt>
<dd><p>Tyler J VanderWeele and Mirjam J Knol. A tutorial on interaction. <em>Epidemiologic Methods</em>, 3(1):33–72, 2014.</p>
</dd>
<dt class="label" id="id2598"><span class="brackets"><a class="fn-backref" href="#id8">10</a></span></dt>
<dd><p>Clarice R Weinberg. Can dags clarify effect modification? <em>Epidemiology</em>, 18(5):569, 2007.</p>
</dd>
<dt class="label" id="id2697"><span class="brackets"><a class="fn-backref" href="#id9">11</a></span></dt>
<dd><p>Sandra Wachter and Brent Mittelstadt. A right to reasonable inferences: re-thinking data protection law in the age of big data and ai. <em>Colum. Bus. L. Rev.</em>, pages 494, 2019.</p>
</dd>
<dt class="label" id="id2698"><span class="brackets">12</span><span class="fn-backref">(<a href="#id10">1</a>,<a href="#id13">2</a>)</span></dt>
<dd><p>Angelina Wang, Vikram V. Ramaswamy, and Olga Russakovsky. Towards intersectionality in machine learning: including more identities, handling underrepresentation, and performing evaluation. In <em>FAccT</em>, 336–349. ACM, 2022.</p>
</dd>
<dt class="label" id="id2699"><span class="brackets"><a class="fn-backref" href="#id11">13</a></span></dt>
<dd><p>Wei Guo and Aylin Caliskan. Detecting emergent intersectional biases: contextualized word embeddings contain a distribution of human-like biases. In <em>AIES</em>, 122–133. ACM, 2021.</p>
</dd>
<dt class="label" id="id2701"><span class="brackets"><a class="fn-backref" href="#id11">14</a></span></dt>
<dd><p>Forest Yang, Mouhamadou Cisse, and Oluwasanmi Koyejo. Fairness with overlapping groups; a probabilistic perspective. In <em>NeurIPS</em>. 2020.</p>
</dd>
<dt class="label" id="id2702"><span class="brackets"><a class="fn-backref" href="#id12">15</a></span></dt>
<dd><p>James R. Foulds, Rashidul Islam, Kamrun Naher Keya, and Shimei Pan. An intersectional definition of fairness. In <em>ICDE</em>, 1918–1921. IEEE, 2020.</p>
</dd>
<dt class="label" id="id2703"><span class="brackets"><a class="fn-backref" href="#id12">16</a></span></dt>
<dd><p>Avijit Ghosh, Lea Genuit, and Mary Reagan. Characterizing intersectional group fairness with worst-case comparisons. In <em>AIDBEI</em>, volume 142 of Proceedings of Machine Learning Research, 22–34. PMLR, 2021.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Karima Makhlouf and Sami Zhioua.</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="binary"><span class="brackets"><a class="fn-backref" href="#id7">1</a></span></dt>
<dd><p>Without loss of generality, all three variables are assumed to be binary with sets of values <span class="math notranslate nohighlight">\(\{a_0,a_1\}\)</span>, <span class="math notranslate nohighlight">\(\{b_0,b_1\}\)</span>, and <span class="math notranslate nohighlight">\(\{y_0,y_1\}\)</span>, respectively.</p>
</dd>
</dl>
</div>
</div>
<span id="document-Diversity_Non-Discrimination_and_Fairness/justice"></span><div class="tex2jax_ignore mathjax_ignore section" id="justice">
<h4>Justice<a class="headerlink" href="#justice" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p><strong>Justice</strong> encompasses three different perspectives: (1) <em>fairness</em> understood as the fair treatment of people, (2) <em>rightness</em> as the quality of being fair or reasonable, and (3) a legal system, the scheme or system of law. Justice can be distinguished between <em>substantive</em> and <em>procedural</em>.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>It is commonly accepted that, <em>justice</em> entails “the proper administration of the law; the fair and equitable treatment of all individuals under the law” <span id="id1">[<a class="reference internal" href="#id2483">1</a>]</span>.
Therefore, <em>justice</em> encompasses three different perspectives, (1) <em>fairness</em> understood as the fair treatment of people, (2) <em>rightness</em> as the quality of being fair or reasonable, and (3) a legal system, the scheme or system of law, in which every person receives his/her/its due from the system, including all rights, both natural and legal <span id="id2">[<a class="reference internal" href="#id2484">5</a>]</span>. Artificial Intelligence (AI) can be a tool for administering justice in the legal system, or it can itself be subject to the requirements of fairness and rightness when used for automated decision making (ADM).
In the former case, AI can be adopted at several levels of autonomy <span id="id3">[<a class="reference internal" href="#id2467">6</a>]</span>, e.g., from no automation to superhuman autonomous AI for legal reasoning. For a state of the art of the use of Machine Learning (ML) in the criminal justice system (mainly in the United States), see <span id="id4">[<a class="reference internal" href="#id2469">7</a>]</span>. Several books and newspapers commentaries warn about the risks of using AI for justice administration <span id="id5">[<a class="reference internal" href="#id2476">8</a>]</span>.
In the latter case, the design of AI-based systems can benefit from discussion and theories of justice in the legal and ethical disciplines.
However, the above conceptualization of <em>justice</em> has given rise to an endless and ongoing debate regarding whether justice is an inherent component of the law, not separate or distinct from it, or is simply a moral judgment about law <span id="id6">[<a class="reference internal" href="#id2485">9</a>]</span>. In essence, the debate considers whether justice understood as fairness and rightness is independent from the law, or to what extent the <em>law</em> includes considerations of justice and the legal system simply applies justice to human conflicts. In conclusion, the concept of <em>justice</em> is as central to legal theory as it is difficult to define.</p>
<p>Nevertheless, a range of different components or categories of justice have been defined, both in the philosophical literature <span id="id7">[<a class="reference internal" href="#id2506">10</a>]</span> and in the legal literature <span id="id8">[<a class="reference internal" href="#id2477">11</a>]</span>. These can be understood along several distinctions, starting with one between <em>substantive</em> and <em>procedural</em> justice. This is the difference between considering justice in terms of the outcomes which have to meet certain standard in order to be just <span id="id9">[<a class="reference internal" href="#id2493">12</a>]</span>, versus considering justice in terms of a procedure which meets certain standards (and possibly considering the outcomes of any such procedure as being just regardless of the resulting distributions <span id="id10">[<a class="reference internal" href="#id2534">13</a>]</span>). It is, however, common to consider procedural justice partly in terms of the results (e.g., a trial procedure is just if it – at least – mostly acquits the innocent and punishes the guilty). As such, substantive justice is the main notion to discuss here, although the use of AI in procedures also affects questions of procedural justice.</p>
<p>Substantive justice in turn can be viewed in different ways. First, there is a question of whether one focuses on <em>distributive</em> justice or on <em>corrective</em> justice. Distributive justice deals with the distribution of the benefits and burdens of social cooperation <span id="id11">[<a class="reference internal" href="#id2493">12</a>]</span>. These can be <em>comparative</em>, such as the theory of (strict) egalitarianism which requires that resources are distributed to minimize overall inequality <span id="id12">[<a class="reference internal" href="#id2535">14</a>]</span> and other versions of egalitarianism. For example, on Luck egalitarianism (and, closely related, Equality of Opportunity) inequalities in the final distribution may be allowed only in so far as they are not the result of luck or a difference of opportunity <span id="id13">[<a class="reference internal" href="#id2537">15</a>, <a class="reference internal" href="#id2536">16</a>]</span>.
By far the most influential, however, has been Rawls’ view of Justice as Fairness, which combines a requirement of equality of opportunity with the difference principle: unequal distributions have to satisfy a min-max condition where “they are to be to the greatest benefit of the least advantaged members of society.” <span id="id14">[<a class="reference internal" href="#id2538">17</a>]</span>
Alternatively, distributive notions of justice can be \textit{non-comparative}. Sufficiency principles <span id="id15">[<a class="reference internal" href="#id2540">18</a>]</span>, requiring that everyone receives a minimally sufficient amount of resources are a clear example of a distributive justice notion that doesn’t involve comparisons between individuals. Desert-based principles <span id="id16">[<a class="reference internal" href="#id2539">19</a>]</span>, which hold that resources should be allocated based on what individuals deserve can also be non-comparative (if they specify absolute amounts based on what one deserves, as opposed to a share of the total). Views thus differ on what the right principles are for distributive justice. Furthermore, it is interesting that most of these principles have only a limited overlapto
<em>fairness</em> in ADM <span id="id17">[<a class="reference internal" href="TAILOR.html#id2542">29</a>]</span> (cf. the entry on <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/fairness"><span class="doc">Fairness notions and metrics</span></a>).</p>
<p>Where distributive justice focuses on the just distribution of goods, corrective justice concerns the rectification of wrongs or the undoing of transactions which can be either voluntary (contract) or involuntary (when defrauded or a victim of misrepresentation) <span id="id18">[<a class="reference internal" href="#id2542">20</a>, <a class="reference internal" href="#id2494">21</a>]</span>. This differs from distributive justice, as corrective justice first requires a wrong that needs to be corrected, and the correction might violate the ideal distribution of goods according to distributive justice principles. As such, disagreements exist over the priority to be placed between these two principles: is corrective justice merely a way to achieve distributive justice or is corrective justice normatively prior? <span id="id19">[<a class="reference internal" href="#id2543">22</a>]</span> However this issue is settled, it is a matter of fact that corrective justice is an important part of current legal systems. Similarly, <em>retributive</em> justice <span id="id20">[<a class="reference internal" href="#id2544">23</a>]</span>, which focuses on the compensation of the victim of criminal behaviour and the punishment of the lawbreaker, is crucial to our current systems.</p>
<p>In all these cases procedures are followed to make decisions on the distribution of resources, the appropriate corrections and potential punishments. <em>Procedural</em> justice relates to the normative conditions that these procedures have to meet. As such, it encompasses principles of legality, proportionality, effective remedy, fair trial, presumption of innocence and right of defence <span id="id21">[<a class="reference internal" href="#id2496">4</a>, <a class="reference internal" href="#id2497">24</a>, <a class="reference internal" href="#id2545">25</a>, <a class="reference internal" href="#id2498">26</a>]</span>.
Procedural justice is also affected by the use of AI, as this changes procedures and so new standards have to be found for when a procedure including AI is just. Such standards are also needed for transparency, and the notion of procedural justice has been used to propose such a standard by <span id="id22">[<a class="reference internal" href="#id2558">27</a>]</span>, who argue that what matters to determine the justness of an algorithm is the goals of the algorithm as well as how effectively they are met. That is intended to allow an evaluation of the justness of the procedure, and thus of the use of the algorithm. For instance, <span id="id23">[<a class="reference internal" href="#id2501">28</a>]</span> conducted experiments involving laypeople, that showed a “fairness gap” between human judges and AI robot judges. Such a gap is reduced by enhancing the interpretability of AI decisions.</p>
<p>Part of the question of what procedures are just is that of which political procedures should be decided on. <em>Political</em> justice addresses the foundational issues of political rights and responsibilities embedded in constitutional theory and how individuals shall share the control over the shape of the constitution<a class="footnote-reference brackets" href="#constitution" id="id24">1</a> <span id="id25">[<a class="reference internal" href="#id2495">29</a>]</span>.
<em>Social</em> justice, on the other hand, addresses how members should compare under the basic structure of the society <span id="id26">[<a class="reference internal" href="#id2495">29</a>]</span>, and, its “primary task is not so much to save the computational infrastructure AI and ICTs rely on but rather to defend society” <span id="id27">[<a class="reference internal" href="#id2566">30</a>]</span>.
This concern decisions about the broad shape of society and thus cannot readily be solved with fair ML tools. Yet algorithms can help in the implementation of these decisions. As such, <em>justice</em> can be seen to differ from <em>fairness</em>: its scope is often broader, and it is not restricted to questions of equality between different groups to which an algorithm is applied. See e.g., <span id="id28">[<a class="reference internal" href="#id2502">31</a>]</span> for a discussion of <em>data justice</em>, pertaining to “the way people are made visible, represented and treated as a result of their production of digital data”,
and the literature on organizational justice theory <span id="id29">[<a class="reference internal" href="#id2459">32</a>, <a class="reference internal" href="#id2458">33</a>]</span> for the notion of <em>interactional justice</em> pertaining to how workers are treated with respect and dignity (<em>interpersonal justice</em>) and how they are provided with explanations of business process and outcomes (<em>informational justice</em>). Still, the design of AI systems intersects with all of the notions of justice discussed here.</p>
<p>The increased use of AI and ADM reflects a tendency to <em>solutionism</em> <span id="id30">[<a class="reference internal" href="#id2500">34</a>]</span>, where technical solutions are offered to solve all social and economic problems <span id="id31">[<a class="reference internal" href="#id2499">35</a>]</span>. However, unfair, discriminatory, and unjustified decisions affecting different aspects of individuals’ economy and private life has encouraged a critical reflection on questions regarding “what, then, do we talk about when we talk about governing algorithms?” <span id="id32">[<a class="reference internal" href="#id2503">36</a>]</span>. Likewise, the proposed <em>algorithmic justice</em> <span id="id33">[<a class="reference internal" href="#id2486">37</a>]</span> strives to address these unintentional effects provoked by the use of ADM for the allocation of welfare services. This novel conception of justice, founded on Nancy Fraser’s <em>abnormal justice theory</em>, defends “the need to expand our collective understanding of justice, beyond issues of equal access to, and equal distribution of justice” <span id="id34">[<a class="reference internal" href="#id2504">38</a>, <a class="reference internal" href="#id2505">39</a>]</span> as referred <span id="id35">[<a class="reference internal" href="#id2486">37</a>]</span> in order to recognise, debate, diagnose and address harmful effects of ADM in the allocation of transformative services <span id="id36">[<a class="reference internal" href="#id2486">37</a>]</span>.
That being the case, (un)just ADM are more and more scrutinised under the lens of <em>procedural justice</em> as several studies <span id="id37">[<a class="reference internal" href="#id2491">40</a>, <a class="reference internal" href="#id2488">41</a>, <a class="reference internal" href="#id2487">42</a>]</span> “suggest that people do not only care about whether the outcome of a decision benefits them, but also whether it meets standards of justice” <span id="id38">[<a class="reference internal" href="#id2492">2</a>]</span>. To this regard, AI’s <a class="reference internal" href="TAILOR.html#document-Transparency/Transparency"><span class="doc std std-doc">explainability and transparency</span></a> would be crucial to justify and explain the algorithmic decisions and decision-making process and therefore ensure the accountability \and intelligibility of the decisions, and, by extension, of the process (a principle known as <em>open justice</em> when referring to the judicial system <span id="id39">[<a class="reference internal" href="#id2468">43</a>]</span>).</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id40"><dl class="citation">
<dt class="label" id="id2483"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Jeffrey Lehman, Shirelle Phelps, and others. <em>West's encyclopedia of American law</em>. Thomson/Gale, 2004.</p>
</dd>
<dt class="label" id="id2492"><span class="brackets"><a class="fn-backref" href="#id38">2</a></span></dt>
<dd><p>Reuben Binns, Max Van Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, and Nigel Shadbolt. 'it's reducing a human being to a percentage': perceptions of justice in algorithmic decisions. In <em>CHI</em>, 377. ACM, 2018.</p>
</dd>
<dt class="label" id="id2541"><span class="brackets"><a class="fn-backref" href="#id17">3</a></span></dt>
<dd><p>Matthias Kuppler, Christoph Kern, Ruben L Bach, and Frauke Kreuter. Distributive justice and fairness metrics in automated decision-making: how much overlap is there? <em>arXiv preprint arXiv:2105.01441</em>, 2021. URL: <a class="reference external" href="https://arxiv.org/abs/2105.01441">https://arxiv.org/abs/2105.01441</a>.</p>
</dd>
<dt class="label" id="id2496"><span class="brackets"><a class="fn-backref" href="#id21">4</a></span></dt>
<dd><p>&quot;European Parliament and the Council&quot;. Charter of fundamental rights of the european union. 2007.</p>
</dd>
<dt class="label" id="id2484"><span class="brackets"><a class="fn-backref" href="#id2">5</a></span></dt>
<dd><p>Gerald N Hill and Kathleen Hill. <em>The people's law dictionary: Taking the mystery out of legal language</em>. MJF Books, 2002.</p>
</dd>
<dt class="label" id="id2467"><span class="brackets"><a class="fn-backref" href="#id3">6</a></span></dt>
<dd><p>Lance Eliot. Identifying a set of autonomous levels for AI-based computational legal reasoning. <em>MIT Computational Law Report</em>, 2021.</p>
</dd>
<dt class="label" id="id2469"><span class="brackets"><a class="fn-backref" href="#id4">7</a></span></dt>
<dd><p>Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal justice risk assessments: the state of the art. <em>Sociological Methods &amp; Research</em>, 50(1):3–44, 2021.</p>
</dd>
<dt class="label" id="id2476"><span class="brackets"><a class="fn-backref" href="#id5">8</a></span></dt>
<dd><p>Katherine B. Forrest. <em>When Machines Can Be Judge, Jury, and Executioner: Justice the the Age of Artificial Intelligence</em>. World Scientific, 2021.</p>
</dd>
<dt class="label" id="id2485"><span class="brackets"><a class="fn-backref" href="#id6">9</a></span></dt>
<dd><p>Anthony D'Amato. On the connection between law and justice. <em>UC Davis L. Rev.</em>, 26:527, 1992.</p>
</dd>
<dt class="label" id="id2506"><span class="brackets"><a class="fn-backref" href="#id7">10</a></span></dt>
<dd><p>David Miller. Justice. In Edward N. Zalta, editor, <em>The Stanford Encyclopedia of Philosophy</em>. Metaphysics Research Lab, Stanford University, Fall 2021 edition, 2021.</p>
</dd>
<dt class="label" id="id2477"><span class="brackets"><a class="fn-backref" href="#id8">11</a></span></dt>
<dd><p>Richard Susskind. <em>Online Courts and the Future of Justice</em>. Oxford University Press, 2019.</p>
</dd>
<dt class="label" id="id2493"><span class="brackets">12</span><span class="fn-backref">(<a href="#id9">1</a>,<a href="#id11">2</a>)</span></dt>
<dd><p>John Rawls. <em>A theory of justice</em>. Harvard university press, 2020.</p>
</dd>
<dt class="label" id="id2534"><span class="brackets"><a class="fn-backref" href="#id10">13</a></span></dt>
<dd><p>Robert Nozick. <em>Anarchy, state, and utopia</em>. Volume 5038. new york: Basic Books, 1974.</p>
</dd>
<dt class="label" id="id2535"><span class="brackets"><a class="fn-backref" href="#id12">14</a></span></dt>
<dd><p>Iwao Hirose. <em>Egalitarianism</em>. Routledge, 2014.</p>
</dd>
<dt class="label" id="id2537"><span class="brackets"><a class="fn-backref" href="#id13">15</a></span></dt>
<dd><p>Elizabeth S Anderson. What is the point of equality? <em>Ethics</em>, 109(2):287–337, 1999.</p>
</dd>
<dt class="label" id="id2536"><span class="brackets"><a class="fn-backref" href="#id13">16</a></span></dt>
<dd><p>Jonathan Wolff. Fairness, respect and the egalitarian ethos revisited. <em>The Journal of Ethics</em>, 14(3):335–350, 2010.</p>
</dd>
<dt class="label" id="id2538"><span class="brackets"><a class="fn-backref" href="#id14">17</a></span></dt>
<dd><p>John Rawls. Political liberalism. <em>The John Dewey essays in philosophy</em>, 1993.</p>
</dd>
<dt class="label" id="id2540"><span class="brackets"><a class="fn-backref" href="#id15">18</a></span></dt>
<dd><p>Gillian Brock. Sufficiency and needs-based approaches. <em>The Oxford Handbook of Distributive Justice</em>, pages 86–108, 2018.</p>
</dd>
<dt class="label" id="id2539"><span class="brackets"><a class="fn-backref" href="#id16">19</a></span></dt>
<dd><p>Jeffrey Moriarty. Desert-based justice. In <em>The Oxford handbook of distributive justice</em>, pages 152–175. Oxford University Press New York, 2018.</p>
</dd>
<dt class="label" id="id2542"><span class="brackets"><a class="fn-backref" href="#id18">20</a></span></dt>
<dd><p>Arthur Ripstein. The division of responsibility and the law of tort. <em>Fordham L. Rev.</em>, 72:1811, 2003.</p>
</dd>
<dt class="label" id="id2494"><span class="brackets"><a class="fn-backref" href="#id18">21</a></span></dt>
<dd><p>Ernest J Weinrib. <em>The idea of private law</em>. Oxford University Press, 2012.</p>
</dd>
<dt class="label" id="id2543"><span class="brackets"><a class="fn-backref" href="#id19">22</a></span></dt>
<dd><p>Steven Walt. Eliminating corrective justice. <em>Va. L. Rev.</em>, 92:1311, 2006.</p>
</dd>
<dt class="label" id="id2544"><span class="brackets"><a class="fn-backref" href="#id20">23</a></span></dt>
<dd><p>Alec Walen. Retributive Justice. In Edward N. Zalta, editor, <em>The Stanford Encyclopedia of Philosophy</em>. Metaphysics Research Lab, Stanford University, Summer 2021 edition, 2021.</p>
</dd>
<dt class="label" id="id2497"><span class="brackets"><a class="fn-backref" href="#id21">24</a></span></dt>
<dd><p>Steven L Blader and Tom R Tyler. A four-component model of procedural justice: defining the meaning of a “fair” process. <em>Personality and social psychology bulletin</em>, 29(6):747–758, 2003.</p>
</dd>
<dt class="label" id="id2545"><span class="brackets"><a class="fn-backref" href="#id21">25</a></span></dt>
<dd><p>Denise Meyerson and Catriona Mackenzie. Procedural justice and the law. <em>Philosophy Compass</em>, 13(12):e12548, 2018.</p>
</dd>
<dt class="label" id="id2498"><span class="brackets"><a class="fn-backref" href="#id21">26</a></span></dt>
<dd><p>Min Kyung Lee, Anuraag Jain, Hea Jin Cha, Shashank Ojha, and Daniel Kusbit. Procedural justice in algorithmic fairness: leveraging transparency and outcome control for fair algorithmic mediation. <em>Proc. ACM Hum. Comput. Interact.</em>, 3(CSCW):182:1–182:26, 2019.</p>
</dd>
<dt class="label" id="id2558"><span class="brackets"><a class="fn-backref" href="#id22">27</a></span></dt>
<dd><p>Michele Loi, Andrea Ferrario, and Eleonora Viganò. Transparency as design publicity: explaining and justifying inscrutable algorithms. <em>Ethics and Information Technology</em>, 23(3):253–263, 2021.</p>
</dd>
<dt class="label" id="id2501"><span class="brackets"><a class="fn-backref" href="#id23">28</a></span></dt>
<dd><p>Benjamin Minhao Chen, Alexander Stremitzer, and Kevin Tobia. Having your day in robot court. <em>Harvard Journal of Law &amp; Technology</em>, 2022. Forthcoming. URL: <a class="reference external" href="https://ssrn.com/abstract=3841534">https://ssrn.com/abstract=3841534</a>.</p>
</dd>
<dt class="label" id="id2495"><span class="brackets">29</span><span class="fn-backref">(<a href="#id25">1</a>,<a href="#id26">2</a>)</span></dt>
<dd><p>David Sobel, Peter Vallentyne, and Steven Wall. <em>Oxford Studies in Political Philosophy, Volume 1</em>. Oxford University Press, 2015.</p>
</dd>
<dt class="label" id="id2566"><span class="brackets"><a class="fn-backref" href="#id27">30</a></span></dt>
<dd><p>Jasmina Tacheva, Sepideh Namvarrad, and Najla Almissalati. A higher purpose: towards a social justice informatics research framework. In <em>iConference (1)</em>, volume 13192 of Lecture Notes in Computer Science, 265–271. Springer, 2022.</p>
</dd>
<dt class="label" id="id2502"><span class="brackets"><a class="fn-backref" href="#id28">31</a></span></dt>
<dd><p>Linnet Taylor. What is data justice? The case for connecting digital rights and freedoms globally. <em>Big Data &amp; Society</em>, 2017.</p>
</dd>
<dt class="label" id="id2459"><span class="brackets"><a class="fn-backref" href="#id29">32</a></span></dt>
<dd><p>Sarah Bankins, Paul Formosa, Yannick Griep, and Deborah Richards. Decision making with dignity? Contrasting workers’ justice perceptions of human and AI decision making in a human resource management context. <em>Information Systems Frontiers</em>, 2022.</p>
</dd>
<dt class="label" id="id2458"><span class="brackets"><a class="fn-backref" href="#id29">33</a></span></dt>
<dd><p>Lionel P. Robert, Casey Pierce, Liz Marquis, Sangmi Kim, and Rasha Alahmad. Designing fair AI for managing employees in organizations: a review, critique, and design agenda. <em>Hum. Comput. Interact.</em>, 35(5-6):545–575, 2020.</p>
</dd>
<dt class="label" id="id2500"><span class="brackets"><a class="fn-backref" href="#id30">34</a></span></dt>
<dd><p>Evgeny Morozov. <em>To save everything, click here : technology, solutionism and the urge to fix problems that don't exist</em>. London : Allen Lane, 2013.</p>
</dd>
<dt class="label" id="id2499"><span class="brackets"><a class="fn-backref" href="#id31">35</a></span></dt>
<dd><p>Aleš Završnik. Algorithmic justice: algorithms and big data in criminal justice settings. <em>European Journal of criminology</em>, 18(5):623–642, 2021.</p>
</dd>
<dt class="label" id="id2503"><span class="brackets"><a class="fn-backref" href="#id32">36</a></span></dt>
<dd><p>Solon Barocas, Sophie Hood, and Malte Ziewitz. Governing algorithms: A provocation piece. 2013. URL: <a class="reference external" href="https://ssrn.com/abstract=2245322">https://ssrn.com/abstract=2245322</a>.</p>
</dd>
<dt class="label" id="id2486"><span class="brackets">37</span><span class="fn-backref">(<a href="#id33">1</a>,<a href="#id35">2</a>,<a href="#id36">3</a>)</span></dt>
<dd><p>Olivera Marjanovic, Dubravka Cecez-Kecmanovic, and Richard Vidgen. Theorising algorithmic justice. <em>European Journal of Information Systems</em>, pages 1–19, 2021.</p>
</dd>
<dt class="label" id="id2504"><span class="brackets"><a class="fn-backref" href="#id34">38</a></span></dt>
<dd><p>Nancy Fraser. Abnormal justice. <em>Critical inquiry</em>, 34(3):393–422, 2008.</p>
</dd>
<dt class="label" id="id2505"><span class="brackets"><a class="fn-backref" href="#id34">39</a></span></dt>
<dd><p>Nancy Fraser. Injustice at intersecting scales: on ‘social exclusion’and the ‘global poor’. <em>European journal of social theory</em>, 13(3):363–371, 2010.</p>
</dd>
<dt class="label" id="id2491"><span class="brackets"><a class="fn-backref" href="#id37">40</a></span></dt>
<dd><p>E Allan Lind and Tom R Tyler. <em>The social psychology of procedural justice</em>. Springer Science &amp; Business Media, 1988.</p>
</dd>
<dt class="label" id="id2488"><span class="brackets"><a class="fn-backref" href="#id37">41</a></span></dt>
<dd><p>Jason A Colquitt and Jessica B Rodell. Measuring justice and fairness. In <em>The Oxford handbook of justice in the workplace</em>, pages 187–202. Oxford University Press, 2015.</p>
</dd>
<dt class="label" id="id2487"><span class="brackets"><a class="fn-backref" href="#id37">42</a></span></dt>
<dd><p>Adrian Bussone, Simone Stumpf, and Dympna O'Sullivan. The role of explanations on trust and reliance in clinical decision support systems. In <em>ICHI</em>, 160–169. IEEE Computer Society, 2015.</p>
</dd>
<dt class="label" id="id2468"><span class="brackets"><a class="fn-backref" href="#id39">43</a></span></dt>
<dd><p>Adam Pah, David Schwartz, Sarath Sanga, Charlotte Alexander, Kristian Hammond, Luis Amaral, and SCALES OKN Consortium. The promise of AI in an open justice system. <em>AI Magazine</em>, 43(1):69–74, 2022.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Alejandra Bringas Colmenarejo, Stefan Buijsman, and Salvatore Ruggieri.</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="constitution"><span class="brackets"><a class="fn-backref" href="#id24">1</a></span></dt>
<dd><p>See <a href="https://www.europeansocialsurvey.org/data/themes.html?t=justfair" target=_blank>the European Social Survey page</a> for a survey on
perceptions of political justice in Europe.</p>
</dd>
</dl>
</div>
</div>
<span id="document-Diversity_Non-Discrimination_and_Fairness/segregation"></span><div class="tex2jax_ignore mathjax_ignore section" id="segregation">
<h4>Segregation<a class="headerlink" href="#segregation" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p><strong>Social segregation</strong> refers to the separation of groups on the grounds of personal or cultural traits. Separation can be physical (e.g., in schools or neighborhoods) or virtual (e.g., in social networks).</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p><em>Social segregation</em> refers to the “<em>separation of socially defined
groups</em>” <span id="id1">[<a class="reference internal" href="#id2618">1</a>]</span>. People are partitioned into two or
more groups on the grounds of personal or cultural traits that can
foster discrimination, such as gender, age, ethnicity, income, skin
color, language, religion, political opinion, membership of a national
minority, etc. (see entry on <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/discrimination"><span class="doc">Grounds of Discrimination</span></a>). Contact, communication, or interaction
among groups are limited by their physical, working or socio-economic
distance. Such a separation is observed when dissecting the society into
organizational units (neighborhoods, schools, job types).</p>
<div class="figure align-center" id="segregationmap">
<a class="reference internal image-reference" href="_images/segregationmap.png"><img alt="_images/segregationmap.png" src="_images/segregationmap.png" style="width: 700px;" /></a>
<p class="caption"><span class="caption-number">Fig. 26 </span><span class="caption-text">Racial spatial segregation in New York City, based on Census 2000 data
<span id="id2">[<a class="reference internal" href="#id2619">2</a>]</span>. One dot for each 500 residents. Red dots are Whites, blue
dots are Blacks, green dots are Asian, orange dots are Hispanic, and
yellow dots are other races.</span><a class="headerlink" href="#segregationmap" title="Permalink to this image">¶</a></p>
</div>
<p>Early studies on residential segregation trace back to 1930’s <span id="id3">[<a class="reference internal" href="#id2620">3</a>]</span>. In this context, social groups are set apart in neighborhoods where they live in, in schools they
attend to, or in companies they work at. As sharply pointed out in Fig. <a class="reference internal" href="#segregationmap"><span class="std std-numref">26</span></a>, racial segregation (a.k.a. residential segregation on the grounds of race) very often
emerges in cities characterized by ethnic diversity. Schelling’s segregation model <span id="id4">[<a class="reference internal" href="#id2621">4</a>, <a class="reference internal" href="#id2622">5</a>]</span> shows that there is a natural tendency to spatial segregation, as a
collective phenomenon, even if each individual is relatively tolerant – in his famous abstract simulation model, Nobel laureate Schelling assumed that a person changes residence only if
less than 30% of the neighbors are of his/her own race.</p>
<p><span id="id5">[<a class="reference internal" href="#id2623">6</a>]</span> argued that segregation is shifting from ancient forms on the grounds of racial, ethnic and gender traits to modern socio-economic and cultural segregation on the basis of income, job position, and political-religious opinions.
An earlier comparison of ideological segregation of the American electorate online and offline is offered in
<span id="id6">[<a class="reference internal" href="#id2624">7</a>]</span>. The paper found that segregation in news consumption is higher online than offline, but significantly lower than the segregation of face-to-face interactions with neighbors, co-workers, or family members. More recently, it has been warned that the filter bubble generated by personalization of online social networks may foster segregation <span id="id7">[<a class="reference internal" href="#id2625">8</a>]</span>, opinion polarization <span id="id8">[<a class="reference internal" href="#id2626">9</a>]</span>, and lack of consensus between
different social groups. Segregation in  social network has been investigated in <span id="id9">[<a class="reference internal" href="#id2634">10</a>]</span>, with experiments on segregation on the grounds of sex
and age for directors in the boards of the  companies. Other works have focused on religious social networks <span id="id10">[<a class="reference internal" href="#id2643">11</a>]</span>.</p>
<p>A segregation index provides a quantitative measure of the degree of segregation of social groups (e.g.,~Blacks, Whites, Hispanics, etc.) distributed among units of social organization (e.g.,~schools, neighborhoods, jobs, etc.).
Several indexes have been proposed in the literature. The surveys <span id="id11">[<a class="reference internal" href="#id2631">12</a>, <a class="reference internal" href="#id2632">13</a>]</span> represent the earliest attempts to categorize them. Afterward, <span id="id12">[<a class="reference internal" href="#id2633">14</a>]</span> provided a shared classification with reference to five key dimensions: evenness, exposure, concentration, centralization, and clustering. Finally, <span id="id13">[<a class="reference internal" href="#id2644">15</a>]</span> adapts segregation measure to graphs representing social networks.
In this entry, we will consider basic evenness and exposure indexes. Other three classes of indexes are specifically concerned with spatial notions of segregation. Concentration indexes measure the relative amount of physical space occupied by social groups in an urban area.
Centralization indexes measure the degree to which a group is spatially located near the center of an urban area. Clustering indexes measure the degree to which group members live disproportionately in contiguous areas.</p>
<p>We restrict here to consider binary indexes, which assume a partitioning
of the population into two groups, say majority and minority (but could
be men/women, native/immigrant, White/NonWhite, etc.). Let <span class="math notranslate nohighlight">\(T\)</span> be size
of the total population, <span class="math notranslate nohighlight">\(0 &lt; M &lt; T\)</span> be the size of the minority group,
and <span class="math notranslate nohighlight">\(P = M/T\)</span> be the overall fraction of the minority group. Assume that
there are <span class="math notranslate nohighlight">\(n\)</span> organizational units (or simply, units), and that for
<span class="math notranslate nohighlight">\(i \in [1, n]\)</span>, <span class="math notranslate nohighlight">\(t_i\)</span> is the size of the population in unit <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(m_i\)</span>
is the size of the minority group in unit <span class="math notranslate nohighlight">\(i\)</span>, and <span class="math notranslate nohighlight">\(p_i = m_i/t_i\)</span> is
the fraction of the minority population in unit <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p><em>Evenness indexes.</em> Evenness indexes measure the difference in the distributions of
social groups among organizational units.
The <em>dissimilarity index</em> <span class="math notranslate nohighlight">\(D\)</span> is the weighted mean absolute deviation of every unit’s minority proportion from the global minority proportion: <br>
<span class="math notranslate nohighlight">\(D = \frac{1}{2 \cdot P \cdot (1-P)} \sum_{i=1}^n \frac{t_i}{T}
\cdot | p_i - P | \label{equ:dissimilarity}\)</span> <br>
The normalization factor <span class="math notranslate nohighlight">\(2 \cdot P \cdot (1-P)\)</span> is to obtain an index in the range <span class="math notranslate nohighlight">\([0, 1]\)</span>.
Since <span class="math notranslate nohighlight">\(D\)</span> measures dispersion of minorities over the units, higher values of the index mean higher segregation. Dissimilarity is minimum when for all <span class="math notranslate nohighlight">\(i \in [1, n]\)</span>, <span class="math notranslate nohighlight">\(p_i = P\)</span>, namely the distribution of the minority group is uniform over units.
It is maximum when for all <span class="math notranslate nohighlight">\(i \in [1, n]\)</span>, either <span class="math notranslate nohighlight">\(p_i = 1\)</span> or <span class="math notranslate nohighlight">\(p_i = 0\)</span>, namely every unit includes members of only one group (complete segregation).</p>
<p>The second widely adopted index is the <em>information index</em>, also known
as the <em>Theil index</em> in social sciences <span id="id14">[<a class="reference internal" href="#id2635">16</a>]</span> and normalized
mutual information in machine learning <span id="id15">[<a class="reference internal" href="#id2636">17</a>]</span>.
Let the population entropy be <span class="math notranslate nohighlight">\(E = -
P \cdot \log{P}-(1-P) \cdot \log{(1-P)}\)</span>, and the entropy of unit <span class="math notranslate nohighlight">\(i\)</span> be
<span class="math notranslate nohighlight">\(E_i = - p_i \cdot \log{p_i}-(1-p_i) \cdot \log{(1-p_i)}\)</span>. The
information index is the weighted mean fractional deviation of every
unit’s entropy from the population entropy: <br>
<span class="math notranslate nohighlight">\(H = \sum_{i=1}^n \frac{t_i}{T} \cdot \frac{(E-E_i)}{E}\)</span> <br>
Information index ranges in <span class="math notranslate nohighlight">\([0, 1]\)</span>. Since it denotes a relative reduction in uncertainty in the distribution of groups after considering units, higher values mean higher segregation of groups over the units. Information index reaches the minimum when all the units respect the global entropy (full integration), and the maximum when every unit contains only one group (complete segregation).</p>
<p>The third evenness measure is the <em>Gini index</em>, defined as the mean absolute difference between minority proportions weighted across all pairs of units, and normalized to the maximum weighted mean difference.
In formula: <br>
<span class="math notranslate nohighlight">\(\label{eq:Gini}
G = \frac{1}{2 \cdot T^2 \cdot P \cdot (1-P)} \cdot \sum_{i=1}^n
\sum_{j=1}^n t_i \cdot t_j \cdot |p_i - p_j|\)</span> <br>
Here
<span class="math notranslate nohighlight">\(\sum_{i=1}^n \sum_{j=1}^n t_i \cdot t_j \cdot |p_i - p_j|\)</span> is the
weighted mean absolute difference. The normalization factor is obtained by maximizing such a value.
The definition of the Gini index stems from econometrics, where it is used as a measure of the inequality of income distribution <span id="id16">[<a class="reference internal" href="#id2637">18</a>]</span>. The Gini index ranges in <span class="math notranslate nohighlight">\([0, 1]\)</span>
with higher values denoting higher segregation. The maximum and minimum values are reached in the same cases of the dissimilarity index.</p>
<p><em>Exposure indexes.</em> Exposure indexes measure the degree of potential contact, or possibility of interaction, between members of social groups.
The most used measure of exposure is the <em>isolation index</em> <span id="id17">[<a class="reference internal" href="#id2639">19</a>]</span>, defined as the likelihood that a member of the minority group is exposed to another member of the same group in a unit.
For a unit <span class="math notranslate nohighlight">\(i\)</span>, this can be estimated as the product of the likelihood that a member of the minority group is in the unit (<span class="math notranslate nohighlight">\(m_i/M\)</span>) by the likelihood that she is exposed to another minority member in the unit (<span class="math notranslate nohighlight">\(m_i/t_i\)</span>, or <span class="math notranslate nohighlight">\(p_i\)</span>) – assuming that the two events are independent.
In formula: <br>
<span class="math notranslate nohighlight">\(I = \frac{1}{M} \cdot \sum_{i=1}^n m_i \cdot p_i\)</span></p>
<p>The right hand-side formula can be read as the minority-weighted average
of minority proportions in units. The isolation index ranges over
<span class="math notranslate nohighlight">\([P, 1]\)</span>, with higher values denoting higher segregation. The minimum
value is reached when for <span class="math notranslate nohighlight">\(i \in [1, n]\)</span>, <span class="math notranslate nohighlight">\(p_i = P\)</span>, namely the
distribution of the minority group is uniform over the units. The
maximum value is reached when there is only one <span class="math notranslate nohighlight">\(k \in [1, n]\)</span> such that
<span class="math notranslate nohighlight">\(m_k = t_k = M\)</span>, namely there is a unit containing all minority members
and no majority member.</p>
<p>A dual measure is the <em>interaction index</em>, which is the likelihood that a member of the minority group is exposed to a member of the majority group in a unit. By reasoning as above, this leads to the formula: <br>
<span class="math notranslate nohighlight">\(\mathit{Int} = \frac{1}{M} \cdot \sum_{i=1}^n m_i \cdot (1-p_i)\)</span> <br>
It clearly holds that <span class="math notranslate nohighlight">\(I + \mathit{Int} = 1\)</span>. Hence, lower values denote higher segregation. A more general definition of interaction index occurs when more than two groups are considered in the analysis, so that the exposure of the minority group to one of the other groups is worth to be considered <span id="id18">[<a class="reference internal" href="#id2633">14</a>]</span>.</p>
<p>The key problem of assessing social segregation has been investigated by hypothesis testing, i.e., by formulating one or more possible contexts of segregation against a certain social group, and then in empirically testing such hypotheses.
Such an approach is currently supported by statistical tools, such as the R packages <em>OasisR</em><a class="footnote-reference brackets" href="#oasis" id="id19">1</a> and <em>seg</em><a class="footnote-reference brackets" href="#seg" id="id20">2</a>
<span id="id21">[<a class="reference internal" href="#id2641">20</a>]</span>, or by GIS tools such as the <em>Geo-Segregation Analyzer</em><a class="footnote-reference brackets" href="#geo-seg" id="id22">3</a> <span id="id23">[<a class="reference internal" href="#id2640">21</a>]</span>.
A tool for multidimensional exploration of segregation index has been proposed<a class="footnote-reference brackets" href="#multi-seg" id="id24">4</a> in <span id="id25">[<a class="reference internal" href="#id2642">22</a>]</span>.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id26"><dl class="citation">
<dt class="label" id="id2618"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Douglas S Massey. Segregation and the perpetuation of disadvantage. In <em>The Oxford Handbook of the Social Science of Poverty</em>, pages 369–393. Oxford University Press, 2016.</p>
</dd>
<dt class="label" id="id2619"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Eric Fischer. Distribution of race and ethnicity in US major cities. 2011. under Creative Commons licence, CC BY-SA 2.0. URL: <a class="reference external" href="http://www.flickr.com/photos/walkingsf/sets/72157624812674967/detail/">http://www.flickr.com/photos/walkingsf/sets/72157624812674967/detail/</a>.</p>
</dd>
<dt class="label" id="id2620"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Nancy A Denton and Douglas S Massey. Residential segregation of Blacks, Hispanics, and Asians by socioeconomic status and generation. <em>Social Science Quarterly</em>, 69(4):797–817, 1988.</p>
</dd>
<dt class="label" id="id2621"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Thomas C Schelling. Dynamic models of segregation. <em>Journal of Mathematical Sociology</em>, 1(2):143–186, 1971.</p>
</dd>
<dt class="label" id="id2622"><span class="brackets"><a class="fn-backref" href="#id4">5</a></span></dt>
<dd><p>W. A. V. Clark. Residential preferences and neighborhood racial segregation: a test of the Schelling segregation model. <em>Demography</em>, 28(1):1–19, 1991.</p>
</dd>
<dt class="label" id="id2623"><span class="brackets"><a class="fn-backref" href="#id5">6</a></span></dt>
<dd><p>Douglas S. Massey, Jonathan Rothwell, and Thurston Domina. The changing bases of segregation in the United States. <em>Annals of the American Academy of Political and Social Science</em>, 626:74–90, 2009.</p>
</dd>
<dt class="label" id="id2624"><span class="brackets"><a class="fn-backref" href="#id6">7</a></span></dt>
<dd><p>Matthew Gentzkow and Jesse M Shapiro. Ideological segregation online and offline. <em>Quarterly Journal of Economics</em>, 126(4):1799–1839, 2011.</p>
</dd>
<dt class="label" id="id2625"><span class="brackets"><a class="fn-backref" href="#id7">8</a></span></dt>
<dd><p>Seth Flaxman, Sharad Goel, and Justin M Rao. Filter bubbles, echo chambers, and online news consumption. <em>Public Opinion Quarterly</em>, 80:298–320, 2016. URL: <a class="reference external" href="http://ssrn.com/abstract=2363701">http://ssrn.com/abstract=2363701</a>.</p>
</dd>
<dt class="label" id="id2626"><span class="brackets"><a class="fn-backref" href="#id8">9</a></span></dt>
<dd><p>Michael Maes and Lukas Bischofberger. Will the personalization of online social networks foster opinion polarization? 2015. URL: <a class="reference external" href="http://ssrn.com/abstract=2553436">http://ssrn.com/abstract=2553436</a>.</p>
</dd>
<dt class="label" id="id2634"><span class="brackets"><a class="fn-backref" href="#id9">10</a></span></dt>
<dd><p>Alessandro Baroni and Salvatore Ruggieri. Segregation discovery in a social network of companies. <em>J. Intell. Inf. Syst.</em>, 51(1):71–96, 2018.</p>
</dd>
<dt class="label" id="id2643"><span class="brackets"><a class="fn-backref" href="#id10">11</a></span></dt>
<dd><p>Jiantao Hu, Qian-Ming Zhang, and Tao Zhou. Segregation in religion networks. <em>EPJ Data Sci.</em>, 8(1):6:1–6:11, 2019.</p>
</dd>
<dt class="label" id="id2631"><span class="brackets"><a class="fn-backref" href="#id11">12</a></span></dt>
<dd><p>Otis Dudley Duncan and Beverly Duncan. A methodological analysis of segregation indexes. <em>American Sociological Review</em>, 20(2):210–217, 1955.</p>
</dd>
<dt class="label" id="id2632"><span class="brackets"><a class="fn-backref" href="#id11">13</a></span></dt>
<dd><p>D. R. James and K. E. Tauber. Measures of segregation. <em>Sociological Methodology</em>, 13:1–32, 1985.</p>
</dd>
<dt class="label" id="id2633"><span class="brackets">14</span><span class="fn-backref">(<a href="#id12">1</a>,<a href="#id18">2</a>)</span></dt>
<dd><p>Douglas S Massey and Nancy A Denton. The dimensions of residential segregation. <em>Social Forces</em>, 67(2):281–315, 1988.</p>
</dd>
<dt class="label" id="id2644"><span class="brackets"><a class="fn-backref" href="#id13">15</a></span></dt>
<dd><p>Michal Bojanowski and Rense Corten. Measuring segregation in social networks. <em>Soc. Networks</em>, 39:14–32, 2014.</p>
</dd>
<dt class="label" id="id2635"><span class="brackets"><a class="fn-backref" href="#id14">16</a></span></dt>
<dd><p>R. Mora and J. Ruiz-Castillo. Entropy-based segregation indices. <em>Sociological Methodology</em>, 41:159–194, 2011.</p>
</dd>
<dt class="label" id="id2636"><span class="brackets"><a class="fn-backref" href="#id15">17</a></span></dt>
<dd><p>T. Mitchell. <em>Machine Learning</em>. The Mc-Graw-Hill Companies, Inc., 1997.</p>
</dd>
<dt class="label" id="id2637"><span class="brackets"><a class="fn-backref" href="#id16">18</a></span></dt>
<dd><p>Joseph L Gastwirth. A general definition of the Lorenz curve. <em>Econometrica: Journal of the Econometric Society</em>, 39(6):1037–1039, 1971.</p>
</dd>
<dt class="label" id="id2639"><span class="brackets"><a class="fn-backref" href="#id17">19</a></span></dt>
<dd><p>Wendell Bell. A probability model for the measurement of ecological segregation. <em>Social Forces</em>, 32(4):357–364, 1954.</p>
</dd>
<dt class="label" id="id2641"><span class="brackets"><a class="fn-backref" href="#id21">20</a></span></dt>
<dd><p>Seong-Yun Hong, David O'Sullivan, and Yukio Sadahiro. Implementing spatial segregation measures in R. <em>PLoS ONE</em>, 9(11):e113767, 2014.</p>
</dd>
<dt class="label" id="id2640"><span class="brackets"><a class="fn-backref" href="#id23">21</a></span></dt>
<dd><p>Philippe Apparicio, Joan Carles Martori, Amber L. Pearson, Éric Fournier, and Denis Apparicio. An open-source software for calculating indices of urban residential segregation. <em>Social Science Computer Review</em>, 32(1):117–128, 2014.</p>
</dd>
<dt class="label" id="id2642"><span class="brackets"><a class="fn-backref" href="#id25">22</a></span></dt>
<dd><p>Alessandro Baroni and Salvatore Ruggieri. Scube: A tool for segregation discovery. In <em>EDBT</em>, 542–545. OpenProceedings.org, 2019.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Alessandro Baroni and Salvatore Ruggieri. Segregation discovery in a social network of companies. J. Intell. Inf. Syst., 51(1):71–96, 2018</em> by Salvatore Ruggieri</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="oasis"><span class="brackets"><a class="fn-backref" href="#id19">1</a></span></dt>
<dd><p><a class="reference external" href="https://cran.r-project.org/package=OasisR">cran.r-project.org/package=OasisR</a></p>
</dd>
<dt class="label" id="seg"><span class="brackets"><a class="fn-backref" href="#id20">2</a></span></dt>
<dd><p><a class="reference external" href="https://cran.r-project.org/package=seg">cran.r-project.org/package=seg</a></p>
</dd>
<dt class="label" id="geo-seg"><span class="brackets"><a class="fn-backref" href="#id22">3</a></span></dt>
<dd><p><a class="reference external" href="http://geoseganalyzer.ucs.inrs.ca">geoseganalyzer.ucs.inrs.ca</a></p>
</dd>
<dt class="label" id="multi-seg"><span class="brackets"><a class="fn-backref" href="#id24">4</a></span></dt>
<dd><p><a class="reference external" href="https://github.com/ruggieris/SCube">github.com/ruggieris/SCube</a></p>
</dd>
</dl>
</div>
</div>
</div>
</div>
<span id="document-Accountability/Accountability_and_Reproducibility"></span><div class="tex2jax_ignore mathjax_ignore section" id="accountability-reproducibility-and-traceability">
<h3>Accountability, Reproducibility, and Traceability<a class="headerlink" href="#accountability-reproducibility-and-traceability" title="Permalink to this headline">¶</a></h3>
<div class="section" id="in-brief">
<h4>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h4>
<p><a class="reference internal" href="TAILOR.html#document-Accountability/L2.Accountability"><span class="doc">Accountability</span></a> and <a class="reference internal" href="TAILOR.html#document-Accountability/L2.Reproducibility"><span class="doc">Reproducibility</span></a> are two interrelated concepts, cornerstones of Trustworthy AI. Accoutable AI systems can contribute to reproducibility, and Reproducible AI systems can contribute to accountability.</p>
</div>
<div class="section" id="more-in-detail">
<h4>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h4>
<p><a class="reference internal" href="TAILOR.html#document-Accountability/L2.Accountability"><span class="doc">Accountability</span></a> and <a class="reference internal" href="TAILOR.html#document-Accountability/L2.Reproducibility"><span class="doc">Reproducibility</span></a> are two cornerstones of Trustworthy AI <span id="id1">[<a class="reference internal" href="TAILOR.html#id2755">1</a>]</span>. Accountability requires mechanisms be put in place to ensure that AI systems and their outcomes, both before and after their development, deployment and use, can be observed and analyzed. This ability to review AI systems involve technical and organisational logging processes <span id="id2">[<a class="reference internal" href="TAILOR.html#id2729">2</a>]</span> to enable investigators to draw the same conclusions from an experiment by following provided guidelines.</p>
<p>In this context, <a class="reference internal" href="TAILOR.html#document-Accountability/L2.Accountability"><span class="doc">Accountability</span></a> and <a class="reference internal" href="TAILOR.html#document-Accountability/L2.Reproducibility"><span class="doc">Reproducibility</span></a> are interrelated concepts. Developing reprodubicle AI systems can enable accountability over AI systems. On the other hand, the process of record-tracking and logging for accountability can support an increasing level of reproducibility.</p>
<p>A third dimension strictly correlated with <a class="reference internal" href="TAILOR.html#document-Accountability/L2.Accountability"><span class="doc">Accountability</span></a> and <a class="reference internal" href="TAILOR.html#document-Accountability/L2.Reproducibility"><span class="doc">Reproducibility</span></a> is <a class="reference internal" href="TAILOR.html#document-Accountability/L2.Traceability"><span class="doc">Traceability</span></a>. We suggest to navigate in the appropriate section of this book for more detailed information about these three dimensions.</p>
</div>
<div class="section" id="main-keywords">
<h4>Main Keywords<a class="headerlink" href="#main-keywords" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><a class="reference internal" href="TAILOR.html#document-Accountability/L2.Accountability"><span class="doc">Accountability</span></a>: <strong>Accountability</strong> is an ethical aspect studied in the <a href="https://tailor-network.eu/" target=_blank>TAILOR project</a> to ensure that a given actor or actors can render an account of the actions of an AI system. The accountability concept is strictly related to the concept of responsibility.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Accountability/L3.Problem_of_many_hands"><span class="doc">The Problem of Many Hands</span></a>: The <strong>problem of many hands</strong> aims to understand who is morally responsible in a situation with several actors.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Accountability/L3.The_frame_problem"><span class="doc">The Frame Problem</span></a>: The <strong>frame problem</strong> is the challenge of knowing and modeling the relevant features and context of situations, and getting an agent to act on those without consideration all the irrelevant facts as well.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Accountability/L3.Wicked_problems"><span class="doc">Wicked problems</span></a>: A class of problems for which science provides insufficient or inappropriate resolution.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Accountability/L2.Reproducibility"><span class="doc">Reproducibility</span></a>: <strong>Reproducibility</strong> is the ability of independent investigators to draw the same conclusions from an experiment by following the documentation shared by the original investigators.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Accountability/L2.Traceability"><span class="doc">Traceability</span></a>: <strong>Traceability</strong> can be defined as the need to maintain a complete and clear documentation of the data, processes, artefacts and actors involved in the entire lifecycle of an AI model, starting from its design and ending with its production serving.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Accountability/L3.Provenance_tracking"><span class="doc">Provenance Tracking</span></a>: <strong>Provenance tracking</strong> represents the tracking of “information that describes the production process of an end product, which can be anything from a piece of data to a physical object. […] Essentially, provenance can be seen as meta-data that, instead of describing data, describes a production process.”</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Accountability/L3.Continuous_performance_monitoring"><span class="doc">Continuous Performance Monitoring</span></a>: <strong>Continuous performance monitoring</strong> is the activity to track, log and monitor over time the behaviour and the performance of Artificial Intelligence and Machine Learning models. This activity is particularly relevant after in-production deployment in order to detect any performance drifts and outages of the model.</p></li>
</ul>
</div>
<div class="section" id="bibliography">
<h4>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h4>
<p id="id3"><dl class="citation">
<dt class="label" id="id2753"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>European Commission, Content Directorate-General for Communications Networks, and Technology. <em>Ethics guidelines for trustworthy AI</em>. Publications Office, 2019. URL: <a class="reference external" href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai">https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</a>.</p>
</dd>
<dt class="label" id="id2706"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Jennifer Cobbe, Michelle Seng Ah Lee, and Jatinder Singh. Reviewable automated decision-making: a framework for accountable algorithmic systems. In <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, 598–609. 2021.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Luciano C Siebert.</p>
</div></blockquote>
</div>
<div class="toctree-wrapper compound">
<span id="document-Accountability/L2.Accountability"></span><div class="tex2jax_ignore mathjax_ignore section" id="accountability">
<h4>Accountability<a class="headerlink" href="#accountability" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p><strong>Accountability</strong> is an ethical aspect studied in the <a href="https://tailor-network.eu/" target=_blank>TAILOR project</a> to ensure that a given actor or actors can render an account of the actions of an AI system. The accountability concept is strictly related to the concept of responsibility.</p>
</div>
<div class="section" id="abstract">
<h5>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">¶</a></h5>
<p>According to <span id="id1">[<a class="reference internal" href="TAILOR.html#id21">1</a>]</span>, the requirement of accountability complements the other ethical dimensions, and is closely linked to the principle of <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/Diversity_Non-Discrimination_and_Fairness"><span class="doc std std-doc">fairness</span></a>. It necessitates that mechanisms be put in place to ensure responsibility and accountability for AI systems and their outcomes, both before and after their development, deployment and use.</p>
</div>
<div class="section" id="motivation-and-background">
<h5>Motivation and Background<a class="headerlink" href="#motivation-and-background" title="Permalink to this headline">¶</a></h5>
<p>Whenever something goes wrong, there is often a call to define who is responsible for this wrongdoing. Responsibility is a broader topic that might have different conceptualizations. However, in this sense, it usually means one’s obligation to render an account of your actions and the consequences of these, i.e. accountability. Accountability can be defined as a form of <em>“passive responsibility”</em> (or backward looking responsibility) in the sense of being held to account for or justify towards others a given action or consequence that happened in the past <span id="id2">[<a class="reference internal" href="#id2794">3</a>]</span>. Although accountability implies having to account for one’s actions, if the account given is considered insufficient, then one might still be considered blameworthy and thus deserving of censure or blame <span id="id3">[<a class="reference internal" href="#id2793">4</a>]</span>.</p>
<p>AI systems bring particular concerns with respect to accountability, as understanding how the systems work can be challenging, and commercial considerations can conceal broader organisations processes <span id="id4">[<a class="reference internal" href="#id2729">2</a>]</span>. Although one might “understand” the inner workings of the algorithms used, the outcomes might still not be predictable, complicating accountability even further <span id="id5">[<a class="reference internal" href="#id2791">5</a>]</span>.</p>
<p>Two often discussed examples to explain the accountability setting are related to autonomous driving cars and medical decisions.</p>
<p>Indeed, imagine a self-driving car that hits a pedestrian. Who should account for or justify the system’s actions? The person within the car that might not have been able or willing to supervise the system? The manufacturer, that designed the systems and thus should be the only responsible of the behaviour of its products? The programmer that did not correctly implement all the necessary checks? The manufacture of the sensor that did not detect the pedestrian? The person who conducted the test that did not foresee that particular circumstance?</p>
<p>Or, again, in the case of a wrong diagnosis following an MRI. Do we expect an account from the doctor that did not see the error or the AI system (at all the same possible levels we saw in the previous example)?</p>
<p>Given the difficulties, a lot of effort put in the definition of accountability regards the <strong>Auditability</strong> principle: <span id="id6">[<a class="reference internal" href="TAILOR.html#id21">1</a>]</span></p>
<blockquote>
<div><p>Auditability entails the enablement of the assessment of algorithms, data and design processes. This does not necessarily imply that information about business models and intellectual property related to the AI system must always be openly available. Evaluation by internal and external auditors, and the availability of such evaluation reports, can contribute to the trustworthiness of the technology. In applications affecting fundamental rights, including safety-critical applications, AI systems should be able to be independently audited.</p>
</div></blockquote>
<p>Other aspects took into consideration in the High Level Expert Group report <span id="id7">[<a class="reference internal" href="TAILOR.html#id21">1</a>]</span> are:</p>
<ul class="simple">
<li><p><strong>Minimisation and reporting of negative impacts</strong>, i.e., assessing, documenting and minimising the potential negative impacts of AI systems, even thanks to the use of impact assessments both prior to and during the development, deployment and use of AI systems.</p></li>
<li><p><strong>Trade-offs</strong> to tackle tensions that may arise between requirements. If conflict arises, trade-offs should be explicitly acknowledged and evaluated in terms of their risk to ethical principles, including fundamental rights.  Any decision about which trade-off to make should be reasoned and properly documented. Whether no ethically acceptable trade-offs can be identified, <span id="id8">[<a class="reference internal" href="TAILOR.html#id21">1</a>]</span> clearly states that the development, deployment and use of the AI system should not proceed in that form.</p></li>
<li><p><strong>Redress</strong> must be ensured when things go wrong, with particular attention to vulnerable persons or groups. The importance of the redress is advocated also by the European Union Agency for Fundamental Rights <span id="id9">[<a class="reference internal" href="#id2795">6</a>]</span>, where particular emphasis is posed to collective redress (i.e., collective redress, a way in which victims can join forces to overcome obstacles), and by the Council of Europe <span id="id10">[<a class="reference internal" href="#id2798">7</a>]</span>, where is specified that a citizen should not necessarily have to pursue legal action straight away and seeking remedies should be available, known, accessible, affordable and capable of providing appropriate redress.</p></li>
</ul>
</div>
<div class="section" id="guidelines">
<h5>Guidelines<a class="headerlink" href="#guidelines" title="Permalink to this headline">¶</a></h5>
<p>Several guidelines and checklists have been proposed to increase accountability over the actions of AI systems, both from EU authorities and industries, such as:</p>
<ul class="simple">
<li><p><strong>The Assessment List for Trustworthy Artificial Intelligence (ALTAI)</strong> <span id="id11">[<a class="reference internal" href="#id2799">8</a>]</span> <br>
This Assessment List (ALTAI) is firmly grounded in the protection of people’s fundamental rights exposed by the High Level Expert Group report <span id="id12">[<a class="reference internal" href="TAILOR.html#id21">1</a>]</span>. It is probably the most complete one so far and the reference point for all other checklists. <br>
The ALTAI checklist helps organisations understand what Trustworthy AI is, in particular what risks an AI system might generate, and how to minimize those risks while maximising the benefit of AI. It is intended to help organisations identify how proposed AI systems might generate risks, and to identify whether and what kind of active measures may need to be taken to avoid and minimise those risks. It aims at raising awareness of the potential impact of AI on society, the environment, consumers, workers and citizens (in particular children and people belonging to marginalised groups) and at encouraging the multidisciplinarity and the involvement of all relevant stakeholders. It helps to gain insight on whether meaningful and appropriate solutions or processes to accomplish adherence to the seven requirements (as outlined above) are already in place or need to be put in place. This could be achieved through internal guidelines, governance processes, etc. <br>
For each requirement, this Assessment List for Trustworthy AI (ALTAI) provides introductory guidance and relevant definitions in the Glossary. The <a href="https://futurium.ec.europa.eu/en/european-ai-alliance/pages/altai-assessment-list-trustworthy-artificial-intelligence" target=_blank>online version</a> of this assessment list contains additional explanatory notes for many of the questions.</p></li>
<li><p><strong>Getting the Future Right</strong> <span id="id13">[<a class="reference internal" href="#id2801">9</a>]</span><br>
The European Union Agency for Fundamental Rights published a report where a fundamental rights-based analysis of concrete ‘use cases’ is provided. The report illustrates some of the ways that companies and the public sector in the EU are looking to use AI to support their work, and whether – and how – they are taking fundamental rights considerations into account. In this way, it contributes empirical evidence, analysed from a fundamental rights perspective, that can inform EU and national policymaking efforts to regulate the use of AI tools.</p></li>
<li><p><strong>ICO’s guidance on the use of artificial intelligence</strong> <span id="id14">[<a class="reference internal" href="#id2796">10</a>, <a class="reference internal" href="#id2797">11</a>]</span> <br>
The UK data protection authority has been very active on all the topics related to accountability, publishing guidance that is constantly updated. In <span id="id15">[<a class="reference internal" href="#id2796">10</a>]</span>, the focus of accountability is on being compliant with data protection law and being capable of minimise risks. It explores some important aspects such as Leadership and oversight (e.g., the structure of the analyzed organization), <a class="reference internal" href="TAILOR.html#document-Transparency/Transparency"><span class="doc std std-doc">Transparency</span></a>, <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/Privacy_and_Data_Governance"><span class="doc std std-doc">Privacy</span></a>, and <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/Technical_Robustness_and_Safety"><span class="doc std std-doc">Security</span></a>; a whole section is dedicated to the Data Protection Impact Assessment.<br>
In <span id="id16">[<a class="reference internal" href="#id2797">11</a>]</span>, a short checklist is presented, even if in the document itself is highlighted that “Accountability is not a box-ticking exercise”  but rather taking responsibility for what you are doing with personal data, considering this as an opportunity to develop and sustain people’s trust.</p></li>
<li><p><strong>IBM’s FactSheets</strong> <span id="id17">[<a class="reference internal" href="#id2800">12</a>]</span><br>
This document starts from Supplier’s Declarations of Conformity (SDoCs), which are documents largely used by many industries even if they are usually not legally required documents, to describe the lineage of a product along with the safety and performance testing it has undergone. SDoCs aims at capturing and quantifying various aspects of the product and its development to make it worthy of consumers’ trust. The chechlist proposed in <span id="id18">[<a class="reference internal" href="#id2800">12</a>]</span> should help increasing trust in AI services. We envision such documents to contain purpose, performance, safety, security, and provenance information to be completed by AI service providers for examination by consumers.<br>
A FactSheet will contain sections on all relevant attributes of an AI service, such as intended use, performance (including appropriate accuracy or risk measures along with timing information), safety, explainability, algorithmic fairness, and security and robustness. Moreover, the FactSheet should help in listing how the service was created, trained, and deployed along with what scenarios it was tested on, how it may respond to untested scenarios, guidelines that specify what tasks it should and should not be used for, and any ethical concerns of its use. Hence, FactSheets help prevent overgeneralization and unintended use of AI services by solidly grounding them with metrics and usage scenarios.
FactSheet is a quite interesting example because in this case a private company highlights the need of ethical procedures, standards, and certifications.</p></li>
<li><p><strong>Microsoft’s guideline for human-AI interaction</strong> <span id="id19">[<a class="reference internal" href="#id2802">13</a>]</span> <br>
In this paper, authors identified 18 question, related to different phases of the use of an AI system, and 10 different kinds of application, ranging from e-commerce recommender systems to route planning  systems, from automatic photo organizers to social network feed filtering systems. Then, authors empirically evaluated both the clarity and the relevance of various questions in the various domains, highlighting potential criticality (e.g., reporting a violation to the question ``Make clear why the system did what it did’’ if a recommender system did not non give any explanations of the reason why a certain product was suggested).</p></li>
</ul>
</div>
<div class="section" id="possible-taxonomy-of-terms">
<h5>Possible Taxonomy of terms<a class="headerlink" href="#possible-taxonomy-of-terms" title="Permalink to this headline">¶</a></h5>
<p>Boven <span id="id20">[<a class="reference internal" href="#id2792">14</a>]</span> defines accountability as</p>
<blockquote>
<div><p>a <strong>relationship</strong> between an <strong>actor</strong> and a <strong>forum</strong>, in which the actor has an obligation to <strong>explain and to justify</strong> his or her conduct, the forum can pose questions and pass judgement, and the actor may face <strong>consequences</strong>.</p>
</div></blockquote>
<p>Wiering <span id="id21">[<a class="reference internal" href="#id2790">15</a>]</span> presented a thorough systematic literature review on algorithmic accountability structured on the five points identified by Boven in his definition, which we briefly summarize below:</p>
<ul class="simple">
<li><p><strong>Arguments on the actor</strong>: Involves a broader discussion on who is responsible for the harm that the system may inflict
when it is working correctly, and who is responsible when it is
working incorrectly. It involves different levels of actors (e.g. individuals, teams, department, organizations) with different roles and possibly also third-parties. Due to these multiple levels and actors, situations known as ``the problem of many hands’’ might occur, in which the
collective can reasonably be held responsible for an outcome, while none
of the individuals can be reasonably held responsible for that outcome <span id="id22">[<a class="reference internal" href="#id2794">3</a>]</span>. In these situations, to be “in the loop” is not enough, calling for a more meaningful ability to control the design and operation process, in other words calling for meaningful human control. <!-- TODO: add link {doc}`L3.meaningful_human_control` --></p></li>
<li><p><strong>The forum</strong>: To whom a given account is directed. The forum might take different shapes such as political, legal, administrative, professional, and towards the civil society. Examples of forum include General Data Protection Regulation (GDPR), the proposed EU AI Act, or even the guidelines for trustworthy AI proposed by the European Commission <span id="id23">[<a class="reference internal" href="TAILOR.html#id21">1</a>]</span>, as discussed in the previous section.</p></li>
<li><p><strong>The relationship between the actor and the forum</strong>: This relationship comes in different forms and shapes, according to all other four points. Nevertheless, they are usually mapped in three phases: the information phase, the deliberation and discussion phase, and the final phase, where consequences can be imposed on the actor by the forum.</p></li>
<li><p><strong>The content and criteria of the account</strong>: Although ex ante analysis, such as impact assessment and simulations, can be helpful, they are limited as they cannot foresee all possible behavior and consequences. The importance of an accountability relationship should depend not only on such ex ante factors, but also on the extent to which a given system impacts society and individuals.</p></li>
<li><p><strong>The consequences which may result from the account</strong>: In situations where there is a more “vertical” accountability relationship between actor and forum (e.g., accountability through legal standers), consequences are usually made more tangible. In more “horizontal” settings (e.g., self-regulation of organizations), consequences are defined based more on a moral imperative.</p></li>
</ul>
</div>
<div class="section" id="accountability-gaps-and-their-implications">
<h5>Accountability gaps and their implications<a class="headerlink" href="#accountability-gaps-and-their-implications" title="Permalink to this headline">¶</a></h5>
<p>As AI systems, especially systems with learning abilities, are deployed “in the wild”, human control and prediction over their behaviour are very difficult if not impossible, leading to so-called <em>“accountability gaps”</em>. Santoni de Sio &amp; Mecacci <span id="id24">[<a class="reference internal" href="#id2789">16</a>]</span> divided such gaps in public accountability gap and moral accountability gaps. <em>Public accountability gaps</em> relate to citizens not being able to get an explanation for decisions taken by public agencies, while <em>moral accountability gap</em> refers to the reduction of human agents’ capacity to make sense of – and explain to each other the behaviour of AI systems.</p>
<p>Accountability gaps point to the fact that accounting requires knowledge and some ability to control <span id="id25">[<a class="reference internal" href="#id2789">16</a>]</span>. Designers and developers of AI systems can only tackle this challenge by acknowledging that this is not a matter of fortuitous allocation of praise or blame, and that systems should be developed in a manner that allows for stakeholders to be held accountable. Among other things, this relates to the social context where these systems are deployed (and whether a given solution or formulation can be argued), how the questions and criteria for accountability are framed, and the level of understanding and control that a given actor might have. In this encyclopedia, we discuss these three interrelated concepts, respectively, in the entries <a class="reference internal" href="TAILOR.html#document-Accountability/L3.Wicked_problems"><span class="doc">Wicked problems</span></a>, <a class="reference internal" href="TAILOR.html#document-Accountability/L3.The_frame_problem"><span class="doc">The Frame Problem</span></a>, and <a class="reference internal" href="TAILOR.html#document-Human_Agency_and_Oversight/Meaningful_human_control"><span class="doc">Meaningful human control</span></a>.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id26"><dl class="citation">
<dt class="label" id="id84"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id6">2</a>,<a href="#id7">3</a>,<a href="#id8">4</a>,<a href="#id12">5</a>,<a href="#id23">6</a>)</span></dt>
<dd><p>High-Level Expert Group on Artificial Intelligence. Ethics Guidelines for Trustworthy AI. 2019. URL: <a class="reference external" href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai">https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</a> (visited on 2022-02-16).</p>
</dd>
<dt class="label" id="id2729"><span class="brackets"><a class="fn-backref" href="#id4">2</a></span></dt>
<dd><p>Jennifer Cobbe, Michelle Seng Ah Lee, and Jatinder Singh. Reviewable automated decision-making: a framework for accountable algorithmic systems. In <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, 598–609. 2021.</p>
</dd>
<dt class="label" id="id2794"><span class="brackets">3</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id22">2</a>)</span></dt>
<dd><p>Ibo R Van de Poel and Lambèr MM Royakkers. <em>Ethics, technology, and engineering: An introduction</em>. Wiley-Blackwell, 2011.</p>
</dd>
<dt class="label" id="id2793"><span class="brackets"><a class="fn-backref" href="#id3">4</a></span></dt>
<dd><p>Ibo van de Poel. The relation between forward-looking and backward-looking responsibility. In <em>Moral responsibility</em>, pages 37–52. Springer, 2011.</p>
</dd>
<dt class="label" id="id2791"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>Marijn Janssen and George Kuk. The challenges and limits of big data algorithms in technocratic governance. 2016.</p>
</dd>
<dt class="label" id="id2795"><span class="brackets"><a class="fn-backref" href="#id9">6</a></span></dt>
<dd><p>European Union Agency for Fundamental Rights. Improving access to remedy in the area of business and human rights at the EU leve. 2016. URL: <a class="reference external" href="https://fra.europa.eu/en/opinion/2017/business-human-rights">https://fra.europa.eu/en/opinion/2017/business-human-rights</a> (visited on 2022-05-10).</p>
</dd>
<dt class="label" id="id2798"><span class="brackets"><a class="fn-backref" href="#id10">7</a></span></dt>
<dd><p>Council of Europe. Guide to human rights for internet users: effective remedies and redress. 2014. URL: <a class="reference external" href="http://www.coe.int/en/web/internet-users-rights/guide">http://www.coe.int/en/web/internet-users-rights/guide</a> (visited on 2022-05-10).</p>
</dd>
<dt class="label" id="id2799"><span class="brackets"><a class="fn-backref" href="#id11">8</a></span></dt>
<dd><p>High Level Expert Group on AI. The Assessment List for Trustworthy Artificial Intelligence (ALTAI). 2020. URL: <a class="reference external" href="https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment">https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment</a> (visited on 2022-05-10).</p>
</dd>
<dt class="label" id="id2801"><span class="brackets"><a class="fn-backref" href="#id13">9</a></span></dt>
<dd><p>European Union Agency for Fundamental Rights. Getting the future right - Artificial Intelligence and Fundamental Rights. 2020. URL: <a class="reference external" href="https://fra.europa.eu/en/publication/2020/artificial-intelligence-and-fundamental-rights">https://fra.europa.eu/en/publication/2020/artificial-intelligence-and-fundamental-rights</a> (visited on 2022-05-10).</p>
</dd>
<dt class="label" id="id2796"><span class="brackets">10</span><span class="fn-backref">(<a href="#id14">1</a>,<a href="#id15">2</a>)</span></dt>
<dd><p>Information Commissioner's Office (ICO). Accountability framework. URL: <a class="reference external" href="https://ico.org.uk/for-organisations/accountability-framework/">https://ico.org.uk/for-organisations/accountability-framework/</a> (visited on 2022-05-25).</p>
</dd>
<dt class="label" id="id2797"><span class="brackets">11</span><span class="fn-backref">(<a href="#id14">1</a>,<a href="#id16">2</a>)</span></dt>
<dd><p>Information Commissioner's Office (ICO). Accountability and governance. URL: <a class="reference external" href="https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/accountability-and-governance/">https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/accountability-and-governance/</a> (visited on 2022-05-25).</p>
</dd>
<dt class="label" id="id2800"><span class="brackets">12</span><span class="fn-backref">(<a href="#id17">1</a>,<a href="#id18">2</a>)</span></dt>
<dd><p>Matthew Arnold, Rachel K. E. Bellamy, Michael Hind, Stephanie Houde, Sameep Mehta, Aleksandra Mojsilovic, Ravi Nair, Karthikeyan Natesan Ramamurthy, Darrell Reimer, Alexandra Olteanu, David Piorkowski, Jason Tsay, and Kush R. Varshney. FactSheets: Increasing Trust in AI Services through Supplier's Declarations of Conformity. 2019. arXiv:1808.07261v2.</p>
</dd>
<dt class="label" id="id2802"><span class="brackets"><a class="fn-backref" href="#id19">13</a></span></dt>
<dd><p>Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N. Bennett, Kori Inkpen, Jaime Teevan, Ruth Kikin-Gil, and Eric Horvitz. Guidelines for human-ai interaction. In <em>ACM International Conference of Human-Computer Interaction (CHI)</em>. 2019.</p>
</dd>
<dt class="label" id="id2792"><span class="brackets"><a class="fn-backref" href="#id20">14</a></span></dt>
<dd><p>Mark Bovens. Analysing and assessing accountability: a conceptual framework 1. <em>European law journal</em>, 13(4):447–468, 2007.</p>
</dd>
<dt class="label" id="id2790"><span class="brackets"><a class="fn-backref" href="#id21">15</a></span></dt>
<dd><p>Maranke Wieringa. What to account for when accounting for algorithms: a systematic literature review on algorithmic accountability. In <em>Proceedings of the 2020 conference on fairness, accountability, and transparency</em>, 1–18. 2020.</p>
</dd>
<dt class="label" id="id2789"><span class="brackets">16</span><span class="fn-backref">(<a href="#id24">1</a>,<a href="#id25">2</a>)</span></dt>
<dd><p>Filippo Santoni de Sio and Giulio Mecacci. Four responsibility gaps with artificial intelligence: why they matter and how to address them. <em>Philosophy &amp; Technology</em>, pages 1–28, 2021.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Luciano C Siebert and Francesca Pratesi.</p>
</div></blockquote>
</div>
<div class="toctree-wrapper compound">
<span id="document-Accountability/L3.Wicked_problems"></span><div class="tex2jax_ignore mathjax_ignore section" id="wicked-problems">
<h5>Wicked problems<a class="headerlink" href="#wicked-problems" title="Permalink to this headline">¶</a></h5>
<div class="section" id="in-brief">
<h6>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p>A class of problems for which science provides insufficient or inappropriate resolution <span id="id1">[<a class="reference internal" href="#id2713">1</a>]</span>.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>Wicked problems are not objectively given, but their formulation already depends on the viewpoint of those presenting them <span id="id2">[<a class="reference internal" href="#id2711">2</a>]</span>.</p>
<p>In spatial planning literature, there is a difference between tame problems and wicked problems. The former is a problem with a set of well-defined rules and clear goal, e.g. problems like solving sudoku’s. There are however another set of problems that do not do well when we think of them in terms of search spaces, constraints, rules, and goal settings.</p>
<p>This class of problems, named wicked problems <span id="id3">[<a class="reference internal" href="#id2713">1</a>]</span> is largely determined by the professional skill of framing and addressing the problem in a particular way. These problems are political like policy around poverty. The setting and solutions are contingent, depending on political view, available information, and dependent on formulation. There are ten different markers that show wickedness <span id="id4">[<a class="reference internal" href="#id2713">1</a>]</span>.</p>
<ol class="simple">
<li><p>There is no definite formulation of a wicked problem.</p></li>
<li><p>Wicked problems have no stopping rule.</p></li>
<li><p>Solutions to wicked problems are not true-or-false, but good-or-bad.</p></li>
<li><p>There is no immediate test of a solution to a wicked problem.</p></li>
<li><p>Every solution to a wicked problem is a ’one-shot operation’;
because there is no opportunity to learn by trial-and-error, every attempt counts significantly.</p></li>
<li><p>Wicked problems do not have an enumerable set of potential solutions,
nor is there a well-described set of permissible operations that may be incorporated into the plan.</p></li>
<li><p>Every wicked problem is essentially unique.</p></li>
<li><p>Every wicked problem can be considered to be a symptom of another problem.</p></li>
<li><p>The existence of a discrepancy representing a wicked problem can be explained in numerous ways. The choice of explanation determines the nature of the problem’s resolution.</p></li>
<li><p>The planner has no right to be wrong</p></li>
</ol>
<p>Spatial planning may be far removed at first sight from engineering, but with the embedding of technology in society, we do move towards a society where tame problems with well-defined rules fall short with regard to the potential impact implementations may have in society. In fact, many algorithms can already be regarded as policy in one way or another, as a particular implementation may provide benefits to a certain train of thought or a particular set of actions. A policy that gives financial benefits to those in dire straits (e.g. those below the poverty line) can have a similar effect as an algorithm that allocates resources to those in need. Point being, we should not underestimate the similarity between administration and algorithm.</p>
<p>If we take the ten different markers for algorithmic implementation into account, then we can draw a number of inferences. First, it shows us the immediate impact of implementation, not only is implementation a one-shot operation because it may skew public perception<span id="id5">[<a class="reference internal" href="#id2714">3</a>]</span>, it also may influence other potential solutions. Second, these solutions are political and not to be framed in terms of optimization (true/false and good/bad). Third, there are a variety of equally effective solutions that would be permissible, meaning that the choice for this particular one carries a certain political or personal weight.</p>
<p>The first point latches onto something else in planning theory, that of path dependency. When an implementation becomes embedded in society it is hard to remove<span id="id6">[<a class="reference internal" href="#id2715">4</a>]</span>. We also see this in philosophy technology through the Collingride dilemma<span id="id7">[<a class="reference internal" href="#id2716">5</a>]</span>. In planning theory it shows that implementation may effect future implementations as it opens certain doors and closes others. Consider for example, our use of the QWERTY keyboard, this is partly due to its widespread use, rather than efficiency (DVORAK is more effective) <span id="id8">[<a class="reference internal" href="#id2717">6</a>]</span>. The claim of path dependency is that these implementations (of which many others would be equivalent) can cause a path that is hard to step away from.</p>
<p>This last inference is one that engineers should take into account when designing algorithms for a societal context. It means that they themselves become a political player in the scheme of things, rather than the executioner of the wishes of certain stake-holders. In essence, the role of the engineers and that of a policy designer are interlinked by  their societal impact. Wicked problems are a way of showing the impact they have when dealing with bureaucracy, algorithmic or otherwise. Accountability comes into play when we consider that the engineer is not a neutral player within this game, they carry some of the blame of the outcome as it was their framing of the problem that led to a particular solution. Of course, there are many ways to alleviate some of these problems</p>
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p id="id9"><dl class="citation">
<dt class="label" id="id2713"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id3">2</a>,<a href="#id4">3</a>)</span></dt>
<dd><p>Horst WJ Rittel and Melvin M Webber. Dilemmas in a general theory of planning. <em>Policy sciences</em>, 4(2):155–169, 1973.</p>
</dd>
<dt class="label" id="id2711"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Richard Coyne. Wicked problems revisited. <em>Design studies</em>, 26(1):5–17, 2005.</p>
</dd>
<dt class="label" id="id2714"><span class="brackets"><a class="fn-backref" href="#id5">3</a></span></dt>
<dd><p>Bas Verplanken. Beliefs, attitudes, and intentions toward nuclear energy before and after chernobyl in a longitudinal within-subjects design. <em>Environment and Behavior</em>, 21(4):371–392, 1989.</p>
</dd>
<dt class="label" id="id2715"><span class="brackets"><a class="fn-backref" href="#id6">4</a></span></dt>
<dd><p>Richard J Lazarus. Super wicked problems and climate change: restraining the present to liberate the future. <em>Cornell L. Rev.</em>, 94:1153, 2008.</p>
</dd>
<dt class="label" id="id2716"><span class="brackets"><a class="fn-backref" href="#id7">5</a></span></dt>
<dd><p>David Collingridge. <em>The social control of technology</em>. St. Martin's Press, New York, 1982.</p>
</dd>
<dt class="label" id="id2717"><span class="brackets"><a class="fn-backref" href="#id8">6</a></span></dt>
<dd><p>Paul A David. Clio and the economics of qwerty. <em>The American economic review</em>, 75(2):332–337, 1985.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Sietze Kuilman.</p>
</div></blockquote>
</div>
</div>
<span id="document-Accountability/L3.The_frame_problem"></span><div class="tex2jax_ignore mathjax_ignore section" id="the-frame-problem">
<h5>The Frame Problem<a class="headerlink" href="#the-frame-problem" title="Permalink to this headline">¶</a></h5>
<div class="section" id="in-brief">
<h6>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p>The <strong>frame problem</strong> is the challenge of knowing and modelling the relevant features and context of situations, and getting an agent to act on those without considering all the irrelevant facts as well.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>The frame problem originated with McCarthy and Hayes<span id="id1">[<a class="reference internal" href="#id2749">1</a>]</span> back in the sixties, but it has been appropriated many times over. The frame problem, as described by McCarthy and Hayes, was mostly about representationalism. They wondered how we could describe an update function such that it does not require a multitude of unnecessary and unaffected statements. It was and is a poignant question. We could have an agent that is able of acting on certain parts of the world, say paint pieces of paper <span id="id2">[<a class="reference internal" href="#id2750">2</a>]</span>, but when we give it other actions, these actions may interact. How does the machine know that the moving said paper won’t also change its colour? In representationalism, it seemed to mean that we had to add a variety of statements that only worked in serious edge-cases.</p>
<p>The frame problem as philosophers appropriated it, was about generalized action. How does an agent keep a faithful representation of the world, after it has acted <span id="id3">[<a class="reference internal" href="#id2751">3</a>]</span>? Such an agent would need to have a kind of update function that does not require going over all the superfluous statements <span id="id4">[<a class="reference internal" href="#id2753">4</a>]</span>. Fodor <span id="id5">[<a class="reference internal" href="#id2754">5</a>]</span> posited it as Hamlet’s problem: How does an agent know when to stop thinking?</p>
<p>The frame problem these days is sometimes regarded as the general relevance problem, not being limited to representationalism but going into connectionism as well <span id="id6">[<a class="reference internal" href="#id2752">6</a>]</span>. In this case, the inheritance of the frame problem for connectionism entails the follow: how does an agent know which data is considered to be relevant to the situation? The problem for connectionists approaches is not that input needs to exert influence on the system, but rather that it produces the correct influence.</p>
<p>This is where one can see problems for accountability on the horizon. In situations where agents have to act, we require that they indeed make the correct inferences given a certain context, have the correct update function, and produce the correct influence. All of these require that the agent understands the context at hand and is able to make the correct inferences such that the relevant action is achieved.</p>
<p>However, that is easier said than done. The task of the agent’s designer is finding a way that all these relevant inferences can be incorporated into the agent. If they don’t, then it introduces a gap. The model of the agent, their capacity for action, and the world will result in an agent that acts while missing (relevant) inferences. This can end up harming people or misaligning with human intention. The obvious examples are those of harmful classifications - Google for example had a problem with the classification of Gorilla’s<span id="id7">[<a class="reference internal" href="#id2755">7</a>]</span>.</p>
<p>The question of the frame problem for accountability is not how to solve the frame problem, as that seems to require solving a variety of tractability question and understanding the relation between agent and the world such that relevancy can be aptly captured. Rather, the designer should be aware of its own limitation and the limitations of the model that is housed within the agent. Meaning that accountability has a social aspect of explaining the necessary limits of the system or boxing possible actions of the agents such that these limits are acceptable in their scope.</p>
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p id="id8"><dl class="citation">
<dt class="label" id="id2749"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>John McCarthy and Patrick J Hayes. Some philosophical problems from the standpoint of artificial intelligence. In <em>Readings in artificial intelligence</em>, pages 431–450. Elsevier, 1981.</p>
</dd>
<dt class="label" id="id2750"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Murray Shanahan. The Frame Problem. In Edward N. Zalta, editor, <em>The Stanford Encyclopedia of Philosophy</em>. Metaphysics Research Lab, Stanford University, Spring 2016 edition, 2016.</p>
</dd>
<dt class="label" id="id2751"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Daniel C Dennett. Cognitive wheels: the frame problem of ai. <em>Minds, machines and evolution</em>, pages 129–151, 1984.</p>
</dd>
<dt class="label" id="id2753"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Drew McDermott. Ai, logic, and the frame problem. In <em>The frame problem in artificial intelligence</em>, pages 105–118. Elsevier, 1987.</p>
</dd>
<dt class="label" id="id2754"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>Jerry A Fodor. Modules, frames, fridgeons, sleeping dogs, and the music of the spheres. In Jay L Garfield, editor, <em>Modularity in Knowledge Representation and Natural-Language Understanding</em>, chapter 1. The MIT Press, Cambridge, 1987.</p>
</dd>
<dt class="label" id="id2752"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p>Richard Samuels. Classical computationalism and the many problems of cognitive relevance. <em>Studies in History and Philosophy of Science Part A</em>, 41(3):280–293, 2010.</p>
</dd>
<dt class="label" id="id2755"><span class="brackets"><a class="fn-backref" href="#id7">7</a></span></dt>
<dd><p>Ludovic Righetti, Raj Madhavan, and Raja Chatila. Unintended consequences of biased robotic and artificial intelligence systems [ethical, legal, and societal issues]. <em>IEEE Robotics &amp; Automation Magazine</em>, 26(3):11–13, 2019.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Sietze Kuilman and Luciano C Siebert.</p>
</div></blockquote>
</div>
</div>
<span id="document-Accountability/L3.Problem_of_many_hands"></span><div class="tex2jax_ignore mathjax_ignore section" id="the-problem-of-many-hands">
<h5>The Problem of Many Hands<a class="headerlink" href="#the-problem-of-many-hands" title="Permalink to this headline">¶</a></h5>
<div class="section" id="in-brief">
<h6>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p>Who is morally responsible in a situation with several actors?</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>When a bridge fails or an algorithm makes a mistake, who is held responsible? Sure in some situations, there may be a clear culprit, but in many situations like these we’ve had several agents involved. In policy making there are often many public officials who carry a part of the responsibility, which can make it difficult even in principle to identify who is morally responsibly <span id="id1">[<a class="reference internal" href="#id40">1</a>]</span>. In complex engineering projects, accidents may occur because of highly complex coincidences that are hard to foresee <span id="id2">[<a class="reference internal" href="#id37">2</a>]</span>. This phenomenon which arises due to complexity and amount of actors, makes it almost impossible to hold someone reasonably responsible is often referred to as the problem of many hands. This has much to do with the fact that there is no locus of decision-making, no single individuals is blameworthy for the harm caused <span id="id3">[<a class="reference internal" href="#id38">3</a>]</span>. This is not to say that there is no more responsibility at all, rather that it is diffuse and that it is hard to identify the appropriate person who ought to make amends for the outcome <span id="id4">[]</span>.</p>
<p>This problem becomes more pressing once we add technology into the mix. The physical and temporal distance introduced by technology means that the connection between actions and events is further blurred <span id="id5">[<a class="reference internal" href="#id39">4</a>]</span>. Technology can propagate one’s action through time and space, being social media a familiar example of this, as it allows us to be vocal towards a far larger crowd than ever before. Before going on about moral responsibility, one should note that the introduction of many different systems, and interaction between systems, could also cause similar problems. The internet-of-things era that we are living in, may for example also cause accidents (conflicts between different systems). The complexity that arises from the interaction between systems may also make it hard to know which particular systems are to blame.</p>
<p>Nonetheless, if we are looking for moral individual responsibility, we need to know what kind of responsibility that is? Roughly speaking one can divide this between backward-looking and forward-looking <span id="id6">[<a class="reference internal" href="#id42">5</a>]</span>. Backward-looking responsibility is a kind of accountability and blameworthiness, as in: this individual can reasonably be blamed for a particular outcome. Forward-looking means an agent can be reasonably expected to prevent a particular state of affairs, it can be seen as an obligation to prevent the harm from happening in the first place. The reason this difference may be of importance is because the difference in responsibility attribution may differ for these two types of responsibilities. Technology for example is built, but also applied and wielded, and during that application it may be used in a context for which it was not appropriate. This may thus complicate the notion of a kind of forward-looking responsibility because it is hard to know under which conditions something is used in the future. Yet, under which condition is it acceptable to hide behind such a statement? In a sense this relates back to backward-looking as a designer may have had influence to prevent that that application in said context in the first place. In a sense, accountability can be approached both through backward-looking and forward-looking, but they imply something different about what our relation to responsibility and accountability ought to be in such considerations.</p>
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p id="id7"><dl class="citation">
<dt class="label" id="id40"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Dennis F Thompson. Moral responsibility of public officials: the problem of many hands. <em>American Political Science Review</em>, 74(4):905–916, 1980.</p>
</dd>
<dt class="label" id="id37"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Ibo Van de Poel, Jessica Nihlén Fahlquist, Neelke Doorn, Sjoerd Zwart, and Lambèr Royakkers. The problem of many hands: climate change as an example. <em>Science and engineering ethics</em>, 18:49–67, 2012.</p>
</dd>
<dt class="label" id="id38"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Helen Nissenbaum. Accountability in a computerized society. <em>Science and engineering ethics</em>, 2:25–42, 1996.</p>
</dd>
<dt class="label" id="id39"><span class="brackets"><a class="fn-backref" href="#id5">4</a></span></dt>
<dd><p>Batya Friedman. Moral responsibility and computer technology. 1990.</p>
</dd>
<dt class="label" id="id42"><span class="brackets"><a class="fn-backref" href="#id6">5</a></span></dt>
<dd><p>Ibo Van de Poel. The relation between forward-looking and backward-looking responsibility. In <em>Moral responsibility: Beyond free will and determinism</em>, pages 37–52. Springer, 2011.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Sietze Kuilman.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
<span id="document-Accountability/L2.Reproducibility"></span><div class="tex2jax_ignore mathjax_ignore section" id="reproducibility">
<h4>Reproducibility<a class="headerlink" href="#reproducibility" title="Permalink to this headline">¶</a></h4>
<p><em>Synonyms:</em> Replicability, Repeatability.</p>
<div class="section" id="in-brief">
<h5>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p><strong>Reproducibility</strong> is the ability of independent investigators to draw the same conclusions from an experiment by following the documentation shared by the original investigators <span id="id1">[<a class="reference internal" href="#id2775">1</a>]</span>.</p>
</div>
<div class="section" id="abstract">
<h5>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">¶</a></h5>
<p>This entry firstly introduces the motivations behind reproducibility in
the scientific process and, then, in artificial intelligence and machine
learning. Due to the rather wide range of different meanings of
reproducibility in the literature and the ambiguity of the terms, a
brief review of the most important definitions is provided and
discussed. In this context, we promote the most stable formulation of
the definition. Practical guidelines to various standards for
documenting code, technical experiment setup, and data are also
discussed.</p>
</div>
<div class="section" id="motivation-and-background">
<h5>Motivation and Background<a class="headerlink" href="#motivation-and-background" title="Permalink to this headline">¶</a></h5>
<p>Reproducibility in science means that one can repeat or replicate the
same (or sufficiently similar) experiment and obtain the same (or
sufficiently similar) research results as the original scientists on the
basis of their publications and descriptions. To this aim and to ease
the replication, the discovered claims, methods and analyses should be
described in a sufficiently detailed and transparent way. Diverse
reproducibility settings have been identified in the literature, see
e.g. <span id="id2">[<a class="reference internal" href="#id2775">1</a>]</span> <span id="id3">[<a class="reference internal" href="#id2792">2</a>]</span>, but from a more general standpoint, reproducibility entails that studies are reproduced
by independent researchers.</p>
<p>Reproducibility is an essential ingredient of the scientific method,
meant to verify the published results and claims and to enable a
continuous self-correcting process in scientific discoveries.
Unfortunately, the rising of a so-called research replication crisis has
been lately pointed out (<span id="id4">[<a class="reference internal" href="#id2772">3</a>]</span>). According to several surveys, a
relatively too large amount of published research results, in such
disciplines as chemistry, biology, medicine and pharmacy, earth and
environmental sciences, cannot be repeated. This may suggest issues with
these results or at least with their good descriptions. Reproducibility
in artificial intelligence (AI) and, in particular machine learning
(ML), are specifically challenging. The continuously increasing
complexity of new methods (often having many hyper-parameters that need
specialized optimization strategies), the size of studied datasets and
the use of advanced computational resources pose many difficulties for
communicating the necessary results as compared to the older works. The
paper <span id="id5">[<a class="reference internal" href="#id2773">4</a>]</span> presents the view of some researchers (such as J.
Pineau citing her interview) claiming that ML was previously more
theoretically based, while it has become a more experimental science in
the past decade, and many proposals of new models, in particular deep
networks, come from running many experiments with the intensive use of
available data. In this context, the authors (<span id="id6">[<a class="reference internal" href="#id2776">5</a>]</span>)
indicate growing difficulties in reproducing the work of others. Other
reasons of difficulty in reproducibility include: lack of access to the
same training data or differences in data distribution;
mis-specification or under-specification of the model or training
procedure; lack of availability of the code necessary to run the
experiments, or errors in the code; under-specification of the metrics
used to report results; selective reporting of results and ignorance of
the danger of adaptive overfitting as well as the use of adaptation
strategies embedded in the development libraries.</p>
<p>Nevertheless, software solutions and systems based on AI and ML are
gaining momentum. Many of them are being used in high-stake applications
where their decisions can have an impact on people and society, and
their improper operation may cause harm. In this frame, the quest for
reproducibility of such methods is even more urgent and reproducibility
becomes one of the key postulates within Responsible AI or Trustworthy
AI. <span id="id7">[<a class="reference internal" href="#id2774">6</a>]</span> also claims that reproducibility of AI is very
important for other reasons. Researchers, students and R&amp;D engineers
need to have a good understanding of new and, often quite complex,
methods, reproduce them (sometimes by their own re-implementations),
carefully check their correctness, examine their working conditions and
limitations, as well as to verify the presented results, especially if
they need to further use them in their systems often applied to complex
tasks. Moreover much of AI new projects receive either public or
business funds, so it should be subject to accountability and it is
necessary to convince others that these projects can produce reliable
results.</p>
</div>
<div class="section" id="terminology">
<h5>Terminology<a class="headerlink" href="#terminology" title="Permalink to this headline">¶</a></h5>
<p>In this handbook, we follow the concept of reproducibility introduced by
<span id="id8">[<a class="reference internal" href="#id2771">7</a>]</span>. According to this concept, which is also adopted in a
number of more recent papers
(e.g.,<span id="id9">[<a class="reference internal" href="#id2775">1</a>]</span>; <span id="id10">[<a class="reference internal" href="#id2782">8</a>]</span>; <span id="id11">[<a class="reference internal" href="#id2776">5</a>]</span>),
<em>reproducibility</em> refers to the ability of an independent researcher to
reproduce the same, or reasonably similar results using the data and the
experimental setup provided by the original authors.</p>
<p>Reproducibility should not be confused with other terms describing the
ability to replicate the results in science, such as replicability and
repeatability (<span id="id12">[<a class="reference internal" href="#id2792">2</a>]</span>]). <em>Replicability</em> defined in a way consistent with our understanding of reproducibility is the
ability of an independent researcher to produce results that are
consistent with the conclusions of the original work, using new data or
different the experimental setup. The term <em>repeatability</em> appears in
some references, e.g. <span id="id13">[<a class="reference internal" href="#id2779">9</a>]</span> that uses a notion of reproducibility
inconsistent with our definition, but should be considered to describe
an ability of a researcher to repeat his/her own experimental procedures
using same experimental setup and data, while achieving reasonably
repeatable results that support the same conclusions.</p>
<p>In order to compare these reproducibility-related terms, the main
conceptual dimensions need to be identified. Based on the analysis of
the literature, the following dimensions can be distinguished: (i)
availability of the components originally deployed in experimental
workflows (i.e., data, code and analysis as considered by
<span id="id14">[<a class="reference internal" href="#id2776">5</a>]</span>; <span id="id15">[<a class="reference internal" href="#id2777">10</a>]</span>; <span id="id16">[<a class="reference internal" href="#id2778">11</a>]</span>); (ii)
teams involved in the experimentation (i.e., whether or not the
experiments was conducted by the same group who is running the
reproducibility validation); (iii) reasons because the experiment or
part of it is re-conducted (i.e., validating the repeatability of the
experiment or as suggested by <span id="id17">[<a class="reference internal" href="#id2775">1</a>]</span> corroborating the
scientific hypothesis and theory the experiment aims to support. With
respect to these conceptual dimensions, the reproducibility-related
terms used in the literature can be clustered in the following way:</p>
<ul class="simple">
<li><p>Most of the literature (including <span id="id18">[<a class="reference internal" href="#id2776">5</a>]</span>; <span id="id19">[<a class="reference internal" href="#id2775">1</a>]</span>; <span id="id20">[<a class="reference internal" href="#id2780">12</a>]</span>]) refers to reproducibility as the
attempt to replicate experiment as much as possible as the original
one, that is by using original data, code and analysis when
available. Computational reproducibility, method reproducibility,
direct replication and recomputation are used in lieu of
reproducibility respectively by <span id="id21">[<a class="reference internal" href="#id2777">10</a>]</span>,
<span id="id22">[<a class="reference internal" href="#id2784">13</a>]</span>, <span id="id23">[<a class="reference internal" href="#id2785">14</a>]</span>, <span id="id24">[<a class="reference internal" href="#id2786">15</a>]</span> and
<span id="id25">[<a class="reference internal" href="#id2787">16</a>]</span>. <span id="id26">[<a class="reference internal" href="#id2775">1</a>]</span> distinguishes the notion of
reproducibility from corroborating the scientific hypotheses or
theory to ground which the experiment is designed for.</p></li>
<li><p>The term replicability is highlighted by
<span id="id27">[<a class="reference internal" href="#id2771">7</a>]</span>, <span id="id28">[<a class="reference internal" href="#id2782">8</a>]</span>, <span id="id29">[<a class="reference internal" href="#id2780">12</a>]</span>, <span id="id30">[<a class="reference internal" href="#id2776">5</a>]</span>, where an independent team can obtain the same result using the data, which could be slightly different, and methods which they develop completely independently or change slightly. Furthermore <span id="id31">[<a class="reference internal" href="#id2782">8</a>]</span>; <span id="id32">[<a class="reference internal" href="#id2776">5</a>]</span> use another name –
robust – for carrying out the experiments with the same data and
some changes in an analysis or code implementations.</p></li>
<li><p>Some works such as <span id="id33">[<a class="reference internal" href="#id2779">9</a>]</span> <span id="id34">[<a class="reference internal" href="#id2782">8</a>]</span>; <span id="id35">[<a class="reference internal" href="#id2783">17</a>]</span>; <span id="id36">[<a class="reference internal" href="#id2775">1</a>]</span> uses repeatability to indicate a weaker level of reproducibility where the replication of the experiment is achieved by the same team that provided the original experiments.</p></li>
</ul>
<p>In the context of the above literature review, it is also worth
clarifying the discussion of what is reproduced as a result of the above
activities and how to understand the term result. In the case of AI
works, <span id="id37">[<a class="reference internal" href="#id2775">1</a>]</span> distinguishes between different possible
results to reproduce:</p>
<ul class="simple">
<li><p>Outcome – the result of applying the model implementation for
selected data (e.g., predictions - labels for test examples)</p></li>
<li><p>Analysis – calculated measures or other indicators (e.g. prediction
accuracy values)</p></li>
<li><p>Interpretation – more general conclusions from the experiments.
According to Gunderesn the last point is the most important in
reproducibility, because in the scientific method certain hypotheses
are tested or certain beliefs are confirmed.</p></li>
</ul>
<p>Similar importance of refining the levels of reproducibility has the
division proposed in <span id="id38">[<a class="reference internal" href="#id2784">13</a>]</span>:</p>
<ul class="simple">
<li><p>Reproducibility of methods: the ability to implement, as exactly as
possible, the experimental and computational procedures, with the
same data and tools, to obtain the same results</p></li>
<li><p>Reproducibility of results: the production of corroborating results
in a new study, having used the same experimental methods</p></li>
<li><p>Reproducibility of inference: the drawing of qualitatively similar
conclusions from either an independent replication of a study or a
reanalysis of the original study</p></li>
</ul>
<p>The general definitions should be however made more specific whenever we
apply it to contemporary artificial intelligence research, and to the
sub-field of machine learning in particular. The reasons are grounded in
the high complication of the modern software processing pipelines, that
often depend on third-party software (frameworks, libraries), use an
extended set of metaparameters that are crucial to arrive at the correct
results, and require modern hardware (e.g. recent GPU cards) with it’s
specific architecture and drivers. These features of AI research and
applications make this field different from the general science, where
reproducibility refers primarily to the careful documentation of the
experimental procedure.</p>
<p>In AI systems, the main components of the experimental setup are
software and data. The software plays the role of our experimental
setup. Although depending on the specific context, hardware components
may be included as well (e.g. in computer vision, robotics), most of the
AI-related research is conducted on pre-recorded datasets, so we can
limit our scope to the software. The other dimension is data. Together,
software and data define the conceptual dimensions of the space on which
the defined terms are spanned in AI. However, as we noticed earlier, AI
is a very broad field, with a number of distinctive sub-fields that have
specific requirements when it comes to defining the exact elements of
software, and sometimes have specific requirements as to the data, such
as elimination of biases or privacy issues. This motivates the
introduction of guidelines or “best practices” for reproducibility, that
often also include terms that define the degree to which the postulate
of full reproducibility is met, usually in relation to the amount of
code, technical details and data that the author shares with readers.</p>
</div>
<div class="section" id="guidelines">
<h5>Guidelines<a class="headerlink" href="#guidelines" title="Permalink to this headline">¶</a></h5>
<p>Definitions of the different reproducibility-related terms are often
accompanied by badges and guidelines helping people in making the
definitions operational.</p>
<ul class="simple">
<li><p>Some definitions differentiate the notion of reproducibility
according to the kind of resource shared. For example
<span id="id39">[<a class="reference internal" href="#id2777">10</a>]</span> focus on <em>computational reproducibility</em> with
<strong>bronze, silver, gold</strong> standards. <span id="id40">[<a class="reference internal" href="#id2778">11</a>]</span> and
<span id="id41">[<a class="reference internal" href="#id2775">1</a>]</span> propose different increasing levels <em>R1, R2, R3, R4</em>
depending on whether experiment descriptions, codes, data and
experiment are stored. &#64;ACMv1.1 recommends that three separate
badges related to artefact review be associated with research
articles in ACM publications: Artifacts Evaluated, Artifacts
Available and Results Validated.</p></li>
<li><p>Guidelines ease the description of experiments. For example,
<span id="id42">[<a class="reference internal" href="#id2776">5</a>]</span> provides a special Machine learning
reproducibility checklist; datasheets <span id="id43">[<a class="reference internal" href="#id2789">18</a>]</span>,
model cards <span id="id44">[<a class="reference internal" href="#id2788">19</a>]</span> and factsheets
<span id="id45">[<a class="reference internal" href="#id2790">20</a>]</span> provides templates for describing datasets
and the AI models deployed increasing the transparency and
accountability of experimentations and operational intelligent
systems.</p></li>
</ul>
<p>Below a few of the above guidelines are precised. Following
<span id="id46">[<a class="reference internal" href="#id2777">10</a>]</span>’s proposal, the three degrees of the reproducibility
standards for ML are based on availability of data, model, and code, as
well as other analyses or programming dependencies. For instance, in the
bronze standards (the minimal requirements for reproducibility) the
authors should make the data, model and its source code publicly
available for downloading. The silver standard extends it by
additionally providing: dependencies of the analysis (in a form to be
installed in a single command), recording key details of the analysis
and used software requirements. Furthermore, all elements in the
analysis should be documented to be set deterministic. Within the gold
standard the authors should also prepare this analysis reproducible with
a single command - which is the most demanding with respect to full
automatization of the reproducibility process.</p>
<p><span id="id47">[<a class="reference internal" href="#id2776">5</a>]</span> specify the necessary elements to be documented and
made public with respect to the following categories: model and
algorithm, theoretical claims, datasets used in experiments, shared code
including dependencies specifications, all reported experimental results
(with all details for the experimental setup, hyper-parameters, training
details, definitions of evaluation measures, and description of the
computing infrastructure used). <span id="id48">[<a class="reference internal" href="#id2782">8</a>]</span> provide similar
recommendations for ML in robotics, by focusing on the reproducibility
of computation experiments on real robots. They stress the role of
managing properly the software dependencies, distinguishing between
experimental code and library code, and documenting the measurement
metrics, which is essential for reinforcement learning.</p>
<p>Datasheets by <span id="id49">[<a class="reference internal" href="#id2789">18</a>]</span> specify how to document the
motivation, composition, collection process, recommended uses for data
deployed in the systems and experiments; model cards by
<span id="id50">[<a class="reference internal" href="#id2788">19</a>]</span> ease the description of model’s intended use
cases limiting their usage in contexts for which they are not well
suited; factsheets <span id="id51">[<a class="reference internal" href="#id2790">20</a>]</span> provide a template for
describing the purpose, performance, safety, security, and provenance
information to be completed by AI service providers for examination by
consumers.</p>
</div>
<div class="section" id="software-frameworks-supporting-reproducibility">
<h5>Software frameworks supporting reproducibility<a class="headerlink" href="#software-frameworks-supporting-reproducibility" title="Permalink to this headline">¶</a></h5>
<p>Lately, a paradigm based on tailoring the DevOps approach to AI and ML
is emerging as a practical tool for ensuring reproducibility. This
paradigm makes use of frameworks for Machine Learning Model
Operationalization Management (MLOps), which streamline the whole
development lifecycle of AI and ML models. MLOps enables developers and
auditors to keep track of and inspect the various choices done and the
artefacts produced in the different phases of AI and ML design and
development (i.e., data gathering, data analysis, data
transformation/preparation, model training and development, model
validation, and model serving). <span id="id52">[<a class="reference internal" href="#id2793">21</a>]</span> analyze some of the
available open tools for MLOps. This allows for maintaining a
comprehensive documentation that is at the basis of model
reproducibility.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id53"><dl class="citation">
<dt class="label" id="id2775"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>,<a href="#id9">3</a>,<a href="#id17">4</a>,<a href="#id19">5</a>,<a href="#id26">6</a>,<a href="#id36">7</a>,<a href="#id37">8</a>,<a href="#id41">9</a>)</span></dt>
<dd><p>Odd Erik Gundersen. The fundamental principles of reproducibility. <em>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</em>, 379(2197):20200210, 2021. URL: <a class="reference external" href="https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2020.0210">https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2020.0210</a>, <a class="reference external" href="https://arxiv.org/abs/https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2020.0210">arXiv:https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2020.0210</a>, <a class="reference external" href="https://doi.org/10.1098/rsta.2020.0210">doi:10.1098/rsta.2020.0210</a>.</p>
</dd>
<dt class="label" id="id2792"><span class="brackets">2</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id12">2</a>)</span></dt>
<dd><p>Hans E. Plesser. Reproducibility vs. Replicability: A Brief History of a Confused Terminology. <em>Frontiers in Neuroinformatics</em>, 11:76, January 2018. URL: <a class="reference external" href="http://journal.frontiersin.org/article/10.3389/fninf.2017.00076/full">http://journal.frontiersin.org/article/10.3389/fninf.2017.00076/full</a> (visited on 2021-11-18), <a class="reference external" href="https://doi.org/10.3389/fninf.2017.00076">doi:10.3389/fninf.2017.00076</a>.</p>
</dd>
<dt class="label" id="id2772"><span class="brackets"><a class="fn-backref" href="#id4">3</a></span></dt>
<dd><p>Monya Baker. 1,500 scientists lift the lid on reproducibility. <em>Nature</em>, 533:452–454, 2016.</p>
</dd>
<dt class="label" id="id2773"><span class="brackets"><a class="fn-backref" href="#id5">4</a></span></dt>
<dd><p>Will Douglas Heaven. Ai is wrestling with a replication crisis. <em>MIT Technology Review</em>, 2020.</p>
</dd>
<dt class="label" id="id2776"><span class="brackets">5</span><span class="fn-backref">(<a href="#id6">1</a>,<a href="#id11">2</a>,<a href="#id14">3</a>,<a href="#id18">4</a>,<a href="#id30">5</a>,<a href="#id32">6</a>,<a href="#id42">7</a>,<a href="#id47">8</a>)</span></dt>
<dd><p>Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence d'Alché-Buc, Emily Fox, and Hugo Larochelle. Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program). <em>arXiv:2003.12206 [cs, stat]</em>, December 2020. arXiv: 2003.12206. URL: <a class="reference external" href="http://arxiv.org/abs/2003.12206">http://arxiv.org/abs/2003.12206</a> (visited on 2021-09-26).</p>
</dd>
<dt class="label" id="id2774"><span class="brackets"><a class="fn-backref" href="#id7">6</a></span></dt>
<dd><p>Edward Raff. A step toward quantifying independently reproducible machine learning research. 2019. <a class="reference external" href="https://arxiv.org/abs/1909.06674">arXiv:1909.06674</a>.</p>
</dd>
<dt class="label" id="id2771"><span class="brackets">7</span><span class="fn-backref">(<a href="#id8">1</a>,<a href="#id27">2</a>)</span></dt>
<dd><p>Jon F. Claerbout and Martin Karrenbach. Electronic documents give reproducible research a new meaning. In <em>SEG Technical Program Expanded Abstracts 1992</em>, 601–604. 2005.</p>
</dd>
<dt class="label" id="id2782"><span class="brackets">8</span><span class="fn-backref">(<a href="#id10">1</a>,<a href="#id28">2</a>,<a href="#id31">3</a>,<a href="#id34">4</a>,<a href="#id48">5</a>)</span></dt>
<dd><p>Nicolai A. Lynnerup, Laura Nolling, Rasmus Hasle, and John Hallam. A survey on reproducibility by evaluating deep reinforcement learning algorithms on real-world robots. In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura, editors, <em>Proceedings of the Conference on Robot Learning</em>, volume 100 of Proceedings of Machine Learning Research, 466–489. PMLR, 30 Oct–01 Nov 2020. URL: <a class="reference external" href="https://proceedings.mlr.press/v100/lynnerup20a.html">https://proceedings.mlr.press/v100/lynnerup20a.html</a>.</p>
</dd>
<dt class="label" id="id2779"><span class="brackets">9</span><span class="fn-backref">(<a href="#id13">1</a>,<a href="#id33">2</a>)</span></dt>
<dd><p>ACM Artifact Review and Badging - Version 1.1. August 2020. URL: <a class="reference external" href="https://www.acm.org/publications/policies/artifact-review-and-badging-current">https://www.acm.org/publications/policies/artifact-review-and-badging-current</a> (visited on 2022-01-20).</p>
</dd>
<dt class="label" id="id2777"><span class="brackets">10</span><span class="fn-backref">(<a href="#id15">1</a>,<a href="#id21">2</a>,<a href="#id39">3</a>,<a href="#id46">4</a>)</span></dt>
<dd><p>Benjamin J. Heil, Michael M. Hoffman, Florian Markowetz, Su-In Lee, Casey S. Greene, and Stephanie C. Hicks. Reproducibility standards for machine learning in the life sciences. <em>Nature Methods</em>, October 2021. URL: <a class="reference external" href="https://www.nature.com/articles/s41592-021-01256-7">https://www.nature.com/articles/s41592-021-01256-7</a> (visited on 2021-11-18), <a class="reference external" href="https://doi.org/10.1038/s41592-021-01256-7">doi:10.1038/s41592-021-01256-7</a>.</p>
</dd>
<dt class="label" id="id2778"><span class="brackets">11</span><span class="fn-backref">(<a href="#id16">1</a>,<a href="#id40">2</a>)</span></dt>
<dd><p>Odd Erik Gundersen and Sigbjørn Kjensmo. State of the Art: Reproducibility in Artificial Intelligence. In <em>Proceedings of the AAAI Conference on Artificial Intelligence,</em>. 2018.</p>
</dd>
<dt class="label" id="id2780"><span class="brackets">12</span><span class="fn-backref">(<a href="#id20">1</a>,<a href="#id29">2</a>)</span></dt>
<dd><p>National Academies of Sciences, Engineering, and Medicine. <em>Reproducibility and Replicability in Science</em>. The National Academies Press, Washington, DC, 2019. ISBN 978-0-309-48616-3. URL: <a class="reference external" href="https://www.nap.edu/catalog/25303/reproducibility-and-replicability-in-science">https://www.nap.edu/catalog/25303/reproducibility-and-replicability-in-science</a>, <a class="reference external" href="https://doi.org/10.17226/25303">doi:10.17226/25303</a>.</p>
</dd>
<dt class="label" id="id2784"><span class="brackets">13</span><span class="fn-backref">(<a href="#id22">1</a>,<a href="#id38">2</a>)</span></dt>
<dd><p>Steven N. Goodman, Daniele Fanelli, and John P. A. Ioannidis. What does research reproducibility mean? <em>Science Translational Medicine</em>, 8(341):341ps12–341ps12, 2016. URL: <a class="reference external" href="https://www.science.org/doi/abs/10.1126/scitranslmed.aaf5027">https://www.science.org/doi/abs/10.1126/scitranslmed.aaf5027</a>, <a class="reference external" href="https://arxiv.org/abs/https://www.science.org/doi/pdf/10.1126/scitranslmed.aaf5027">arXiv:https://www.science.org/doi/pdf/10.1126/scitranslmed.aaf5027</a>, <a class="reference external" href="https://doi.org/10.1126/scitranslmed.aaf5027">doi:10.1126/scitranslmed.aaf5027</a>.</p>
</dd>
<dt class="label" id="id2785"><span class="brackets"><a class="fn-backref" href="#id23">14</a></span></dt>
<dd><p>Stephan Guttinger. The limits of replicability. <em>European Journal for Philosophy of Science</em>, 10(2):1–17, 2020.</p>
</dd>
<dt class="label" id="id2786"><span class="brackets"><a class="fn-backref" href="#id24">15</a></span></dt>
<dd><p>Ian P Gent and Lars Kotthoff. Recomputation. org: experiences of its first year and lessons learned. In <em>2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing</em>, 968–973. IEEE, 2014.</p>
</dd>
<dt class="label" id="id2787"><span class="brackets"><a class="fn-backref" href="#id25">16</a></span></dt>
<dd><p>Victoria C Stodden. Trust your science? open your data and code. 2011. URL: <a class="reference external" href="https://magazine.amstat.org/blog/2011/07/01/trust-your-science/">https://magazine.amstat.org/blog/2011/07/01/trust-your-science/</a>.</p>
</dd>
<dt class="label" id="id2783"><span class="brackets"><a class="fn-backref" href="#id35">17</a></span></dt>
<dd><p>Joint Committee for Guides in Metrology. The international vocabulary of metrology – basic and general concepts and associated terms - 3rd edition with minor corrections. <em>JcGM</em>, 2012. URL: <a class="reference external" href="https://www.bipm.org/utils/common/documents/jcgm/JCGM_200_2012.pdf">https://www.bipm.org/utils/common/documents/jcgm/JCGM_200_2012.pdf</a>.</p>
</dd>
<dt class="label" id="id2789"><span class="brackets">18</span><span class="fn-backref">(<a href="#id43">1</a>,<a href="#id49">2</a>)</span></dt>
<dd><p>Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. Datasheets for Datasets. <em>arXiv:1803.09010 [cs]</em>, March 2020. arXiv: 1803.09010. URL: <a class="reference external" href="http://arxiv.org/abs/1803.09010">http://arxiv.org/abs/1803.09010</a> (visited on 2021-09-14).</p>
</dd>
<dt class="label" id="id2788"><span class="brackets">19</span><span class="fn-backref">(<a href="#id44">1</a>,<a href="#id50">2</a>)</span></dt>
<dd><p>Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model Cards for Model Reporting. In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, 220–229. Atlanta GA USA, January 2019. ACM. URL: <a class="reference external" href="https://dl.acm.org/doi/10.1145/3287560.3287596">https://dl.acm.org/doi/10.1145/3287560.3287596</a> (visited on 2021-09-14), <a class="reference external" href="https://doi.org/10.1145/3287560.3287596">doi:10.1145/3287560.3287596</a>.</p>
</dd>
<dt class="label" id="id2790"><span class="brackets">20</span><span class="fn-backref">(<a href="#id45">1</a>,<a href="#id51">2</a>)</span></dt>
<dd><p>Matthew Arnold, Rachel K. E. Bellamy, Michael Hind, Stephanie Houde, Sameep Mehta, Aleksandra Mojsilovic, Ravi Nair, Karthikeyan Natesan Ramamurthy, Darrell Reimer, Alexandra Olteanu, David Piorkowski, Jason Tsay, and Kush R. Varshney. FactSheets: Increasing Trust in AI Services through Supplier's Declarations of Conformity. <em>arXiv:1808.07261 [cs]</em>, February 2019. URL: <a class="reference external" href="http://arxiv.org/abs/1808.07261">http://arxiv.org/abs/1808.07261</a> (visited on 2021-09-14).</p>
</dd>
<dt class="label" id="id2793"><span class="brackets"><a class="fn-backref" href="#id52">21</a></span></dt>
<dd><p>Philipp Ruf, Manav Madan, Christoph Reich, and Djaffar Ould-Abdeslam. Demystifying mlops and presenting a recipe for the selection of open-source tools. <em>Applied Sciences</em>, 2021. URL: <a class="reference external" href="https://www.mdpi.com/2076-3417/11/19/8861">https://www.mdpi.com/2076-3417/11/19/8861</a>, <a class="reference external" href="https://doi.org/10.3390/app11198861">doi:10.3390/app11198861</a>.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Riccardo Albertoni, Sara Colantonio, Piotr Skrzypczyński, and Jerzy Stefanowski.</p>
</div></blockquote>
</div>
</div>
<span id="document-Accountability/L2.Traceability"></span><div class="tex2jax_ignore mathjax_ignore section" id="traceability">
<h4>Traceability<a class="headerlink" href="#traceability" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p><strong>Traceability</strong> can be defined as the need to maintain a complete and clear
documentation of the data, processes, artefacts and actors involved in the entire
lifecycle of an AI model, starting from its design and ending with its production
serving <span id="id1">[<a class="reference internal" href="#id2753">2</a>]</span>.</p>
</div>
<div class="section" id="abstract">
<h5>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">¶</a></h5>
<p>This entry introduces the motivations behind traceability and
illustrates its core requirements, which encompass documenting the
entire development cycle of an AI model and tracking its live
functioning after the deployment in production.</p>
</div>
<div class="section" id="motivation-and-background">
<h5>Motivation and Background<a class="headerlink" href="#motivation-and-background" title="Permalink to this headline">¶</a></h5>
<p>Developing an Artificial Intelligence (AI) model or an AI-powered system
entails a considerable number of choices along the entire development
process, which may result in diverse behaviours and functioning of the
same model or system. This phenomenon is particularly relevant when
learning-based approaches comes into play, due to the dependency of
Machine Learning (ML) models on the data used for their training as well
as the complexity and variety of the ML methods that might be used,
especially when based on Deep Learning (DL). Furthermore, the
development of such models relies often on large trial-&amp;-error
experimental processes, which are not commonly well documented (see the
reproducibility entry).</p>
<p>This condition makes it evident the need for a comprehensive and clear
documentation of the actions taken as well as the various processing
steps performed when developing an AI or ML model, as, without this
documentation, it might be difficult to reconstruct the reasons behind
the outcomes and the functioning of an AI model. In consideration of
this, the High-Level Expert Group on AI (AI HLEG) has included the
traceability of an AI model as one of the main mean to enable the
transparency principle for Trustworthy AI <span id="id2">[<a class="reference internal" href="#id2755">1</a>]</span>. Overall,
traceability aims to ensure the avoidance of any “grey” area about the
AI model or system, thus guaranteeing the transparency of and the trust
in the development, production functioning and usage of an AI system.
The record-keeping activity entailed by traceability should regard the
data used, the data pre-processing steps as well as the development
settings, the development workflows and the actors involved
<span id="id3">[<a class="reference internal" href="#id2754">3</a>]</span>. This encompasses the detailed provision of information
about the provenance and the usage of any data and artefacts involved in
the development of the AI model or system. In this view, traceability
incorporates the measures to ensure reproducibility and it can be
understood as the technological mean for guaranteeing the auditability
and accountability of AI models and systems <span id="id4">[<a class="reference internal" href="#id2767">4</a>]</span>.</p>
</div>
<div class="section" id="the-two-souls-of-traceability">
<h5>The two souls of traceability<a class="headerlink" href="#the-two-souls-of-traceability" title="Permalink to this headline">¶</a></h5>
<p>AI models based on learning are data-inductive and dynamic systems,
whose development relies on an initial set of data. This set, although
large, might not necessarily span the whole variability range of
real-world cases or conditions. This implies that, when used in
practice, the AI model or system can encounter slightly different or
novel data that differ from those it has been exposed to during
training. This phenomenon calls for the monitoring of AI models after
their deployment in production, in order to log their usage as well as
to track over time their performance, vitality and conduct. Such
an AI maintenance system is an important part of traceability, which can
be then seen as one principle, two souls:</p>
<ul class="simple">
<li><p><a class="reference internal" href="TAILOR.html#document-Accountability/L3.Provenance_tracking"><span class="doc std std-doc">provenance tracking of data, processes and artefacts involved in the
development of the AI model</span></a>,</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Accountability/L3.Continuous_performance_monitoring"><span class="doc std std-doc">continuous performance monitoring of the AI model after deployment
in production</span></a>.</p></li>
</ul>
<p>These two aspects will be further explained in the following entries.</p>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id5"><dl class="citation">
<dt class="label" id="id2755"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>European Commission, Content Directorate-General for Communications Networks, and Technology. <em>Ethics guidelines for trustworthy AI</em>. Publications Office, 2019. URL: <a class="reference external" href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai">https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</a>.</p>
</dd>
<dt class="label" id="id2753"><span class="brackets"><a class="fn-backref" href="#id1">2</a></span></dt>
<dd><p>European Commission, Content Directorate-General for Communications Networks, and Technology. <em>The Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self assessment</em>. Publications Office, 2020. URL: <a class="reference external" href="https://doi.org/10.2759/002360">https://doi.org/10.2759/002360</a>, <a class="reference external" href="https://doi.org/10.2759/002360">doi:10.2759/002360</a>.</p>
</dd>
<dt class="label" id="id2754"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Michael Bücker, Gero Szepannek, Alicja Gosiewska, and Przemyslaw Biecek. Transparency, auditability, and explainability of machine learning models in credit scoring. <em>Journal of the Operational Research Society</em>, 73(1):70–90, 2022. <a class="reference external" href="https://doi.org/10.1080/01605682.2021.1922098">doi:10.1080/01605682.2021.1922098</a>.</p>
</dd>
<dt class="label" id="id2767"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Karim Lekadir, Richard Osuala, Catherine Gallin, Noussair Lazrak, Kaisar Kushibar, Gianna Tsakou, Susanna Aussó, Leonor Cerdá Alberich, Kostas Marias, Manolis Tsiknakis, Sara Colantonio, Nickolas Papanikolaou, Zohaib Salahuddin, Henry C Woodruff, Philippe Lambin, and Luis Martí-Bonmatí. Future-ai: guiding principles and consensus recommendations for trustworthy artificial intelligence in medical imaging. 2021. URL: <a class="reference external" href="https://arxiv.org/abs/2109.09658">https://arxiv.org/abs/2109.09658</a>, <a class="reference external" href="https://doi.org/10.48550/ARXIV.2109.09658">doi:10.48550/ARXIV.2109.09658</a>.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Sara Colantonio.</p>
</div></blockquote>
</div>
<div class="toctree-wrapper compound">
<span id="document-Accountability/L3.Provenance_tracking"></span><div class="tex2jax_ignore mathjax_ignore section" id="provenance-tracking">
<h5>Provenance Tracking<a class="headerlink" href="#provenance-tracking" title="Permalink to this headline">¶</a></h5>
<div class="section" id="in-brief">
<h6>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p><strong>Provenance Tracking</strong> represents the tracking of  “information that describes the production process of an end product, which can be anything from a piece of data to a physical object. […] Essentially, provenance can be seen as meta-data that, instead of describing data, describes a production process.” <span id="id1">[<a class="reference internal" href="#id2728">1</a>]</span></p>
</div>
<div class="section" id="more-in-detail">
<h6>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>“Traceability in AI shares part of its scope with general-purpose recommendations for provenance … ”<span id="id2">[<a class="reference internal" href="#id2727">2</a>]</span>. In fact, provenance is “any information that describes the production process of an end product, which can be anything from a piece of data to a physical object”
<span id="id3">[<a class="reference internal" href="#id2728">1</a>]</span>.</p>
<p>Keeping track of provenance is vital in various settings. It contributes
to increasing the trust on produced systems. According to Gil et al.
<span id="id4">[<a class="reference internal" href="#id2729">3</a>]</span>, the provenance of scientific results, i.e., “how results were derived, what parameters influenced the
derivation, what datasets were used as input to the experiment, etc.”
facilitates the reproducibility of the whole process.</p>
<p>Data and workflows deployed in an AI system are two key ingredients in
traceability and provenance tracking. In particular, the distinction
between <em>prospective</em> and <em>retrospective</em> provenance is introduced in
the literature when dealing with workflows. The prospective provenance
models workflows in an abstract and informative way, as templates
composed of tasks that can be instantiated, modified, and combined. The
retrospective provenance models past workflows, highlighting what task
was executed and how data or other artifacts were derived
<span id="id5">[<a class="reference internal" href="#id2730">4</a>]</span>.
Herschel et al. <span id="id6">[<a class="reference internal" href="#id2728">1</a>]</span> claims that “provenance can be seen as metadata that,
instead of describing data, describes a production process”. Considering the central role metadata play
in tracking provenance, this section discusses some popular models to
track provenance. In particular, Garijo et al.  <span id="id7">[<a class="reference internal" href="#id2726">5</a>]</span> have
provided a holistic, Linked Data compliant, and ready-to-use solution to
document workflow specifications and their executions, which exploits PROV-O
<span id="id8">[<a class="reference internal" href="#id2724">6</a>]</span>, P-PLAN <span id="id9">[<a class="reference internal" href="#id2725">7</a>]</span> and the Open Provenance Model for
Workflows (OPMW)<a class="footnote-reference brackets" href="#id2924" id="id10">1</a>.</p>
<p>PROV is a metadata model defined as a W3C recommendation. It captures
the provenance documenting the entities, agents, actions, and the
involved in a production chain, and the relations among them (e.g.,
attribution and usage). PROV acknowledges the need to represent
workflows, also called plans, by including a construct such as
<em>prov:Plan</em>. However, “it does not elaborate any
further on how plans can be described or related to other provenance elements
of the execution.” <span id="id11">[<a class="reference internal" href="#id2725">7</a>]</span>.</p>
<p>P-PLAN vocabulary extends PROV-O introducing constructs for plans
(<em>p-plan:Plan</em> subclass of <em>prov:Plan</em>), their steps (<em>p-plan:Step</em>) and
their input and output variables (<em>p-plan:Variable</em>). Still, P-PLAN does
not model a full-fledged notion of workflow.</p>
<p>OPMW extends P-PLAN and the “Open Provenance Model (OPM), a legacy
provenance model developed by the workflow community that was used as a
reference to create PROV” <span id="id12">[<a class="reference internal" href="#id2726">5</a>]</span>. OPMW
distinguishes between workflow specifications, namely <em>templates</em>, and
their workflow execution traces.</p>
<p>OPMW specifies workflow <em>templates</em> as instances of the class
<em>opmw:WorkflowTemplate</em> (subclass of <em>p-plan:Plan</em>); the template
processes/actions as <em>opmw:WorkflowTemplateProcess</em> (subclass of
<em>p-plan:Step</em>); the template artifacts, manipulated or produced by
processes, as <em>opmw:WorkflowTemplateArtifact</em> (subclass of
<em>p-plan:Variable</em>). Accordingly, the template for the generic n-th step
is an instance of <em>opmw:WorkflowTemplateProcess</em>. The n-th template
steps’ input and output are indicated by the properties
<em>p-plan:hasInputVar</em> and <em>p-plan:isOutputVarOf</em>, and are instances of
<em>opmw:WorkflowTemplateArtifact</em>, representing any expected file,
parameter, and collection of documents considered and manipulated by the
template step. The classes <em>opmw:WorkflowExecutionAccount</em>,
<em>opmw:WorkflowExecutionProcess</em> and <em>opmw:WorkflowExecutionTemplate</em>
represent the execution counterparts of the template instances. The
properties <em>opmw:correspondsToTemplate</em>,
<em>opmw:correspondsToTemplateProcess</em>,
<em>opmw:correspondsToTemplateArtifact</em> bind the execution and the template
counterparts. Thus, n-th step is the actual execution of the n-th
template step and it is an instance of <em>opmw:WorkflowExecutionProcess</em>,
which is a specialization of the class <em>prov:Activity</em>. The actual
execution’s n-th input and output steps are indicated by the PROV
properties <em>prov:used</em> and <em>prov:wasGeneratedby</em>, and are instances of
<em>opmw:workflowExecutionArtifact</em> which is a particular kind of
<em>prov:Entity</em>. Albertoni et al. <span id="id13">[<a class="reference internal" href="#id2723">8</a>]</span> provides examples of
the use of the above metadata models when documenting scientific
experiments.</p>
<p>Although not specific to AI experiments and systems, the models
mentioned above offer some excellent standing and a backbone for
describing data, actors, other kinds of entities, and how these might
relate in experiments. Such a standing needs to be refined and extended
to capture the gist of specific AI experiments. AI-related controlled
terminologies might be required, for example, to complements the
backbones with the hyper-parameters, tasks and metrics for AI
techniques. Adopting a backbone, which is defined according to linked
data best practices, offers the ability to combine different models and
terminologies as needed, easing the tailoring of such backbone with the
required AI-specific and community-governed refinements.</p>
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p id="id14"><dl class="citation">
<dt class="label" id="id2728"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id3">2</a>,<a href="#id6">3</a>)</span></dt>
<dd><p>Melanie Herschel, Ralf Diestelkämper, and Houssem Ben Lahmar. A survey on provenance: What for? What form? What from? <em>VLDB Journal</em>, 26(6):881–906, 2017. <a class="reference external" href="https://doi.org/10.1007/s00778-017-0486-1">doi:10.1007/s00778-017-0486-1</a>.</p>
</dd>
<dt class="label" id="id2727"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Marçal Mora-Cantallops, Salvador Sánchez-Alonso, Elena García-Barriocanal, and Miguel-Angel Sicilia. Traceability for trustworthy AI: a review of models and tools. <em>Big Data and Cognitive Computing</em>, 2021. URL: <a class="reference external" href="https://www.mdpi.com/2504-2289/5/2/20">https://www.mdpi.com/2504-2289/5/2/20</a>, <a class="reference external" href="https://doi.org/10.3390/bdcc5020020">doi:10.3390/bdcc5020020</a>.</p>
</dd>
<dt class="label" id="id2729"><span class="brackets"><a class="fn-backref" href="#id4">3</a></span></dt>
<dd><p>Yolanda Gil, Ewa Deelman, Mark H. Ellisman, Thomas Fahringer, Geoffrey C. Fox, Dennis Gannon, Carole A. Goble, Miron Livny, Luc Moreau, and Jim Myers. Examining the challenges of scientific workflows. <em>IEEE Computer</em>, 40(12):24–32, 2007. URL: <a class="reference external" href="https://doi.org/10.1109/MC.2007.421">https://doi.org/10.1109/MC.2007.421</a>, <a class="reference external" href="https://doi.org/10.1109/MC.2007.421">doi:10.1109/MC.2007.421</a>.</p>
</dd>
<dt class="label" id="id2730"><span class="brackets"><a class="fn-backref" href="#id5">4</a></span></dt>
<dd><p>Chunhyeok Lim, Shiyong Lu, Artem Chebotko, and Farshad Fotouhi. Prospective and retrospective provenance collection in scientific workflow environments. In <em>IEEE SCC</em>, 449–456. IEEE Computer Society, 2010. URL: <a class="reference external" href="http://dblp.uni-trier.de/db/conf/IEEEscc/scc2010.html#LimLCF10">http://dblp.uni-trier.de/db/conf/IEEEscc/scc2010.html#LimLCF10</a>.</p>
</dd>
<dt class="label" id="id2726"><span class="brackets">5</span><span class="fn-backref">(<a href="#id7">1</a>,<a href="#id12">2</a>)</span></dt>
<dd><p>Daniel Garijo, Yolanda Gil, and Óscar Corcho. Abstract, link, publish, exploit: an end to end framework for workflow sharing. <em>Future Generation Comp. Syst.</em>, 75:271–283, 2017. <a class="reference external" href="https://doi.org/10.1016/j.future.2017.01.008">doi:10.1016/j.future.2017.01.008</a>.</p>
</dd>
<dt class="label" id="id2724"><span class="brackets"><a class="fn-backref" href="#id8">6</a></span></dt>
<dd><p>Deborah McGuinness, Timothy Lebo, and Satya Sahoo. PROV-o: the PROV ontology. W3C Recommendation, W3C, April 2013. URL: <a class="reference external" href="http://www.w3.org/TR/2013/REC-prov-o-20130430/">http://www.w3.org/TR/2013/REC-prov-o-20130430/</a>.</p>
</dd>
<dt class="label" id="id2725"><span class="brackets">7</span><span class="fn-backref">(<a href="#id9">1</a>,<a href="#id11">2</a>)</span></dt>
<dd><p>Daniel Garijo and Yolanda Gil. Augmenting PROV with Plans in P-PLAN: scientific processes as linked data. In <em>Proceedings of the 2nd International Workshop on Linked Science</em>, volume 951 of CEUR Workshop Proceedings. 2012. URL: <a class="reference external" href="http://oa.upm.es/19478/">http://oa.upm.es/19478/</a>.</p>
</dd>
<dt class="label" id="id2723"><span class="brackets"><a class="fn-backref" href="#id13">8</a></span></dt>
<dd><p>Riccardo Albertoni, Monica De Martino, and Alfonso Quarati. Documenting context-based quality assessment of controlled vocabularies. <em>IEEE Trans. Emerg. Top. Comput.</em>, 9(1):144–160, 2021. URL: <a class="reference external" href="https://doi.org/10.1109/TETC.2018.2865094">https://doi.org/10.1109/TETC.2018.2865094</a>, <a class="reference external" href="https://doi.org/10.1109/TETC.2018.2865094">doi:10.1109/TETC.2018.2865094</a>.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Riccardo Albertoni.</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id2924"><span class="brackets"><a class="fn-backref" href="#id10">1</a></span></dt>
<dd><p><a class="reference external" href="http://www.opmw.org/model/OPMW/">http://www.opmw.org/model/OPMW/</a></p>
</dd>
</dl>
</div>
</div>
<span id="document-Accountability/L3.Continuous_performance_monitoring"></span><div class="tex2jax_ignore mathjax_ignore section" id="continuous-performance-monitoring">
<h5>Continuous Performance Monitoring<a class="headerlink" href="#continuous-performance-monitoring" title="Permalink to this headline">¶</a></h5>
<div class="section" id="in-brief">
<h6>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p><strong>Continuous performance monitoring</strong> is the activity to track, log and monitor over time the behaviour and the performance of Artificial Intelligence and Machine Learning models. This activity is particularly relevant after in-production deployment in order to detect any performance drifts and outages of the model.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>Monitoring the live functioning of a produtionalised ML/AI model or
system is an emergent topic that is gaining increasing attention as more
and more methods are being deployed in industrial, commercial and public
sectors. As any other piece of software, any tool based on AI/ML needs
to be maintained over time, for fixing bugs and ensuring quality. ML
models and systems require specific strategies that take into account
their nature of learning from data.</p>
<p>Idealistically, the behaviour of ML models trained on sample
well-curated data is expected to generalise on new, unseen data in the
post-deployment phase. Nonetheless, this happens rarely in practice, and
a model’s performance assessed live is often different from the
performance evaluated offline during development. Furthermore, it is
well-known that the performance of an AI model or system degrades over
time.</p>
<p>Several phenomena have been identified as drivers of this decay. The
input data fed into the ML model may contain unexpected patterns not
present in the training datasets. Moreover, the characteristics of data
may change over time, causing that the relationships at the core of the
ML methods do not stand valid any more.</p>
<p>This phenomenon, termed <em>concept</em> or <em>model drift</em> <span id="id1">[<a class="reference internal" href="#id2762">1</a>]</span>,
can lead the model to make wrong predictions. Additionally, if the
nature (or distribution) of the input data become vastly different with
respect to those used for training, the performance can even drop below
acceptance. This phenomenon is known as <em>covariate shift</em>
<span id="id2">[<a class="reference internal" href="#id2763">2</a>]</span>. Performance degradation can also result from the impact
that the same deployed ML model may have on the decision process that it
supports. The ML model may influence other elements involved in the
decision or induce an overall change in the phenomenon that is being
modelled, which was not taken into account during training.</p>
<p>Overall, after its deployment, an ML method can come across several
difficulties and changes that the level of efforts and skills needed in
its maintenance could be an order of magnitude higher than that needed
in model building.</p>
<p>Given these concerns, several strategies and best practices have been
investigated to monitor the behaviour of ML methods after deployment,
also in relation to any consequence the methods can have. The first work
published in 2015 described the various challenges that ML methods raise
after deployment in relation to data dependencies, model complexity,
reproducibility, testing, and changes in the external world
<span id="id3">[<a class="reference internal" href="#id2764">3</a>]</span>. After that, several methods have been presented in
the literature, focusing specifically on data <span id="id4">[<a class="reference internal" href="#id2765">4</a>]</span>, on
the role of humans in ML deployment <span id="id5">[<a class="reference internal" href="#id2766">5</a>]</span>, on testing
strategies <span id="id6">[<a class="reference internal" href="#id2767">6</a>]</span>, or the definition of a general framework to
track ML methods in their live functioning (e.g., pipelines, datasets,
execution configurations, code and human actions) <span id="id7">[<a class="reference internal" href="#id2768">7</a>]</span>.</p>
<p>Overall, the best practices, promoted also from industrial actors
<span id="id8">[<a class="reference internal" href="#id2770">8</a>, <a class="reference internal" href="#id2769">9</a>]</span>, include a continuous monitoring of the
ML system to assess its quality and “vitality”. Various types of metrics
are suggested in this respect, focusing mainly on performance
evaluation. The idea is to detect changes in the behaviour and then act
via re-training or implementing an active learning approach (when
reinforcement learning is adopted), so as to rectify any wrong conduct.
It should be noted that model maintenance can be seen as nurturing the
model, as it can take advantage of the new knowledge coming from the
real-setting scenario, thus it can produce an improvement of the
original version released.</p>
<p>Monitoring and maintenance can be performed in a <em>proactive</em> or
<em>reactive</em> fashion. Proactive monitoring works to identify the input
samples that deviate significantly from the patterns seen in the
training phase and to analyse them more in detail to understand any
drifts. The reactive approach entails detecting a wrong output and
identifying its causes, so as to understand how the method can be
rectified.</p>
<p>The Continuous Delivery <span id="id9">[<a class="reference internal" href="#id2771">10</a>]</span> and DevOps <span id="id10">[<a class="reference internal" href="#id2772">11</a>]</span>
approaches have been also proposed to better manage the risks of
releasing changes to Machine Learning applications and, then, do them in
a safe and reliable way.</p>
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p id="id11"><dl class="citation">
<dt class="label" id="id2762"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Alexey Tsymbal. The problem of concept drift: definitions and related work. 2004.</p>
</dd>
<dt class="label" id="id2763"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Masashi Sugiyama and Motoaki Kawanabe. <em>Machine Learning in Non-Stationary Environments</em>. MIT Press, 2012. ISBN 9780262017091.</p>
</dd>
<dt class="label" id="id2764"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-François Crespo, and Dan Dennison. Hidden technical debt in machine learning systems. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, <em>Advances in Neural Information Processing Systems</em>, volume 28. Curran Associates, Inc., 2015. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf">https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf</a>.</p>
</dd>
<dt class="label" id="id2765"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Neoklis Polyzotis, Sudip Roy, Steven Euijong Whang, and Martin Zinkevich. Data management challenges in production machine learning. In <em>Proceedings of the 2017 ACM International Conference on Management of Data</em>, SIGMOD '17, 1723–1726. New York, NY, USA, 2017. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3035918.3054782">https://doi.org/10.1145/3035918.3054782</a>, <a class="reference external" href="https://doi.org/10.1145/3035918.3054782">doi:10.1145/3035918.3054782</a>.</p>
</dd>
<dt class="label" id="id2766"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>Ilias Flaounas. Beyond the technical challenges for deploying machine learning solutions in a software company. 2017. URL: <a class="reference external" href="https://arxiv.org/abs/1708.02363">https://arxiv.org/abs/1708.02363</a>, <a class="reference external" href="https://doi.org/10.48550/ARXIV.1708.02363">doi:10.48550/ARXIV.1708.02363</a>.</p>
</dd>
<dt class="label" id="id2767"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p>Eric Breck, Shanqing Cai, Eric Nielsen, Michael Salib, and D. Sculley. What’s your ml test score? a rubric for ml production systems. In <em>NIPS 2016 Workshop (2016)</em>. 2016.</p>
</dd>
<dt class="label" id="id2768"><span class="brackets"><a class="fn-backref" href="#id7">7</a></span></dt>
<dd><p>Vinay Sridhar, Sriram Subramanian, Dulcardo Arteaga, Swaminathan Sundararaman, Drew Roselli, and Nisha Talagala. Model governance: reducing the anarchy of production ML. In <em>2018 USENIX Annual Technical Conference (USENIX ATC 18)</em>, 351–358. Boston, MA, jul 2018. USENIX Association. URL: <a class="reference external" href="https://www.usenix.org/conference/atc18/presentation/sridhar">https://www.usenix.org/conference/atc18/presentation/sridhar</a>.</p>
</dd>
<dt class="label" id="id2770"><span class="brackets"><a class="fn-backref" href="#id8">8</a></span></dt>
<dd><p>Accenture. Model behavior. nothing artificial. 2017.</p>
</dd>
<dt class="label" id="id2769"><span class="brackets"><a class="fn-backref" href="#id8">9</a></span></dt>
<dd><p>SaS. Machine learning model governance. white paper. 2019.</p>
</dd>
<dt class="label" id="id2771"><span class="brackets"><a class="fn-backref" href="#id9">10</a></span></dt>
<dd><p>Wolff. <em>A Practical Guide to Continuous Delivery</em>. Addison-Wesley, 2017. ISBN.</p>
</dd>
<dt class="label" id="id2772"><span class="brackets"><a class="fn-backref" href="#id10">11</a></span></dt>
<dd><p>Loukides. <em>MWhat is DevOps?</em> O'Reilly Media, 2012. ISBN.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Sara Colantonio.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
</div>
</div>
<span id="document-Privacy_and_Data_Governance/Privacy_and_Data_Governance"></span><div class="tex2jax_ignore mathjax_ignore section" id="privacy-and-data-governance">
<h3>Privacy and Data Governance<a class="headerlink" href="#privacy-and-data-governance" title="Permalink to this headline">¶</a></h3>
<!-- Respect for Privacy-->
<div class="section" id="in-brief">
<h4>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h4>
<p>Respect for Privacy is an ethical aspect studied in the <a href="https://tailor-network.eu/" target=_blank>TAILOR project</a> for ensuring personal data protection that is at the core of the General
Data Protection Regulation (GDPR) <span id="id1">[<a class="reference internal" href="TAILOR.html#id8">1</a>]</span>. GDPR, in its <a href="https://gdpr-info.eu/art-5-gdpr/" target=_blank>Article 5</a>, promote <em>privacy by design</em> in the form of a certain number of general principles for ensuring <em>privacy as the default</em> in the whole chain of data processing for a given task. We outline the challenges and solutions for enforcing privacy by design approaches.</p>
</div>
<div class="section" id="abstract">
<h4>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">¶</a></h4>
<p>When protecting personal data, we are faced to the dilemma of disclosing no sensitive data while learning useful information about a population.
One approach for solving this tension between privacy and utility is based on <em>data encryption</em> and consists in developping secure computation protocols able to learn models or to compute statistics on encrypted data. A lot of scientific literature <span id="id2">[<a class="reference internal" href="#id2811">2</a>]</span> <span id="id3">[<a class="reference internal" href="#id2812">3</a>]</span> <span id="id4">[<a class="reference internal" href="#id2813">4</a>]</span> <span id="id5">[<a class="reference internal" href="#id2814">5</a>]</span> has been exploring this security-based approach depending on the target computational task.
Another approach consists in conducting data analysis tasks on datasets made anonymous by the application of some <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L1.privacy_model"><span class="doc std std-doc">privacy_mechanisms</span></a>. Anonymization must no be reduced to <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.pseudonymization"><span class="doc">Pseudonymization</span></a> (see also <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.reidentification"><span class="doc">Re-identification Attack</span></a>), which is defined in GDPR <a href="https://gdpr-info.eu/art-4-gdpr/" target=_blank>Article 4</a> as “the processing
of personal data in such a way that the data cannot be attributed to a specific data subject without the use of additional information.”
Anonymization, as defined in GDPR (see <a href="https://gdpr-info.eu/recitals/no-26/" target=_blank>Recital 26</a>), refers to a process that removes any possibility of identifying a person even with additional information. In the resulting anonymized data, the connection should be completely lost between data and the individuals. Based on such a definition, anonymization is very difficult to model formally and to verify algorithmically. We will briefly survey the main privacy models and their properties, as well as the main privacy mechanisms which can be applied for enforcing the corresponding privacy properties or for providing strong guarantees of robustness to attacks.</p>
</div>
<div class="section" id="motivation-and-background">
<h4>Motivation and Background<a class="headerlink" href="#motivation-and-background" title="Permalink to this headline">¶</a></h4>
<p>Publishing datasets plays an essential role in open data research and in promoting transparency of government agencies. Unfortunately, the process of data publication can be highly risky as it may disclose individuals’ sensitive information. Hence, an essential step before publishing datasets is to remove any uniquely identifiable information from them. This is called <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.pseudonymization"><span class="doc">Pseudonymization</span></a> and consists in masking or replacing by pseudonyms values of properties that directly identify persons such as their name, address, postcode, telephone number, photograph or image, or some other unique personal characteristic.</p>
<p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.pseudonymization"><span class="doc">Pseudonymization</span></a> is not sufficient however for preserving the privacy of users. Adversaries can re-identify individuals in datasets based on common attributes called quasi-identifiers or may have prior knowledge about the users. Such side information enables them to reveal sensitive information that can cause physical, financial, and reputational harms to people.</p>
<p>Therefore, it is crucial to assess carefully <em>privacy risks</em> before the publication of datasets. Detection of privacy breaches should come with <a class="reference internal" href="TAILOR.html#document-Transparency/Transparency"><span class="doc std std-doc">explanations</span></a> that can then be used to guide the choice of the appropriate <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L1.privacy_model"><span class="doc std std-doc">anonymization mechanisms</span></a> to mitigate the detected privacy risks. <em>Anonymization</em> should provide provable guarantees for privacy properties induced by some <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L1.privacy_model"><span class="doc">Privacy Models</span></a>. <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.differential_privacy"><span class="doc">Differential Privacy</span></a> and <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.k_anonymity"><span class="doc">k-anonymity</span></a> are the two main privacy models for which <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L1.privacy_model"><span class="doc">Privacy Models</span></a> have been designed. They enjoy different properties based on the type of perturbations or transfomations applied on the data to anonymize.</p>
<p>The strength of <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.pseudonymization"><span class="doc">Pseudonymization</span></a> and anonymization techniques can be assessed by their robustness to <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L1.attacks"><span class="doc std std-doc">privacy attacks</span></a> that aim at re-identifying individuals in datasets based on common attributes called quasi-identifiers or on prior knowledge.</p>
</div>
<div class="section" id="guidelines">
<h4>Guidelines<a class="headerlink" href="#guidelines" title="Permalink to this headline">¶</a></h4>
<p><em>EDBP</em> has published several guidelines. The EDPB <a href="https://ec.europa.eu/newsroom/article29/items/611236" target=_blank>Guidelines on Data Protection Impact Assessment</a>
focus on determining whether a processing operation is likely to result in a high risk to the data subject or not. It provides guidance on how to assess data protection risks and how to carry out a data protection risk assessment.</p>
<p><em>Data minimisation</em> is a strong recommendation to limit the collection of personal information to what is directly relevant and necessary to accomplish a specified purpose, and to retain the data only for as long as is necessary to fulfil that purpose.</p>
<p>The most authoritative guideline on <a href="https://edpb.europa.eu/our-work-tools/our-documents/guidelines/guidelines-42019-article-25-data-protection-design-and" target=_blank>data protection “by design and by default”</a> outlines the data subject’s rights and freedoms and the data protection principles that are illustrated through examples of practical cases. It emphasizes the obligation for controllers to stay up to date on technological advances on handling data protection risks, and to implement and update the measures and safeguards taking into account the evolving technological landscape.</p>
</div>
<div class="section" id="software-frameworks-supporting-dimension">
<h4>Software Frameworks Supporting Dimension<a class="headerlink" href="#software-frameworks-supporting-dimension" title="Permalink to this headline">¶</a></h4>
<p>There are some practical tools that help in enanching respect for privacy and awareness, in particular definining and mitigating potential privacy risks.
This is compliant with the <a href="https://gdpr-info.eu/art-35-gdpr/" target=_blank>Data Protection Impact Assessment</a> introduced in the GDPR.</p>
<p>Risks can be identified and addressed at an early stage by analyzing how the proposed uses of personal information and technology will work in practice. We should identify the privacy and related risks, evaluate the privacy solutions and integrate them into the project plan. <!-- missing citation: Information Commissioner’s Office. Conducting privacy impact assessments code
of practice, 2017. https://ico.org.uk/media/for-organisations/documents/1595/pia-code-of-practice.pdf --></p>
<p>Currently, a lot of frameworks have implemented to manage this task. University of British Colombia provides a <a href="https://privacymatters.ubc.ca/privacy-impact-assessment" target=_blank>tool</a> for determining a project’s privacy and security risk classification. <a href="https://trustarc.com/" target=_blank>TrustArc</a> offers a consulting service for analyzing personally identifiable information, looking at risk factors and assisting in the development of policies and training programs. Information Commissioner’s Office provides a <a href="https://ico.org.uk/for-organisations/sme-web-hub/checklists/data-protection-self-assessment/" target=_blank>handy step by step guide</a> through the process of deciding whether to share personal data<a class="footnote-reference brackets" href="#ico1" id="id6">1</a><a class="footnote-reference brackets" href="#ico2" id="id7">2</a>. Also the US Department of Homeland Security implemented such <a href="https://www.dhs.gov/privacy-impact-assessments" target=_blank>decision tool</a>.</p>
<p>A (non-exhaustive) list of more practical tools includes:</p>
<ul class="simple">
<li><p><a href="https://amnesia.openaire.eu/" target=_blank>Amnesia</a>, a tool for anonymize tabular data relying on <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.k_anonymity"><span class="doc">k-anonymity</span></a> paradigm;</p></li>
<li><p><a href="https://arx.deidentifier.org/anonymization-tool/" target=_blank>AXR</a>, a tool that incorporates different <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L1.privacy_model"><span class="doc">Privacy Models</span></a>;</p></li>
<li><p><a href="https://github.com/scikit-mobility/scikit-mobility" target=_blank>Scikit-mobility</a>, a Python library for mobility analysis that includes the computation of privacy risks in such setting, based on the work presented in <span id="id8">[<a class="reference internal" href="#id2815">6</a>]</span> <span id="id9">[<a class="reference internal" href="#id2816">7</a>]</span>.</p></li>
</ul>
</div>
<div class="section" id="taxonomic-organisation-of-terms">
<h4>Taxonomic Organisation of Terms<a class="headerlink" href="#taxonomic-organisation-of-terms" title="Permalink to this headline">¶</a></h4>
<p>The <em>Respect for Privacy</em> dimension mainly regards the Data Protection. The Assessment of Privacy Risks can be performed, and two diffent strategies are available two protect the data privacy. The first one regards the application of <span class="xref myst">Anonymization Mechanisms</span>, such as <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.pseudonymization"><span class="doc">Pseudonymization</span></a>, <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.k_anonymity"><span class="doc">k-anonymity</span></a>, or <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.differential_privacy"><span class="doc">Differential Privacy</span></a>. The second strategy is Data Encryption, which is strictly related with the <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/Technical_Robustness_and_Safety"><span class="doc std std-doc">Security Dimension</span></a>.
In Fig. <a class="reference internal" href="#t3-5taxonomy35"><span class="std std-numref">27</span></a>, one can find the taxonomy proposed here. In blue, there are highlighted the possible attacks related to the various strategies, i.e., <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.reidentification"><span class="doc">Re-identification Attack</span></a>, <span class="xref std std-doc">./L2.membership</span>, and Security Attacks.</p>
<div class="figure align-center" id="t3-5taxonomy35">
<a class="reference internal image-reference" href="_images/TAILOR_fig_privacy_blue.png"><img alt="_images/TAILOR_fig_privacy_blue.png" src="_images/TAILOR_fig_privacy_blue.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">Fig. 27 </span><span class="caption-text">A possible taxonomy about solutions to the Respect for Privacy dimension.</span><a class="headerlink" href="#t3-5taxonomy35" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="main-keywords">
<h4>Main Keywords<a class="headerlink" href="#main-keywords" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L1.anonymization"><span class="doc">Data Anonymization (and Pseudonymization)</span></a>: A data subject is considered anonymous if it is <em>reasonably</em> hard to attribute his personal data to him/her.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.pseudonymization"><span class="doc">Pseudonymization</span></a>: <strong>Pseudonymisation</strong> aims to substitute one or more identifiers that link(s) the identity of an individual to its data with a surrogate value, called <strong>pseudonym</strong> or <strong>token</strong>.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L1.privacy_model"><span class="doc">Privacy Models</span></a>: There are essentially two families of models, based on different goals and mechanisms: anonymity by randomization (where the most recent paradigm is Differential Privacy) and anonymity by indistinguishability (whose most famous example is k-anonymity).</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.randomization"><span class="doc">Randomization Methods</span></a>: <strong>Randomization methods</strong> are used to perturb data in order to preserve the privacy of sensitive information.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.differential_privacy"><span class="doc">Differential Privacy</span></a>: <strong>Differential privacy</strong> implies that adding or deleting a single record does not significantly affect the result of any analysis.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L3.epsilon_DP"><span class="doc">\epsilon-Differential Privacy</span></a>: <strong><span class="math notranslate nohighlight">\(\epsilon\)</span>-Differential Privacy</strong> is the simpler form of <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.differential_privacy"><span class="doc">Differential Privacy</span></a>, where <span class="math notranslate nohighlight">\(\epsilon\)</span> represents the level of privacy guarantee.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L3.epsilon_delta_DP"><span class="doc">(\epsilon,\delta)-Differential Privacy</span></a>: A relaxed version of <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.differential_privacy"><span class="doc">Differential Privacy</span></a>, named <strong>(<span class="math notranslate nohighlight">\(\epsilon\)</span>,<span class="math notranslate nohighlight">\(\delta\)</span>)-Differential Privacy</strong>, allows a little privacy loss (<span class="math notranslate nohighlight">\(\delta\)</span>) due to a variation in the output distribution for the privacy mechanism.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.perturbation_mechanisms"><span class="doc">Achieving Differential Privacy</span></a>: Differential privacy guarantees can be provided by perturbation mechanisms aim at randomizing the output distributions of functions in order to provide privacy guarantees.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.indistinguishability"><span class="doc">Anonymity by Indistinguishability</span></a>: The family of <strong>anonymity by indistinguishability</strong> models is based on comparison among individuals present in data, and it aims to make each individual so similar as to be indistinguishable from others. They aims to produce anonymity sets, i.e., equivalence classes, having specific properties.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.k_anonymity"><span class="doc">k-anonymity</span></a>: <strong>k-anonimity</strong> (and the whole family of <strong>anonymity by indistinguishability</strong> models) is based on comparison among individuals present in data, and it aims to make each individual so similar as to be indistinguishable from at least <em>k-1</em> others.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.l_diversity"><span class="doc">l-diversity</span></a>: <strong>l-diversity</strong> aims to protect the diversity of sensitive attributes in the <strong>anonymity by indistinguishability</strong> paradigm. An anonymity set is <em>l-diverse</em> if contains at least <em>l</em> “well-represented” values for the sensitive attribute.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.t_closeness"><span class="doc">t-closeness</span></a>: <strong>t-closeness</strong> aims to maintain the distribution of sensitive attributes in the <strong>anonymity by indistinguishability</strong> paradigm, ensuring that the distance between the two distributions (the original and the private ones) should be limited by a threshold <em>t</em>.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.federated"><span class="doc">Federated Learning</span></a>: <strong>Federated Learning</strong> is a paradigm of distributed processing, where models instead of data are shared among peers.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L1.attacks"><span class="doc">Attacks on anonymization schemes</span></a>: There are a variety of attacks that involve data privacy. Some of them are very context-specific (for example, there exists attacks on partition-based algorithms, such as deFinetti Attack or Minimality Attack), while other are more general.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.reidentification"><span class="doc">Re-identification Attack</span></a>: <strong>Re-identification attack</strong> aims to link a certain set of data related to an individual in a dataset (which does not contain direct identifiers) to a real identity, relying on additional information.</p></li>
</ul>
</div>
<div class="section" id="bibliography">
<h4>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h4>
<p id="id10"><dl class="citation">
<dt class="label" id="id15"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>European Parliament &amp; Council. General data protection regulation. 2016. L119, 4/5/2016, p. 1–88.</p>
</dd>
<dt class="label" id="id2811"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>C. Castelluccia, A. C.-F. Chan, E. Mykletun, and G. Tsudik. Efficient and Provably Secure Aggregation of Encrypted Data in Wireless Sensor Networks. <em>ACM Transactions on Sensor Networks (TOSN)</em>, 5(3):20:1–20:36, 2009.</p>
</dd>
<dt class="label" id="id2812"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Q.-C. To, B. Nguyen, and P. Pucheral. Private and Scalable Execution of SQL Aggregates on a Secure Decentralized Architecture. <em>ACM Transactions on Database Systems (TODS)</em>, 41(3):16:1–16:43, 2016.</p>
</dd>
<dt class="label" id="id2813"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>J. Mirval, L. Bouganim, and I. Sandu Popa. Practical Fully-Decentralized Secure Aggregation for Personal Data Management Systems. In <em>International Conference on Scientific and Statistical Database Management (SSDBM)</em>, 259–264. 2021.</p>
</dd>
<dt class="label" id="id2814"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>R. Ciucanu, M. Giraud, P. Lafourcade, and L. Ye. Secure Grouping and Aggregation with MapReduce. In <em>International Joint Conference on e-Business and Telecommunications (ICETE) – Volume 2: International Conference on Security and Cryptography (SECRYPT)</em>, 514–521. 2018.</p>
</dd>
<dt class="label" id="id2815"><span class="brackets"><a class="fn-backref" href="#id8">6</a></span></dt>
<dd><p>Luca Pappalardo, Filippo Simini, Gianni Barlacchi, and Roberto Pellungrini. Scikit-mobility: a python library for the analysis, generation and risk assessment of mobility data. 2019. https://arxiv.org/abs/1907.07062.</p>
</dd>
<dt class="label" id="id2816"><span class="brackets"><a class="fn-backref" href="#id9">7</a></span></dt>
<dd><p>Francesca Pratesi, Anna Monreale, Roberto Trasarti, Fosca Giannotti, Dino Pedreschi, and Tadashi Yanagihara. PRUDEnce: a system for assessing privacy risk vs utility in data sharing ecosystems. <em>Transaction Data Privacy</em>, 11(2):139–167, 2018.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Marie-Christine Rousset, Tristan Allard, and Francesca Pratesi.</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="ico1"><span class="brackets"><a class="fn-backref" href="#id6">1</a></span></dt>
<dd><p><a class="reference external" href="https://ico.org.uk/for-organisations/sme-web-hub/checklists/data-protection-self-assessment/data-sharing-and-subject-access-checklist/">https://ico.org.uk/for-organisations/sme-web-hub/checklists/data-protection-self-assessment/data-sharing-and-subject-access-checklist/</a></p>
</dd>
<dt class="label" id="ico2"><span class="brackets"><a class="fn-backref" href="#id7">2</a></span></dt>
<dd><p><a class="reference external" href="https://ico.org.uk/for-organisations/guide-to-data-protection/ico-codes-of-practice/data-sharing-a-code-of-practice/">https://ico.org.uk/for-organisations/guide-to-data-protection/ico-codes-of-practice/data-sharing-a-code-of-practice/</a></p>
</dd>
</dl>
</div>
<div class="toctree-wrapper compound">
<span id="document-Privacy_and_Data_Governance/L1.anonymization"></span><div class="tex2jax_ignore mathjax_ignore section" id="data-anonymization-and-pseudonymization">
<h4>Data Anonymization (and Pseudonymization)<a class="headerlink" href="#data-anonymization-and-pseudonymization" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p>A data subject is considered anonymous if it is <em>reasonably</em> hard to attribute his personal data to him/her.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>A data subject is considered anonymous if it is <em>reasonably</em> hard to attribute his personal data to him/her. What “reasonably” actually means depends both on the context and on the requirements given by data respondents. Both the identity of a subject and other information related to him/her are considered in the context of anonymity, for example sensitive information regarding health, religion, political tendencies and, more in general, any kind of information that can somehow distinguish the subject from others.
The European General Data Protection Regulation (GDPR <span id="id1">[<a class="reference internal" href="TAILOR.html#id8">1</a>]</span>) defines anonymous data as</p>
<blockquote>
<div><p>“[…] information which does not relate to an identified or identifiable natural person or to personal data rendered anonymous in such a manner that the data subject is not or no longer identifiable”.</p>
</div></blockquote>
<p>Thus, data to have this property has to be deprived of all distinctive elements of a person, i.e., those elements that permit to identify both directly or indirectly that person in the data. Since anonymous data does not enable <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.reidentification"><span class="doc std std-doc">re-identification</span></a> of data subjects, even with the use of additional information, this type of data is not subject to the privacy regulations since it is not considered personal data.</p>
<p>Note that this is process is absolutely different from removing the direct identifiers only (e.g., name, surname, social security number). This process is called <em>de-identification</em>, and it can be subjected to privacy leaks, such as <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.reidentification"><span class="doc std std-doc">re-identification</span></a>. The process of substitute a set of direct identifiers with a surrogate value, or psudonym, is called <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.pseudonymization"><span class="doc">Pseudonymization</span></a>. It suffers from the same weaknesses of de-identification.
Nevertheless, the GDPR strongly advocates the application of <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.pseudonymization"><span class="doc">Pseudonymization</span></a>:</p>
<blockquote>
<div><p>“The application of pseudonymisation to personal data can reduce the risks to the data subjects concerned and help controllers and processors to meet their data-protection obligations” (<a href="https://gdpr-info.eu/recitals/no-28/" target=_blank>Recital 28</a>).</p>
</div></blockquote>
<blockquote>
<div><p>“In order to be able to demonstrate compliance with this Regulation, the controller should adopt internal policies and implement measures […]. Such measures could consist, inter alia, of […] pseudonymising personal data as soon as possible […]” (<a href="https://gdpr-info.eu/recitals/no-78/" target=_blank>Recital 78</a>).</p>
</div></blockquote>
<blockquote>
<div><p>“The further processing of personal data for archiving purposes in the public interest, scientific or historical research purposes or statistical purposes is to be carried out […] provided that appropriate safeguards exist (such as, for instance, pseudonymisation of the data)” (<a href="https://gdpr-info.eu/recitals/no-156/" target=_blank>Recital 156</a>).</p>
</div></blockquote>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id2"><dl class="citation">
<dt class="label" id="id7"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>European Parliament &amp; Council. General data protection regulation. 2016. L119, 4/5/2016, p. 1–88.</p>
</dd>
</dl>
</p>
<hr class="docutils" />
<p>This entry was readapted from <em>Comandé et al. Elgar Encyclopedia of Law and Data Science. Edward Elgar Publishing (2022) ISBN: 978 1 83910 458 9</em> by Francesca Pratesi, Roberto Pellungrini, and Anna Monreale.</p>
</div>
<div class="toctree-wrapper compound">
<span id="document-Privacy_and_Data_Governance/L2.pseudonymization"></span><div class="tex2jax_ignore mathjax_ignore section" id="pseudonymization">
<h5>Pseudonymization<a class="headerlink" href="#pseudonymization" title="Permalink to this headline">¶</a></h5>
<div class="section" id="in-brief">
<h6>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p><strong>Pseudonymisation</strong> aims to substitute one or more identifiers that link(s) the identity of an individual to its data with a surrogate value, called <strong>pseudonym</strong> or <strong>token</strong>.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>To preserve a subject’s privacy, one of the most basic methodology is to
de-couple the identity of said subject from its data. This is process is
called pseudonymisation. The typical practical approach to achieve
pseudonymity is to detect which attributes in the data may reveal the
subject’s identity, called <em>personal identifiers</em>, and substitute them
with some other value.</p>
<p>However, re-identification may be needed in certain cases (for example, to contact data subject for further questions), therefore
personal identifiers are often maintained for re-associating subject and
identity. This association should be secured and inaccessible to anybody
having access tho the pseudonymised data, so that protection is
guaranteed.</p>
<p>Following the description in the <a href="https://gdpr-info.eu/art-4-gdpr/" target=_blank>Article 4(5)</a> of the
European General Data Protection Regulation (GDPR) <span id="id1">[<a class="reference internal" href="TAILOR.html#id8">1</a>]</span>:</p>
<!--````{panels}
"the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organisational measures to ensure that the personal data are not attributed to an identified or identifiable natural person".
````-->
<blockquote>
<div><p>“the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organisational measures to ensure that the personal data are not attributed to an identified or identifiable natural person”.</p>
</div></blockquote>
<p>This definition indicates that the additional information needed to
actually link a subject’s identity to it’s data should be the focus of
pseudonymisation techniques. Indeed, pseudonymization reduces the risk of publishing data due to a direct re-identification (See <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.reidentification"><span class="doc">Re-identification Attack</span></a>).</p>
<p><a href="https://gdpr-info.eu/recitals/no-28/" target=_blank>Recital 28</a> of the GDPR, states that “explicit introduction of pseudonymisation in this Regulation is not intended to preclude any other measures of data protection”. <a href="https://gdpr-info.eu/art-6-gdpr/" target=_blank>Article 6(4)</a> of the GDPR also reports that pseudonymisation could be an “appropriate safeguards” and that data controller should operate “pseudonymising personal data as soon as possible” (<a href="https://gdpr-info.eu/recitals/no-78/" target=_blank>Recital 78 GDPR</a>) and implementing “appropriate technical and organisational measures, such as pseudonymisation” (<a href="https://gdpr-info.eu/art-25-gdpr/" target=_blank>Article 25(1) GDPR</a>) both at the time of the determination of the means for processing and at the time of the processing itself.</p>
<!--The goal of pseudonymization process is to reduce the risk of a direct
re-identification of the data subjects based on the published data (See {ref}`L2.reidentification`).-->
<p>The pseudonym must be distinguishable and irreversible in the absence of additional information. This means that it should not be possible to reconstruct the original value by just considering the pseudonym, i.e., there does not exist any function that computes the original value with the pseudonym as input. The correspondence between original value and pseudonym must be stored in a separate location and must be secured against data breaches. Surrogate values need also to be managed after the generation, either internally or externally. In the latter case, the institution who owns data outsources this service to a qualified (and trusted) third party.</p>
<div class="section" id="pseudonymisation-techniques">
<h7>Pseudonymisation techniques<a class="headerlink" href="#pseudonymisation-techniques" title="Permalink to this headline">¶</a></h7>
<p>There are several techniques that perform pseudonymisation. They can be
generally summarized in three main categories:</p>
<ol class="simple">
<li><p><strong>Cryptographic with secret key</strong>: these techniques use mathematical
mechanism to alter the original value through the application of a
secret <em>key</em>. This key is at the core of the mechanism: with it, the
pseudonymisation can be reversed, so the key has to be secured at
all times.</p></li>
<li><p><strong>Hash-based</strong>: these techniques use a function that, given an
identifier (composed by one or more attributes) with arbitrary
length returns a value of fixed size (e.g., size 256 bits, which correspond to
32 characters), being called hash value or message digest. The hash
function is usually a deterministic function and must be
irreversible, i.e., for any input of the function it is infeasible
to compute the inverse function from the output. Functions typically
used for hashing are <em>SHA-2</em> <span id="id2">[<a class="reference internal" href="#id41">1</a>]</span> and <em>SHA-3</em> <span id="id3">[<a class="reference internal" href="#id42">2</a>]</span>, for example
the <em>SHA3-512</em> which has output values of length 512 bits.</p></li>
<li><p><strong>Keyed-hash based</strong>: a combination of the previous techniques where
the hash function requires a key, called <em>salt</em> <span id="id4">[<a class="reference internal" href="#id43">3</a>]</span>, to compute
its output. This is generally considered a more robust approach that
simple hashing. Varying the key, the same data subject’s identifier
can be translated in several different pseudonyms. In cryptography
literature, these are referred to as <em>message authentication codes</em>
<span id="id5">[<a class="reference internal" href="#id44">4</a>]</span>. This family of techniques is more robust against some
brute-force attacks, especially if the salt is changed sufficiently
often.</p></li>
<li><p><strong>Keyed-hash function with deletion</strong>: equal to the previous one,
but after the generation of the pseudonym, the correspondence table
is deleted, i.e., we cannot associate again pseudonyms to personal
identifiers.</p></li>
<li><p><strong>Tokenization</strong>: the idea of tokenization is to substitute the
subjects’ identifiers with a token generated with some cryptographic
methods. However, tokenization is a non-mathematical approach: data
is replaced but the type or length is not altered. Typically
knowledge of a token has no usefulness for a third party. Another
difference is that tokenization is fast and can be done with few
computational resources.<span id="id6">[<a class="reference internal" href="#id45">5</a>]</span></p></li>
</ol>
</div>
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p id="id7"><dl class="citation">
<dt class="label" id="id41"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>Information Technology Laboratory, National Institute of Standards and Technology. Secure hash standard (SHS). URL: <a class="reference external" href="https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.180-4.pdf">https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.180-4.pdf</a> (visited on 2022-05-02).</p>
</dd>
<dt class="label" id="id42"><span class="brackets"><a class="fn-backref" href="#id3">2</a></span></dt>
<dd><p>Information Technology Laboratory, National Institute of Standards and Technology. SHA-3 standard: permutation-based hash and extendable-output functions. URL: <a class="reference external" href="https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.202.pdf">https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.202.pdf</a> (visited on 2022-05-02).</p>
</dd>
<dt class="label" id="id43"><span class="brackets"><a class="fn-backref" href="#id4">3</a></span></dt>
<dd><p>Information Technology Laboratory, National Institute of Standards and Technology. The keyed-hash message authentication code (hmac). URL: <a class="reference external" href="https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.198-1.pdf">https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.198-1.pdf</a> (visited on 2022-05-02).</p>
</dd>
<dt class="label" id="id44"><span class="brackets"><a class="fn-backref" href="#id5">4</a></span></dt>
<dd><p>Alfred J. Menezes, Scott A. Vanstone, and Paul C. van Oorschot. <em>Handbook of Applied Cryptography</em>. CRC Press, 1996. URL: <a class="reference external" href="https://cacr.uwaterloo.ca/hac/">https://cacr.uwaterloo.ca/hac/</a> (visited on 2022-05-02).</p>
</dd>
<dt class="label" id="id45"><span class="brackets"><a class="fn-backref" href="#id6">5</a></span></dt>
<dd><p>Data Protection Working Party. Opinion 05/2014 on Anonymisation Techniques. URL: <a class="reference external" href="https://ec.europa.eu/justice/article-29/documentation/opinion-recommendation/files/2014/wp216\_en.pdf">https://ec.europa.eu/justice/article-29/documentation/opinion-recommendation/files/2014/wp216\_en.pdf</a> (visited on 2022-05-02).</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Comandé et al. Elgar Encyclopedia of Law and Data Science. Edward Elgar Publishing (2022) ISBN: 978 1 83910 458 9</em> by Francesca Pratesi, Roberto Pellungrini, and Anna Monreale.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
<span id="document-Privacy_and_Data_Governance/L1.privacy_model"></span><div class="tex2jax_ignore mathjax_ignore section" id="privacy-models">
<h4>Privacy Models<a class="headerlink" href="#privacy-models" title="Permalink to this headline">¶</a></h4>
<p>There are essentially two families of models, based on different goals and mechanisms: <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.randomization"><span class="doc std std-doc">anonymity by randomization</span></a> (where the most recent paradigm is <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.differential_privacy"><span class="doc">Differential Privacy</span></a>) and <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.indistinguishability"><span class="doc std std-doc">anonymity by indistinguishability</span></a> (whose most famous example is <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.k_anonymity"><span class="doc">k-anonymity</span></a>).</p>
<blockquote>
<div><p>This entry was written by Francesca Pratesi.</p>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-Privacy_and_Data_Governance/L2.randomization"></span><div class="tex2jax_ignore mathjax_ignore section" id="randomization-methods">
<h5>Randomization Methods<a class="headerlink" href="#randomization-methods" title="Permalink to this headline">¶</a></h5>
<div class="section" id="in-brief">
<h6>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p><strong>Randomization methods</strong> are used to perturb data in order to preserve the privacy of sensitive information.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>Randomization methods are employed to alter data in order to protect the privacy of sensitive information. They were initially used for statistical disclosure control [5] and have later been extended to the problem of privacy-preserving data mining <span id="id1">[<a class="reference internal" href="#id2791">1</a>]</span>. Randomization techniques modify the data using a noise component; from the perturbed data, it is still feasible to extract patterns and models. In the literature, there are two types of random perturbation techniques: additive random perturbation and multiplicative random perturbation.</p>
<p>In the <strong>additive random perturbation methods</strong>, the original dataset is represented by <span class="math notranslate nohighlight">\(X = {x_1 ... x_m}\)</span> and the new distorted dataset, represented by <span class="math notranslate nohighlight">\(Z = {z_1 ... z_m}\)</span>, is obtained by independently drawing a noise quantity <span class="math notranslate nohighlight">\(n_i\)</span> from a probability distribution (usually Uniform or Gaussian) and adding it to each record <span class="math notranslate nohighlight">\(x_i ∈ X\)</span>.</p>
<p>It is important to note that both the <span class="math notranslate nohighlight">\(m\)</span> instances of the probability distribution <span class="math notranslate nohighlight">\(Z\)</span> and the distribution of the noise are known, also by the attackers. The original record values cannot be easily inferred from the distorted data, while the dataset distribution can be effectively recovered using one of the methods discussed in <span id="id2">[<a class="reference internal" href="#id2791">1</a>, <a class="reference internal" href="#id2790">2</a>]</span>. Thus, individual records are not accessible, but it is possible to obtain distributions only along individual dimensions that describe the behavior of the original dataset <span class="math notranslate nohighlight">\(X\)</span>.
However, traditional algorithms (especially whether they are related to machine learning) are not suitable as they rely on statistics derived from individual records or multivariate distributions. Consequently, new (machine learning) methods must be developed to work with aggregate distributions of the data to achieve mining results.
The studies presented in <span id="id3">[<a class="reference internal" href="#id2791">1</a>, <a class="reference internal" href="#id2792">3</a>, <a class="reference internal" href="#id2793">4</a>]</span> proposed techniques based on the randomization approach to perturb data and then build classification models over randomized data. In <span id="id4">[<a class="reference internal" href="#id2790">2</a>]</span>, Agrawal and Aggarwal demonstrate that the choice of the reconstruction algorithm impacts the accuracy of the original probability distribution. Furthermore, they propose a method that converges to the maximum likelihood estimate of the data distribution. The authors of <span id="id5">[<a class="reference internal" href="#id2792">3</a>, <a class="reference internal" href="#id2793">4</a>]</span> introduce methods to construct a Naive Bayesian classifier over perturbed data. Randomization approaches are also applied to solve the privacy-preserving association rules mining problem, as seen in {cite]<code class="docutils literal notranslate"><span class="pre">Evfimievski,Rizvi</span></code>.</p>
<p>For privacy-preserving data mining and machine learning, <strong>multiplicative random perturbation techniques</strong> can also be utilized. The primary techniques of multiplicative perturbation are based on the work presented in <span id="id6">[<a class="reference internal" href="#id2796">5</a>]</span>.</p>
<div class="section" id="weakness-of-randomization-models">
<h7>Weakness of randomization models<a class="headerlink" href="#weakness-of-randomization-models" title="Permalink to this headline">¶</a></h7>
<p>Unfortunately, the main issue of randomization methods is that they are not secure against attacks with prior knowledge. In fact, in the work <span id="id7">[<a class="reference internal" href="#id2797">6</a>]</span>, Kargupta et al. show that the original data matrix can be reconstructed from a randomized data matrix using the random matrix-based spectral filtering technique.</p>
<p>In order to overcome this problem, <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.differential_privacy"><span class="doc">Differential Privacy</span></a>, which is a novel schema of randomization algorithms, has been proposed.</p>
</div>
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p id="id8"><dl class="citation">
<dt class="label" id="id2791"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>,<a href="#id3">3</a>)</span></dt>
<dd><p>Rakesh Agrawal and Ramakrishnan Srikant. Privacy-preserving data mining. In <em>SIGMOD Conferences</em>. 2000.</p>
</dd>
<dt class="label" id="id2790"><span class="brackets">2</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id4">2</a>)</span></dt>
<dd><p>Dakshi Agrawal and Charu C. Aggarwal. On the design and quantification of privacy preserving data mining algorithms. In <em>20th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems</em>. 2001.</p>
</dd>
<dt class="label" id="id2792"><span class="brackets">3</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id5">2</a>)</span></dt>
<dd><p>Justin Z. Zhan, Stan Matwin, and LiWu Chang. Privacy-preserving collaborative association rule mining. In <em>DBSec</em>. 2005.</p>
</dd>
<dt class="label" id="id2793"><span class="brackets">4</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id5">2</a>)</span></dt>
<dd><p>Peng Zhang, Yunhai Tong, Shiwei Tang, and Dongqing Yang. Privacy preserving naive bayes classification. In <em>ADMA</em>. 2005.</p>
</dd>
<dt class="label" id="id2796"><span class="brackets"><a class="fn-backref" href="#id6">5</a></span></dt>
<dd><p>J. Lindenstrauss and W. Johnson. Extensions of lipshitz mapping into hilbert space. <em>Contemporary Math.</em>, 1984.</p>
</dd>
<dt class="label" id="id2797"><span class="brackets"><a class="fn-backref" href="#id7">6</a></span></dt>
<dd><p>Hillol Kargupta, Souptik Datta, Qi Wang, and Krishnamoorthy Sivakumar. On the privacy preserving properties of random data perturbation techniques. In <em>3rd IEEE International Conference on Data Mining</em>. 2003.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Francesca Pratesi.</p>
</div></blockquote>
</div>
</div>
<span id="document-Privacy_and_Data_Governance/L2.differential_privacy"></span><div class="tex2jax_ignore mathjax_ignore section" id="differential-privacy">
<h5>Differential Privacy<a class="headerlink" href="#differential-privacy" title="Permalink to this headline">¶</a></h5>
<div class="section" id="in-brief">
<h6>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p><strong>Differential privacy</strong> implies that adding or deleting a single record does not significantly affect the result of any analysis.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<div class="section" id="the-family-of-differential-privacy-models">
<h7>The Family of Differential Privacy Models<a class="headerlink" href="#the-family-of-differential-privacy-models" title="Permalink to this headline">¶</a></h7>
<p>Differential privacy is a prominent family of privacy-preserving data
publishing models (see <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L1.privacy_model"><span class="doc">Privacy Models</span></a>). It comprehends
privacy as the ability to set a limit on the impact of any single
individual on the outputs of the function that produces the
information to publish (computation, e.g., of a set of statistics, of
a machine learning model, of generated synthetic data). In other
words, a differentially private function promises to each individual
that its outputs will be more or less the same whether the
individual’s data is input by the function or not. Differential
privacy models all share this common intuitive goal but they differ in
the way they formalize it - for example, on the quantification of the
impact of an individual or on the tolerance to possible failures of
the guarantees (though improbable). They usually exhibit properties
that have been identified as key requirements to privacy models. <!--(see {doc}`./L2.privmod_properties` for details).--></p>
</div>
<div class="section" id="achieving-differential-privacy">
<h7>Achieving Differential Privacy<a class="headerlink" href="#achieving-differential-privacy" title="Permalink to this headline">¶</a></h7>
<p>Designing a function that satisfies differential privacy often boils
down to carefully combining basic perturbation mechanisms (such as,
e.g., the Laplace mechanism) and to demonstrating formally either that
data is only accessed through a differentially private function
(leveraging the safety under post-processing and the
self-composability properties), or that the output distribution of the
function complies with the targeted differential privacy model
(through, e.g., randomness alignments). We refer the interested reader
to <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.perturbation_mechanisms"><span class="doc">Achieving Differential Privacy</span></a> for more information.</p>
</div>
<div class="section" id="an-expanding-universe">
<h7>An expanding universe<a class="headerlink" href="#an-expanding-universe" title="Permalink to this headline">¶</a></h7>
<p>The seminal differential privacy models were proposed in the
mid-2000’s and include <span class="math notranslate nohighlight">\(\epsilon\)</span>-differential privacy (see
<a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L3.epsilon_DP"><span class="doc std std-doc"><span class="math notranslate nohighlight">\(\epsilon\)</span>-Differential Privacy</span></a>) or <span class="math notranslate nohighlight">\((\epsilon,
\delta)\)</span>-differential privacy (see <!--{doc}`./L3.epsilon_delta_DP` -
--> <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L3.epsilon_delta_DP"><span class="doc std std-doc">(<span class="math notranslate nohighlight">\(\epsilon\)</span>,<span class="math notranslate nohighlight">\(\delta\)</span>)-Differential
Privacy</span></a>). The number of differential
privacy models has grown fastly over the years (more than 200
extensions or variants have been reported in a 2020 survey
paper). Differential privacy is often considered in the academia as a
<em>de facto</em> standard for privacy-preserving data publishing and has
earned the original authors the prestigious Gödel Prize
in 2017. Famous organizations (e.g., the US Census Bureau) and
companies (e.g., Google, Apple, LinkedIn, Microsoft) have launched
ambitious real-life applications of differential privacy.</p>
</div>
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p>The seminal differential privacy models were introduced in
<span id="id1">[<a class="reference internal" href="TAILOR.html#id2798">1</a>]</span>, <span id="id2">[<a class="reference internal" href="TAILOR.html#id2797">2</a>]</span>,
and <span id="id3">[<a class="reference internal" href="TAILOR.html#id2800">1</a>]</span>. Differential privacy is thoroughly
introduced in <span id="id4">[<a class="reference internal" href="#id2800">4</a>]</span> and numerous variants and
extensions are surveyed in <span id="id5">[<a class="reference internal" href="#id2803">5</a>]</span>. The book
<span id="id6">[<a class="reference internal" href="#id2798">6</a>]</span> surveys differential privacy
techniques related to database queries.</p>
<p id="id7"><dl class="citation">
<dt class="label" id="id2802"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Cynthia Dwork. Differential privacy. In <em>ICALP</em>. 2006.</p>
</dd>
<dt class="label" id="id2801"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noise to sensitivity in private data analysis. In <em>TCC</em>. 2006.</p>
</dd>
<dt class="label" id="id2804"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: privacy via distributed noise generation. In <em>EUROCRYPT</em>. 2006.</p>
</dd>
<dt class="label" id="id2800"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. <em>Found. Trends Theor. Comput. Sci.</em>, 9:211–407, 2014.</p>
</dd>
<dt class="label" id="id2803"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>Damien Desfontaines and Balázs Pejó. Sok: differential privacies. <em>Proceedings on Privacy Enhancing Technologies</em>, 2020:288 – 313, 2020.</p>
</dd>
<dt class="label" id="id2798"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p>Joseph P. Near and Xi He. Differential privacy for databases. <em>Foundations and Trends in Databases</em>, 11:109–225, 2021.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Tristan Allard.</p>
</div></blockquote>
</div>
<div class="toctree-wrapper compound">
<span id="document-Privacy_and_Data_Governance/L3.epsilon_DP"></span><div class="tex2jax_ignore mathjax_ignore section" id="epsilon-differential-privacy">
<h6><span class="math notranslate nohighlight">\(\epsilon\)</span>-Differential Privacy<a class="headerlink" href="#epsilon-differential-privacy" title="Permalink to this headline">¶</a></h6>
<p><em>Synonyms</em>: <span class="math notranslate nohighlight">\(\epsilon\)</span>-indistinguishability.</p>
<div class="section" id="in-brief">
<h7>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h7>
<p><strong><span class="math notranslate nohighlight">\(\epsilon\)</span>-Differential Privacy</strong> is the simpler form of <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.differential_privacy"><span class="doc">Differential Privacy</span></a>, where <span class="math notranslate nohighlight">\(\epsilon\)</span> represents the level of privacy guarantee.</p>
</div>
<div class="section" id="more-in-detail">
<h7>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h7>
<div class="section" id="the-statistical-databases-context">
<h8>The Statistical Databases Context<a class="headerlink" href="#the-statistical-databases-context" title="Permalink to this headline">¶</a></h8>
<p>The seminal <span class="math notranslate nohighlight">\(\epsilon\)</span>-differential privacy model lays down the basic
notions related to differential privacy by formalizing the intuitive
requirement that <em>the possible impact of any single individual on the
output of a differentially private function must be limited</em> (see
<a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.differential_privacy"><span class="doc">Differential Privacy</span></a>). The initial <span class="math notranslate nohighlight">\(\epsilon\)</span>-differential
privacy model focuses on the context of statistical databases: the
private dataset is a table <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> in which each individual
contributes at most one record, the system answers interactively to a
sequence of statistical queries over <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> (e.g., a sequence
of queries containing counts, sums, averages, <em>etc</em>), and the
<span class="math notranslate nohighlight">\(\epsilon\)</span>-differential privacy model aims at limiting the information
leakage about the private dataset.</p>
<!--(Remember that disclosing raw aggregates might lead at least to membership inference attacks and to reconstruction attacks - see {doc}`./L2.membership` and {doc}`./L2.reconstruction`.)-->
</div>
<div class="section" id="formalizing-differential-privacy">
<h8>Formalizing Differential Privacy<a class="headerlink" href="#formalizing-differential-privacy" title="Permalink to this headline">¶</a></h8>
<p>In a nutshell, the <span class="math notranslate nohighlight">\(\epsilon\)</span>-differential privacy model requires that
the presence/absence of any <em>possible</em> individual does not shift any
output probability by more than a factor of <span class="math notranslate nohighlight">\(e^\epsilon\)</span>. More
precisely, a <em>random function</em> <span class="math notranslate nohighlight">\(\mathtt{f}\)</span> with range <span class="math notranslate nohighlight">\(\mathcal{O}\)</span>
satisfies <span class="math notranslate nohighlight">\(\epsilon\)</span>-differential privacy if and only if for all
possible pairs of datasets (<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, <span class="math notranslate nohighlight">\(\mathcal{D}'\)</span>) such that
<span class="math notranslate nohighlight">\(\mathcal{D}'\)</span> is <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> with <em>one record more or one record
less</em>, and for all <span class="math notranslate nohighlight">\(\mathcal{S} \subseteq \mathcal{O}\)</span>, then it holds
that: <span class="math notranslate nohighlight">\(\mathtt{Pr} [ \mathtt{f} ( \mathcal{D} ) \in \mathcal{S} ] \leq
e^\mathbf{\epsilon} \times \mathtt{Pr} [ \mathtt{f} ( \mathcal{D}' )
\in \mathcal{S} ]\)</span> where <span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span> is the privacy parameter.</p>
<p>Let us comment the above definition. First, the function <span class="math notranslate nohighlight">\(\mathtt{f}\)</span>
can be any arbitrary function, including the usual statistical
functions (e.g., counts, sums) but not restricted to them. Second, the
pairs of datasets whose output distributions must not differ too much
are taken from the full space of the possible datasets; they are not
derived from the actual private dataset. Third, the impact of an
individual is defined based on the presence (or absence) of his/her
record in (or from) any possible dataset. Pairs of datasets that
differ on the presence/absence of a single record are called
<em>neighboring datasets</em>. Note that variants might exist (e.g., by
considering that neighboring datasets are datasets that differ on the
value of a single row). Fourth, the value of <span class="math notranslate nohighlight">\(\epsilon\)</span> sets the
tolerance of the model to the possible impacts of individuals on the
output of <span class="math notranslate nohighlight">\(\mathtt{f}\)</span>: the lower the <span class="math notranslate nohighlight">\(\epsilon\)</span> the more stringent
the requirement. Common values range from <span class="math notranslate nohighlight">\(\epsilon=0.01\)</span> to
<span class="math notranslate nohighlight">\(\epsilon=10\)</span>.</p>
<p>Please see <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.perturbation_mechanisms"><span class="doc">Achieving Differential Privacy</span></a> for a synthesis of
<strong>how</strong> common functions can be adapted in order to satisfy
<span class="math notranslate nohighlight">\(\epsilon\)</span>-differential privacy.</p>
</div>
<div class="section" id="self-composability-and-safety-under-post-processing">
<h8>Self-Composability and Safety Under Post-Processing<a class="headerlink" href="#self-composability-and-safety-under-post-processing" title="Permalink to this headline">¶</a></h8>
<p>The <span class="math notranslate nohighlight">\(\epsilon\)</span>-differential privacy model is self-composable as
follows. The <em>parallel composition</em> of two functions, respectively
satisfying <span class="math notranslate nohighlight">\(\epsilon_1\)</span>-differential privacy and
<span class="math notranslate nohighlight">\(\epsilon_2\)</span>-differential privacy, satisfies <span class="math notranslate nohighlight">\(\max (\epsilon_1,
\epsilon_2)\)</span>-differential privacy. Their <em>sequential composition</em>
satisfies <span class="math notranslate nohighlight">\((\epsilon_1 + \epsilon_2)\)</span>-differential privacy. The
<span class="math notranslate nohighlight">\(\epsilon\)</span>-differential privacy model is as well convex and safe under
post-processing.</p>
<!-- We refer to {doc}`./L2.privmod_properties` for the definitions of these properties.-->
</div>
</div>
<div class="section" id="bibliography">
<h7>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h7>
<p>The <span class="math notranslate nohighlight">\(\epsilon\)</span>-differential privacy model was introduced in
<span id="id1">[<a class="reference internal" href="#id2798">1</a>]</span> and the
<span class="math notranslate nohighlight">\(\epsilon\)</span>-indistinguishability model in
<span id="id2">[<a class="reference internal" href="#id2797">2</a>]</span>.</p>
<p id="id3"><dl class="citation">
<dt class="label" id="id2798"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Cynthia Dwork. Differential privacy. In <em>ICALP</em>. 2006.</p>
</dd>
<dt class="label" id="id2797"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noise to sensitivity in private data analysis. In <em>TCC</em>. 2006.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Tristan Allard.</p>
</div></blockquote>
</div>
</div>
<span id="document-Privacy_and_Data_Governance/L3.epsilon_delta_DP"></span><div class="tex2jax_ignore mathjax_ignore section" id="epsilon-delta-differential-privacy">
<h6>(<span class="math notranslate nohighlight">\(\epsilon\)</span>,<span class="math notranslate nohighlight">\(\delta\)</span>)-Differential Privacy<a class="headerlink" href="#epsilon-delta-differential-privacy" title="Permalink to this headline">¶</a></h6>
<div class="section" id="in-brief">
<h7>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h7>
<p>A relaxed version of <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.differential_privacy"><span class="doc">Differential Privacy</span></a>, named <strong>(<span class="math notranslate nohighlight">\(\epsilon\)</span>,<span class="math notranslate nohighlight">\(\delta\)</span>)-Differential Privacy</strong>,
allows a little privacy loss (<span class="math notranslate nohighlight">\(\delta\)</span>) due to a variation in the output distribution for the privacy mechanism.</p>
</div>
<div class="section" id="more-in-detail">
<h7>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h7>
<div class="section" id="relaxing-epsilon-differential-privacy">
<h8>Relaxing <span class="math notranslate nohighlight">\(\epsilon\)</span>-Differential Privacy<a class="headerlink" href="#relaxing-epsilon-differential-privacy" title="Permalink to this headline">¶</a></h8>
<p>The (<span class="math notranslate nohighlight">\(\epsilon\)</span>,<span class="math notranslate nohighlight">\(\delta\)</span>)-differential privacy model is a common
relaxation of <span class="math notranslate nohighlight">\(\epsilon\)</span>-differential privacy. Under the
<span class="math notranslate nohighlight">\(\epsilon\)</span>-differential privacy model, the probabilities that the
function <span class="math notranslate nohighlight">\(\mathtt{f}\)</span> outputs the same output when computed over
neighboring datasets are allowed to diverge up to an <span class="math notranslate nohighlight">\(e^\epsilon\)</span>
factor. The (<span class="math notranslate nohighlight">\(\epsilon\)</span>,<span class="math notranslate nohighlight">\(\delta\)</span>)-differential privacy model
additionally tolerates the two probabilities to diverge by a small
additional quantity, denoted <span class="math notranslate nohighlight">\(\delta\)</span>.  <!-- TODO : discuss why it is
useful (utility). --></p>
<p>This leads to revisiting the formal definition of
<span class="math notranslate nohighlight">\(\epsilon\)</span>-differential privacy as follows. A <em>random function</em>
<span class="math notranslate nohighlight">\(\mathtt{f}\)</span> with range <span class="math notranslate nohighlight">\(\mathcal{O}\)</span> satisfies (<span class="math notranslate nohighlight">\(\epsilon\)</span>,
<span class="math notranslate nohighlight">\(\delta\)</span>)-differential privacy if and only if for all possible pairs
of datasets (<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, <span class="math notranslate nohighlight">\(\mathcal{D}'\)</span>) such that <span class="math notranslate nohighlight">\(\mathcal{D}'\)</span>
is <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> with one record more or one record less, and for all
<span class="math notranslate nohighlight">\(\mathcal{S} \subseteq \mathcal{O}\)</span>, then it holds that: <span class="math notranslate nohighlight">\(\mathtt{Pr}
[ \mathtt{f} ( \mathcal{D} ) \in \mathcal{S} ] \leq
e^\mathbf{\epsilon} \times \mathtt{Pr} [ \mathtt{f} ( \mathcal{D}' )
\in \mathcal{S} ] + \delta\)</span> where <span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span> and <span class="math notranslate nohighlight">\(\delta \geq 0\)</span> are
the privacy parameters. When <span class="math notranslate nohighlight">\(\delta&gt;0\)</span>,
(<span class="math notranslate nohighlight">\(\epsilon\)</span>,<span class="math notranslate nohighlight">\(\delta\)</span>)-differential privacy is also called <em>approximate
differential privacy</em>.</p>
</div>
</div>
<div class="section" id="bibliography">
<h7>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h7>
<p>The (<span class="math notranslate nohighlight">\(\epsilon\)</span>,<span class="math notranslate nohighlight">\(\delta\)</span>)-differential privacy model is introduced in
<span id="id1">[<a class="reference internal" href="#id2800">1</a>]</span> and thoroughly studied in
<span id="id2">[<a class="reference internal" href="#id2793">2</a>]</span>.</p>
<p id="id3"><dl class="citation">
<dt class="label" id="id2800"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: privacy via distributed noise generation. In <em>EUROCRYPT</em>. 2006.</p>
</dd>
<dt class="label" id="id2793"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Sebastian Meiser. Approximate and probabilistic differential privacy definitions. <em>IACR Cryptol. ePrint Arch.</em>, 2018:277, 2018.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Tristan Allard.</p>
</div></blockquote>
</div>
</div>
<span id="document-Privacy_and_Data_Governance/L2.perturbation_mechanisms"></span><div class="tex2jax_ignore mathjax_ignore section" id="achieving-differential-privacy">
<h6>Achieving Differential Privacy<a class="headerlink" href="#achieving-differential-privacy" title="Permalink to this headline">¶</a></h6>
<!--# Perturbation Mechanisms-->
<div class="section" id="in-brief">
<h7>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h7>
<p>Differential privacy guarantees can be provided by perturbation mechanisms aim at randomizing the output distributions of functions in order to provide privacy guarantees.</p>
</div>
<div class="section" id="more-in-detail">
<h7>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h7>
<p>Perturbation mechanisms aim at randomizing the output distributions of
functions in order to provide privacy guarantees. We focus on the
major mechanisms able to provide differential privacy guarantees (see
<a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.differential_privacy"><span class="doc">Differential Privacy</span></a>) and concentrate on the major
mechanisms. <em>The Laplace Mechanism</em> <!--{doc}`./L3.laplace`-->
is dedicated to perturb functions
that output real values, allowing them to satisfy
<a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L3.epsilon_DP"><span class="doc std std-doc"><span class="math notranslate nohighlight">\(\epsilon\)</span>-Differential Privacy</span></a>. The <span class="xref myst">Exponential
Mechanism</span> is a generalization of the Laplace
mechanism and also allows to satisfy <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L3.epsilon_DP"><span class="doc std std-doc"><span class="math notranslate nohighlight">\(\epsilon\)</span>-differential
privacy</span></a>. The éGaussian Mechanism* <!--[Gaussian mechanism](./L3.gaussian) -->
is a variant of the Laplace mechanism, applying to functions that output
real values as well, but allowing them to satisfy the <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L3.epsilon_delta_DP"><span class="doc std std-doc"><span class="math notranslate nohighlight">\((\epsilon,\delta)\)</span>-differential privacy</span></a>
relaxation. These three mechanisms are appropriate for perturbing
centralized functions that input a full dataset (e.g., a sum
query). They are often used as basic building blocks, combined or not,
for perturbing elaborate functions. Finally, <span class="xref myst">Randomized Response
Mechanisms</span> input and output one single row
at a time (represented as a vector of bits): they are local mechanisms
and can be applied prior to the data collection.</p>
<div class="section" id="history">
<h8>History<a class="headerlink" href="#history" title="Permalink to this headline">¶</a></h8>
<p>The earliest known randomized response mechanism has been proposed in
the 1960’s (decades before differential privacy) by Warner, a
sociologist who wanted to improve the reliability of responses to
sensitive questions by letting the interviewee perturb his/her answer
in a controlled manner. Thanks to the simplicity of their
implementation and to the differential privacy guarantees that they
provide, they generate a renewed interest both from the academia and
from the industry.</p>
<!-- (see, e.g.,
https://www.chromium.org/developers/design-documents/rappor/)--> 
<p>The Laplace mechanism (resp. the Gaussian mechanism) has been proposed jointly with the seminal <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L3.epsilon_DP"><span class="doc std std-doc"><span class="math notranslate nohighlight">\(\epsilon\)</span>-differential privacy</span></a> model (resp. the <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L3.epsilon_delta_DP"><span class="doc std std-doc"><span class="math notranslate nohighlight">\((\epsilon, \delta)\)</span>-differential privacy</span></a> model). Its generalization as the Exponential mechanism was proposed the following year.</p>
</div>
</div>
<div class="section" id="bibliography">
<h7>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h7>
<p>The early randomized response mechanism was proposed in
<span id="id1">[<a class="reference internal" href="#id2805">3</a>]</span>, the Laplace mechanism in
<span id="id2">[<a class="reference internal" href="TAILOR.html#id2797">2</a>]</span> and the Exponential mechanism in
<span id="id3">[<a class="reference internal" href="#id2804">4</a>]</span>. The Gaussian mechanism was shown to
satisfy <span class="math notranslate nohighlight">\((\epsilon, \delta)\)</span>-differential privacy in
<span id="id4">[<a class="reference internal" href="TAILOR.html#id2800">1</a>]</span>. Finally,
<span id="id5">[<a class="reference internal" href="#id2806">5</a>]</span> provides an overview and an
evaluation of various randomized response mechanisms proposed before
2021.</p>
<p id="id6"><dl class="citation">
<dt class="label" id="id2800"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noise to sensitivity in private data analysis. In <em>TCC</em>. 2006.</p>
</dd>
<dt class="label" id="id2803"><span class="brackets"><a class="fn-backref" href="#id4">2</a></span></dt>
<dd><p>Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: privacy via distributed noise generation. In <em>EUROCRYPT</em>. 2006.</p>
</dd>
<dt class="label" id="id2805"><span class="brackets"><a class="fn-backref" href="#id1">3</a></span></dt>
<dd><p>Stanley L. Warner. Randomized response: a survey technique for eliminating evasive answer bias. <em>Journal of the American Statistical Association</em>, 60 309:63–6, 1965.</p>
</dd>
<dt class="label" id="id2804"><span class="brackets"><a class="fn-backref" href="#id3">4</a></span></dt>
<dd><p>Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. <em>48th Annual IEEE Symposium on Foundations of Computer Science (FOCS'07)</em>, pages 94–103, 2007.</p>
</dd>
<dt class="label" id="id2806"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>Graham Cormode, Samuel Maddock, and Carsten Maple. Frequency estimation under local differential privacy. <em>Proc. VLDB Endow.</em>, 14(11):2046–2058, 2021.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Tristan Allard.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
<span id="document-Privacy_and_Data_Governance/L2.indistinguishability"></span><div class="tex2jax_ignore mathjax_ignore section" id="anonymity-by-indistinguishability">
<h5>Anonymity by Indistinguishability<a class="headerlink" href="#anonymity-by-indistinguishability" title="Permalink to this headline">¶</a></h5>
<!-- NOTE: Table 1 is hard written in caption of Table 2 -->
<div class="section" id="in-brief">
<h6>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p>The family of <strong>anonymity by indistinguishability</strong> models is based on comparison among individuals present in data, and it aims to make each individual so similar as to be indistinguishable from others. They aims to produce anonymity sets, i.e., equivalence classes, having specific properties.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>The first paradigm of <strong>anonymity by indistinguishability</strong> models is the <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.k_anonymity"><span class="doc">k-anonymity</span></a>, where a subject is considered anonymous if an adversary cannot achieve identification of her within a set of other <em>k-1</em> subjects. This set of <em>k</em> individuals is called the <em>anonymity set</em>.</p>
<p>Unfortunately, the <em>k</em>-anonymity framework can be vulnerable in some cases. To overcome to such vulnerabilities, other variants guaranteeing different properties were designed,
In particular, the main models are the <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.l_diversity"><span class="doc">l-diversity</span></a> and the <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.t_closeness"><span class="doc">t-closeness</span></a>.</p>
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<span class="target" id="id1"></span><blockquote>
<div><p>This entry was written by Francesca Pratesi.</p>
</div></blockquote>
</div>
<div class="toctree-wrapper compound">
<span id="document-Privacy_and_Data_Governance/L2.k_anonymity"></span><div class="tex2jax_ignore mathjax_ignore section" id="k-anonymity">
<h6>k-anonymity<a class="headerlink" href="#k-anonymity" title="Permalink to this headline">¶</a></h6>
<!-- NOTE: Table 1 is hard written in caption of Table 2 -->
<div class="section" id="in-brief">
<h7>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h7>
<!--Essentially, *k*-anonimity paradigm is based on comparison among individuals present in data, and it aims to make each individual so similar as to be indistinguishable from others.-->
<p><strong>k-anonimity</strong> (and the whole family of <strong>anonymity by indistinguishability</strong> models) is based on comparison among individuals present in data, and it aims to make each individual so similar as to be indistinguishable from at least <em>k-1</em> others.</p>
</div>
<div class="section" id="more-in-detail">
<h7>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h7>
<p>One of the most common models for anonymization is <em>k</em>-anonymity, where a subject is considered anonymous if an adversary cannot achieve identification of her within a set of other <em>k-1</em> subjects. This set of <em>k</em> individuals is called the <em>anonymity set</em>. Identification, in this context, means the ability to distinguish the subject from other individuals with whom his/her data are grouped. This definition of anonymity is implicitly quantifiable as it depends from the size of the anonymity set. Clearly, a bigger anonymity set implies a better guarantee regarding indistinguishability. For example, the anonymity of a subject with an anonymity set of size 30 is better than the one that can be achieved with an anonymity set of size 5.</p>
<p>The k-anonymity framework was originally applied on relational tables <span id="id1">[<a class="reference internal" href="TAILOR.html#id2818">1</a>]</span>. The basic assumption is that attributes are partitioned in <em>quasi-identifiers</em> and <em>sensitive attributes</em> <span id="id2">[<a class="reference internal" href="TAILOR.html#id2812">2</a>]</span>. The quasi-identifiers are attributes that can be linked to external information to re-identify the individual to whom the information refers (so-called <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.reidentification"><span class="doc">Re-identification Attack</span></a>). Usually, they are publicly known or easy to obtain with a superficial knowledge of the subject: for example, age, zip-code, and gender are classic quasi identifiers. The sensitive attributes, instead, represent the information to be protected. Indeed, a dataset satisfies the property of <em>k</em>-anonymity if each released record has at least <em>k−1</em> other records also visible in the release whose values are indistinct over the quasi-identifiers.</p>
<p>In <em>k</em>-anonymity techniques, strategies such as <em>generalization</em> and <em>suppression</em> are usually applied to reduce the granularity of representation of quasi-identifiers. It is clear that these methods guarantee privacy but also reduce the accuracy of applications on the transformed data. Indeed, one of the main challenge of <em>k</em>-anonymity is to find the minimum level of changes (in terms of generalization or suppression) that allows us to guarantee high privacy and good data precision. In <code class="xref std std-numref docutils literal notranslate"><span class="pre">tab_k_anonymity_applied</span></code>, one can find an example of a privacy mitigation through generalization that allows to reach <em>3</em>-anonymity, i.e., <em>k</em>-anonymity with <em>3</em> as the minimum size of each anonymity set, starting from the situation depicted in <code class="xref std std-numref docutils literal notranslate"><span class="pre">tab_k_anonymity_original</span></code>.</p>
<table class="colwidths-auto docutils align-default" id="tab-k-anonymity-original">
<caption><span class="caption-number">Table 1 </span><span class="caption-text">A potential extract from a medical dataset. Gender, date of birth, and ZIP code are the quasi-identifiers, while the Disease is the sensitive attribute. An adversary knowing that Alice was born in 1974 and lives in Boston, MA, who gains access to this dataset, will discover that Alice is the patient number 1000 and that she has Lyme Disease.</span><a class="headerlink" href="#tab-k-anonymity-original" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Pseudo-id</p></th>
<th class="head"><p>Gender</p></th>
<th class="head"><p>Date of Birth</p></th>
<th class="head"><p>ZIP code</p></th>
<th class="head"><p>Disease</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1000</p></td>
<td><p>F</p></td>
<td><p>5 June 1975</p></td>
<td><p>02108</p></td>
<td><p>Lyme Disease</p></td>
</tr>
<tr class="row-odd"><td><p>1001</p></td>
<td><p>F</p></td>
<td><p>15 May 1973</p></td>
<td><p>01970</p></td>
<td><p>Hypertension</p></td>
</tr>
<tr class="row-even"><td><p>1002</p></td>
<td><p>F</p></td>
<td><p>3 December 1977</p></td>
<td><p>02657</p></td>
<td><p>Vertigo</p></td>
</tr>
<tr class="row-odd"><td><p>1003</p></td>
<td><p>M</p></td>
<td><p>5 September 1941</p></td>
<td><p>10238</p></td>
<td><p>Stroke</p></td>
</tr>
<tr class="row-even"><td><p>1004</p></td>
<td><p>M</p></td>
<td><p>15 June 1947</p></td>
<td><p>10042</p></td>
<td><p>Stroke</p></td>
</tr>
<tr class="row-odd"><td><p>1005</p></td>
<td><p>M</p></td>
<td><p>25 April 1946</p></td>
<td><p>10133</p></td>
<td><p>Stroke</p></td>
</tr>
<tr class="row-even"><td><p>1006</p></td>
<td><p>F</p></td>
<td><p>25 December 1942</p></td>
<td><p>10053</p></td>
<td><p>Diabetes</p></td>
</tr>
<tr class="row-odd"><td><p>1007</p></td>
<td><p>F</p></td>
<td><p>5 October 1949</p></td>
<td><p>10053</p></td>
<td><p>Osteoporosis</p></td>
</tr>
<tr class="row-even"><td><p>1008</p></td>
<td><p>F</p></td>
<td><p>6 July 1946</p></td>
<td><p>10053</p></td>
<td><p>Arthritis</p></td>
</tr>
</tbody>
</table>
<table class="colwidths-auto docutils align-default" id="tab-k-anonymity-applied">
<caption><span class="caption-number">Table 2 </span><span class="caption-text">The <em>3</em>-anonymous version of the dataset shown in Table 1. The gender attribute is maintained as is, while the date of birth was replaced by an interval of years and the precision of the ZIP code was reduced. An adversary knowing the same information described in Table 1 cannot be sure if Alice is the patient number 1000, 1001 or 1002. Indeed, now each patient is included in an anonymity set of at least <em>3</em> individuals.</span><a class="headerlink" href="#tab-k-anonymity-applied" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Pseudo-id</p></th>
<th class="head"><p>Gender</p></th>
<th class="head"><p>Date of Birth</p></th>
<th class="head"><p>ZIP code</p></th>
<th class="head"><p>Disease</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1000</p></td>
<td><p>F</p></td>
<td><p>[1970-1979]</p></td>
<td><p>0****</p></td>
<td><p>Lyme Disease</p></td>
</tr>
<tr class="row-odd"><td><p>1001</p></td>
<td><p>F</p></td>
<td><p>[1970-1979]</p></td>
<td><p>0****</p></td>
<td><p>Hypertension</p></td>
</tr>
<tr class="row-even"><td><p>1002</p></td>
<td><p>F</p></td>
<td><p>[1970-1979]</p></td>
<td><p>0****</p></td>
<td><p>Vertigo</p></td>
</tr>
<tr class="row-odd"><td><p>1003</p></td>
<td><p>M</p></td>
<td><p>[1940-1949]</p></td>
<td><p>10***</p></td>
<td><p>Stroke</p></td>
</tr>
<tr class="row-even"><td><p>1004</p></td>
<td><p>M</p></td>
<td><p>[1940-1949]</p></td>
<td><p>10***</p></td>
<td><p>Stroke</p></td>
</tr>
<tr class="row-odd"><td><p>1005</p></td>
<td><p>M</p></td>
<td><p>[1940-1949]</p></td>
<td><p>10***</p></td>
<td><p>Stroke</p></td>
</tr>
<tr class="row-even"><td><p>1006</p></td>
<td><p>F</p></td>
<td><p>[1940-1949]</p></td>
<td><p>10053</p></td>
<td><p>Diabetes</p></td>
</tr>
<tr class="row-odd"><td><p>1007</p></td>
<td><p>F</p></td>
<td><p>[1940-1949]</p></td>
<td><p>10053</p></td>
<td><p>Osteoporosis</p></td>
</tr>
<tr class="row-even"><td><p>1008</p></td>
<td><p>F</p></td>
<td><p>[1940-1949]</p></td>
<td><p>10053</p></td>
<td><p>Arthritis</p></td>
</tr>
</tbody>
</table>
<div class="section" id="weakness-of-k-anonymity-model">
<h8>Weakness of <em>k</em>-anonymity model<a class="headerlink" href="#weakness-of-k-anonymity-model" title="Permalink to this headline">¶</a></h8>
<p>Unfortunately, the <em>k</em>-anonymity framework can be vulnerable in some cases. In particular, it is not safe against <em>homogeneity attack</em> and <em>background knowledge attack</em>. The homogeneity attack exploits a possible lack of variety in the sensitive attributes. Indeed, an adversary can infer the value of the sensitive attributes when a <em>k</em>-anonymous dataset contains a group of <em>k</em> entries with the same value for the sensitive attributes. As an example, suppose that the attacker is searching for Bob in the dataset presented in <code class="xref std std-numref docutils literal notranslate"><span class="pre">tab_k_anonymity_applied</span></code>: the adversary knows that Bob was born in July 1946. Even if the attacker is not able to discriminate Bob between patients 1003, 1004, and 1005, the disease associated with all these three patients is a stroke; thus, the adversary cannot re-identify Bob, but he/she can still infer that Bob suffers from heart disease.
In a background knowledge attack, instead, an attacker knows information useful to associate some quasi-identifiers with some sensitive attributes: as an example, remaining in the medical domain, it is common knowledge that certain diseases are more frequent in a specific gender.</p>
</div>
</div>
<div class="section" id="bibliography">
<h7>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h7>
<p id="id3"><dl class="citation">
<dt class="label" id="id2818"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Latanya Sweeney. Uniqueness of simple demographics in the U.S. population. 2000. LIDAP-WP4.</p>
</dd>
<dt class="label" id="id2812"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Latanya Sweeney. K-anonymity: a model for protecting privacy. <em>Int. J. Uncertain. Fuzziness Knowl.-Based Syst.</em>, 10(5):557–570, oct 2002. URL: <a class="reference external" href="https://doi.org/10.1142/S0218488502001648">https://doi.org/10.1142/S0218488502001648</a>, <a class="reference external" href="https://doi.org/10.1142/S0218488502001648">doi:10.1142/S0218488502001648</a>.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Comandé et al. Elgar Encyclopedia of Law and Data Science. Edward Elgar Publishing (2022) ISBN: 978 1 83910 458 9</em> by Francesca Pratesi, Roberto Pellungrini, and Anna Monreale.</p>
</div></blockquote>
</div>
</div>
<span id="document-Privacy_and_Data_Governance/L2.l_diversity"></span><div class="tex2jax_ignore mathjax_ignore section" id="l-diversity">
<h6>l-diversity<a class="headerlink" href="#l-diversity" title="Permalink to this headline">¶</a></h6>
<!-- NOTE: Table 1 is hard written in caption of Table 2 -->
<div class="section" id="in-brief">
<h7>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h7>
<p><strong>l-diversity</strong> aims to protect the diversity of sensitive attributes in the <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.k_anonymity"><span class="doc std std-doc">anonymity by indistinguishability</span></a> paradigm. An anonymity set is <em>l-diverse</em> if contains at least <em>l</em> “well-represented” values for the sensitive attribute.</p>
<!--
**l-diversity** aims to protect the diversity of sensitive attributes in the **anonymity by indistinguishability** paradigm. An anonymity set is *l-diverse* if contains at least *l* "well-represented" values for the sensitive attribute.
-->
</div>
<div class="section" id="more-in-detail">
<h7>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h7>
<p><strong>l-diversity</strong> <span id="id1">[<a class="reference internal" href="#id2782">1</a>]</span> was proposed to overcome one of the weakness of <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.k_anonymity"><span class="doc">k-anonymity</span></a> against the <em>homogeneity attack</em>.</p>
<p>Indeed, suppose to publish the <em>3</em>-anonymous dataset depicted in <a class="reference internal" href="#tab-l-diversity"><span class="std std-numref">Table 3</span></a>, and suppose that an attacker is searching for Bob in the dataset, knowing that their target, Bob, was born in July 1946. Even if the attacker is not able to discriminate Bob between patients 1003, 1004, and 1005, the disease associated with all these three patients is a stroke. Thus, the adversary cannot re-identify Bob, but they can still infer that Bob suffers from heart disease.</p>
<table class="colwidths-auto docutils align-default" id="tab-l-diversity">
<caption><span class="caption-number">Table 3 </span><span class="caption-text">A toy example representing a <em>3</em>-anonymous dataset, obtained generalizing the date of birth and the ZIP code of patients. Here, each patient is included in an anonymity set of at least <em>3</em> individuals.</span><a class="headerlink" href="#tab-l-diversity" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Pseudo-id</p></th>
<th class="head"><p>Gender</p></th>
<th class="head"><p>Date of Birth</p></th>
<th class="head"><p>ZIP code</p></th>
<th class="head"><p>Disease</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1000</p></td>
<td><p>F</p></td>
<td><p>[1970-1979]</p></td>
<td><p>0****</p></td>
<td><p>Lyme Disease</p></td>
</tr>
<tr class="row-odd"><td><p>1001</p></td>
<td><p>F</p></td>
<td><p>[1970-1979]</p></td>
<td><p>0****</p></td>
<td><p>Hypertension</p></td>
</tr>
<tr class="row-even"><td><p>1002</p></td>
<td><p>F</p></td>
<td><p>[1970-1979]</p></td>
<td><p>0****</p></td>
<td><p>Vertigo</p></td>
</tr>
<tr class="row-odd"><td><p>1003</p></td>
<td><p>M</p></td>
<td><p>[1940-1949]</p></td>
<td><p>10***</p></td>
<td><p>Stroke</p></td>
</tr>
<tr class="row-even"><td><p>1004</p></td>
<td><p>M</p></td>
<td><p>[1940-1949]</p></td>
<td><p>10***</p></td>
<td><p>Stroke</p></td>
</tr>
<tr class="row-odd"><td><p>1005</p></td>
<td><p>M</p></td>
<td><p>[1940-1949]</p></td>
<td><p>10***</p></td>
<td><p>Stroke</p></td>
</tr>
<tr class="row-even"><td><p>1006</p></td>
<td><p>F</p></td>
<td><p>[1940-1949]</p></td>
<td><p>10053</p></td>
<td><p>Diabetes</p></td>
</tr>
<tr class="row-odd"><td><p>1007</p></td>
<td><p>F</p></td>
<td><p>[1940-1949]</p></td>
<td><p>10053</p></td>
<td><p>Osteoporosis</p></td>
</tr>
<tr class="row-even"><td><p>1008</p></td>
<td><p>F</p></td>
<td><p>[1940-1949]</p></td>
<td><p>10053</p></td>
<td><p>Arthritis</p></td>
</tr>
</tbody>
</table>
<p>The basic idea behind l-diversity is to maintain the diversity of sensitive attributes. Essentially, each anonymity set should mantain at least <em>l</em> different values of sentive attributes.</p>
<p>Unfortunately, l-diversity is insufficient to prevent attacks when the overall distribution is skewed. The attacker can infer the value of sensitive attribute knowing the global distribution of the attributes.</p>
</div>
<div class="section" id="bibliography">
<h7>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h7>
<p id="id2"><dl class="citation">
<dt class="label" id="id2782"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Ashwin Machanavajjhala, Johannes Gehrke, Daniel Kife, and Muthuramakrishnan Venkitasubramaniam. L-diversity: privacy beyond k-anonymity. In <em>22nd International Conference on Data Engineering</em>. 2006.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Francesca Pratesi.</p>
</div></blockquote>
</div>
</div>
<span id="document-Privacy_and_Data_Governance/L2.t_closeness"></span><div class="tex2jax_ignore mathjax_ignore section" id="t-closeness">
<h6>t-closeness<a class="headerlink" href="#t-closeness" title="Permalink to this headline">¶</a></h6>
<div class="section" id="in-brief">
<h7>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h7>
<p><strong>t-closeness</strong> aims to maintain the distribution of sensitive attributes in the <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.k_anonymity"><span class="doc std std-doc">anonymity by indistinguishability</span></a> paradigm, ensuring that the distance between the two distributions (the original and the private ones) should be limited by a threshold <em>t</em>.</p>
<!--**t-closeness** aims to maintain the distribution of sensitive attributes in the **anonymity by indistinguishability** paradigm, ensuring that the distance between the two distributions (the original and the private ones) should be limited by a threshold *t*.-->
</div>
<div class="section" id="more-in-detail">
<h7>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h7>
<p><strong>t-closeness</strong> <span id="id1">[<a class="reference internal" href="#id2783">1</a>]</span> was proposed to overcome one of the weakness of <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.k_anonymity"><span class="doc">k-anonymity</span></a> against the <em>backgound knowledge attack</em>.
In a background knowledge attack, an attacker knows information useful to associate some quasi-identifiers with some sensitive attributes: as an example, in the medical domain, it is common knowledge that certain diseases are more frequent in a specific gender.
Or, again, it is a well-known information that Asian people are less suscettible to heart diseases.</p>
<p>Indeed, suppose to publish the <em>3</em>-anonymous dataset depicted in <a class="reference internal" href="#tab-t-closeness"><span class="std std-numref">Table 4</span></a>, and suppose that an attacker is searching for Umeko, a 20 years old Japanese, living in the 300112 area.
The anonymity set containing Umeko is also l-diverse (see <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.l_diversity"><span class="doc">l-diversity</span></a>. However, it is well-known that Japanese have an extremely low incidence of heart disease. Thus, the adversary can exploit this statistical information to infer, with a good probability, that Umeko suffers from a viral infection.</p>
<table class="colwidths-auto docutils align-default" id="tab-t-closeness">
<caption><span class="caption-number">Table 4 </span><span class="caption-text">A toy example representing a <em>3</em>-anonymous dataset, obtained generalizing the date of birth and the ZIP code of patients. Here, each patient is included in an anonymity set of at least <em>3</em> individuals.</span><a class="headerlink" href="#tab-t-closeness" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Pseudo-id</p></th>
<th class="head"><p>Gender</p></th>
<th class="head"><p>Date of Birth</p></th>
<th class="head"><p>ZIP code</p></th>
<th class="head"><p>Disease</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1000</p></td>
<td><p>M</p></td>
<td><p>[2000-2010]</p></td>
<td><p>30011*</p></td>
<td><p>Lyme Disease</p></td>
</tr>
<tr class="row-odd"><td><p>1001</p></td>
<td><p>M</p></td>
<td><p>[2000-2010]</p></td>
<td><p>30011*</p></td>
<td><p>Viral Infection</p></td>
</tr>
<tr class="row-even"><td><p>1002</p></td>
<td><p>M</p></td>
<td><p>[2000-2010]</p></td>
<td><p>30011*</p></td>
<td><p>Stroke</p></td>
</tr>
<tr class="row-odd"><td><p>1003</p></td>
<td><p>F</p></td>
<td><p>[2000-2010]</p></td>
<td><p>30011*</p></td>
<td><p>Stroke</p></td>
</tr>
<tr class="row-even"><td><p>1004</p></td>
<td><p>F</p></td>
<td><p>[2000-2010]</p></td>
<td><p>30011*</p></td>
<td><p>Stroke</p></td>
</tr>
<tr class="row-odd"><td><p>1005</p></td>
<td><p>F</p></td>
<td><p>[2000-2010]</p></td>
<td><p>30011*</p></td>
<td><p>Stroke</p></td>
</tr>
<tr class="row-even"><td><p>1006</p></td>
<td><p>F</p></td>
<td><p>[2000-2010]</p></td>
<td><p>30011*</p></td>
<td><p>Viral Infection</p></td>
</tr>
</tbody>
</table>
<p>In order to protect people in a dataset with the <em>t</em>-closeness mechanism, indeed, we need to guarantee that the distribution of a sensitive attribute in any equivalence class must be close to the distribution of the attribute in the overall dataset (i.e., the distance between the two distributions should be no more than a threshold <em>t</em>)</p>
</div>
<div class="section" id="bibliography">
<h7>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h7>
<p id="id2"><dl class="citation">
<dt class="label" id="id2783"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Ninghui Li, Tiancheng Li, and Suresh Venkatasubramanian. T-closeness: privacy beyond k-anonymity and l-diversity. In <em>23rd International Conference on Data Engineering</em>. 2007.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Francesca Pratesi.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
<span id="document-Privacy_and_Data_Governance/L2.federated"></span><div class="tex2jax_ignore mathjax_ignore section" id="federated-learning">
<h5>Federated Learning<a class="headerlink" href="#federated-learning" title="Permalink to this headline">¶</a></h5>
<div class="section" id="in-brief">
<h6>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p><strong>Federated Learning</strong> is a paradigm of distributed processing, where models instead of data are shared among peers.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>Data are often massively distributed over different parties, such as mobile devices and organizations. Due to privacy restrictions and regulations, like the GDPR, these distributed datasets cannot be transmitted to a central server to train a unique ML model.
To address this issue, in 2016 McMahan et al. <span id="id1">[<a class="reference internal" href="#id2781">1</a>]</span> proposed the idea of Federated Learning (FL), which allows a set of distributed parties to train a global model
while keeping their data private.</p>
<p>Depending on the clients nature, we can identify two main scenarios: the <em>cross-device</em> and <em>cross-silo</em> FL. In the first case the clients are mobile devices (smartphones, laptops, sensors), while in the second are organizations (hospitals, banks, data centers.
One important aspect that differentiates the two scenarios is the clients’ statefulness. While in the cross-device setting, the parties are supposed to be stateless, since they are supposed to be able to leave the federation at any time due to connection problems, the clients in the cross-silo scenario are assumed to be stateful, that is they can save intermediate states of the training process and reuse them later on if needed.</p>
<p>Another key factor in FL is the architecture employed. The most widely used architecture is the centralized one, where a central server orchestrates the communication between the clients and the server itself. However, to reduce the workload on the central server, Liu et al. <span id="id2">[<a class="reference internal" href="#id2782">2</a>]</span> suggested adopting a hierarchical architecture, putting one or more intermediate layers made up of proxies between the server and the participants.</p>
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p id="id3"><dl class="citation">
<dt class="label" id="id2781"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>J. Konečný, H.B. McMahan, D. Ramage, and P. Richtárik. Federated optimization: distributed machine learning for on-device intelligence. <em>CoRR</em>, 2016.</p>
</dd>
<dt class="label" id="id2782"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>L. Liu, J. Zhang, S. Song, and K.B. Letaief. Client-edge-cloud hierarchical federated learning. In <em>IEEE International Conference on Communications</em>. 2020.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Comandé et al. Elgar Encyclopedia of Law and Data Science. Edward Elgar Publishing (2022) ISBN: 978 1 83910 458 9</em> by Francesca Pratesi, Roberto Pellungrini, and Anna Monreale.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
<span id="document-Privacy_and_Data_Governance/L1.attacks"></span><div class="tex2jax_ignore mathjax_ignore section" id="attacks-on-anonymization-schemes">
<h4>Attacks on anonymization schemes<a class="headerlink" href="#attacks-on-anonymization-schemes" title="Permalink to this headline">¶</a></h4>
<p>There are a variety of attacks that involve data privacy. Some of them are very context-specific (for example, there exists attacks on partition-based algorithms, such as deFinetti Attack or Minimality Attack), while other are more general. For example, we can have the <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.reidentification"><span class="doc">Re-identification Attack</span></a>, where the aim is to link a identity in the data with a real identity; <em>Membership Attack</em>, where the goal is to understand whether or not a particular individual is present in the considered dataset; <em>Reconstruction Attack</em>, where one wants to reconstruct (even partially) a private dataset from public aggregate information.</p>
<!--There are a variety of attacks that involve data privacy. Some of them are very context-specific (for example, there exists attacks on partition-based algorithms, such as deFinetti Attack or Minimality Attack), while other are more general. For example, we can have the {doc}`./L2.reidentification`, where the aim is to link a identity in the data with a real identity; {doc}`L2.membership`, where the goal is to understand whether or not a particular individual is present in the considered dataset; {doc}`L2.reconstruction`, where one wants to reconstruct (even partially) a private dataset from public aggregate information.-->
<blockquote>
<div><p>This entry was written by Francesca Pratesi.</p>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-Privacy_and_Data_Governance/L2.reidentification"></span><div class="tex2jax_ignore mathjax_ignore section" id="re-identification-attack">
<h5>Re-identification Attack<a class="headerlink" href="#re-identification-attack" title="Permalink to this headline">¶</a></h5>
<p><em>Synonyms</em>: Linking attack, Attack on pseudonymised data</p>
<div class="section" id="in-brief">
<h6>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p><strong>Re-identification attack</strong> aims to link a certain set of data related to an individual in a dataset (which does not contain direct identifiers) to a real identity, relying on additional information.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>A first, basic, and simple way to preserve a subject’s privacy is to
de-couple the identity of said subject from its data. This is process is
called <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.pseudonymization"><span class="doc">Pseudonymization</span></a>. The typical practical approach to achieve
pseudonymity is to detect which attributes in the data may reveal the
subject’s identity, called <em>personal identifiers</em>, and substitute them
with some other value. While this process can help in reducing the risk of a direct
re-identification of the data subjects based on the published data, re-identification can still be possible in certain cases.</p>
<p>Indeed, additional information (usually called <em>background information</em>) can be used by a malicious third party (often called <em>adversary</em> or <em>attacker</em>) to link a subject’s identity to its data.
Additional information makes the difference between <em>pseudonymised</em> and
<em>anonymised</em> data. Anonymous data are deprived of all distinctive
elements of the person, i.e., those elements that permit to identify
both directly or indirectly that person in the data. Anonymous data
cannot be re-identified by definition, even when using additional
information, therefore this type of data is not subject to the current privacy
regulations (e.g., the GDPR <span id="id1">[<a class="reference internal" href="TAILOR.html#id8">1</a>]</span>).</p>
<p>This key difference can be better understood with the famous real life
example of the attack on the privacy of the Governor of Massachussetts.
In 1996, <em>William Floyd Weld</em>, then Governor of Massachusetts, lost his
consciousness during a public event. Rushed to the nearby Deaconess
Waltham Hospital, he was officially diagnosed with influenza and
consequently discharged the following day <span id="id2">[<a class="reference internal" href="#id2812">3</a>]</span>. Some time
later, professor Latanya Sweeney, a graduate computer science student at
MIT at the time, successfully reconstructed what had happened to the
governor and inferred his diagnosis linking two different data sources:
a publicly available voter rolls dataset and a hospital dataset without patients’ names, thus
considered anonymous <span id="id3">[<a class="reference internal" href="#id2813">4</a>]</span>. The voter rolls dataset contained
the name, address, ZIP code, birth date, sex and other attributes of
every voter in the city of Cambridge (Middlesex County). The hospital
dataset was issued to researchers by the Massachusetts Group Insurance
Commission and contained diagnosis of patients along with some demographic
information. Since the identity of the different patients was not
present in the hospital data, the information published there
was considered harmless; indeed, this is almost true if thery are considered by themselves. But Sweeney knew that the governor was
admitted to the hospital, so she also knew he was present in the data.
Therefore, she (in a complete legittimate way) gained access to both datasets and she intersected the demographic information in the two
dataset, discoverying some important facts: <em>six individuals in the hospital dataset shared the Governor’s
birth date; only three of these were men; but only one of these men lived
in the Governor’s own ZIP code</em>.</p>
<!--
\centering
[\[tab\_roll\]]{#tab_roll label="tab_roll"}

   **Surname**     **Name**      **Date of birth**   **Sex**   **Address**    **ZIP code**   **Last vote**
  ------------- --------------- ------------------- --------- -------------- -------------- ---------------
       ...            ...               ...            ...         ...            ...             ...
      Weld       William Floyd     31 July 1945         M      75, Essex St      02139        22 May 1998
      Welsh         Alice           4 July 1952         F      150, Main St      02139        22 May 1998
     Weltcher        Bob           13 July 1947         M      148, Gold Rd      02138        22 May 1998
       ...            ...               ...            ...         ...            ...             ...

  : Cambridge Voter Roll Dataset 

\centering
[\[tab\_hosp\]]{#tab_hosp label="tab_hosp"}

   **Id**   **Sex**   **Date of birth**   **ZIP code**    **Visit**     **Diagnosis**
  -------- --------- ------------------- -------------- -------------- ---------------
    ...       ...            ...              ...            ...             ...
     1         M        31 July 1945         02138        9 May 1996     Diabetes
     2         M        31 July 1945         02139       18 May 1996       Stroke
     3         F        31 July 1945         02138       18 May 1996    Osteoporosis
     4         F        31 July 1945         02139       23 May 1996       Stroke
     5         M         4 July 1945         02138       5 June 1996      Diabetes
     6         M        13 June 1945         02139       9 June 1996      Arthritis
     7         F        5 April 1945         02139       4 July 1996    Hypertension
    ...       ...            ...              ...            ...             ...

  : Hospital Dataset

-->
<!-- <span style='background:blue;color:white'> -->
<table class="colwidths-auto docutils align-default" id="tab-vote">
<caption><span class="caption-number">Table 5 </span><span class="caption-text">Cambridge Voter Roll Dataset: this table represents an extract of the voter dataset.</span><a class="headerlink" href="#tab-vote" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Surname</p></th>
<th class="head"><p>Name</p></th>
<th class="head"><p>Date of birth</p></th>
<th class="head"><p>Sex</p></th>
<th class="head"><p>Address</p></th>
<th class="head"><p>ZIP code</p></th>
<th class="head"><p>Last vote</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
</tr>
<tr class="row-odd"><td><p>Weld</p></td>
<td><p>William Floyd</p></td>
<td><p><span style='background:#00bfec'>31 July 1945 </span></p></td>
<td><p><span style='background:#00bfec'> M </span></p></td>
<td><p>75, Essex St</p></td>
<td><p><span style='background:#00bfec'> 02139 </span></p></td>
<td><p>22 May 1998</p></td>
</tr>
<tr class="row-even"><td><p>Welsh</p></td>
<td><p>Alice</p></td>
<td><p>4 July 1952</p></td>
<td><p>F</p></td>
<td><p>150, Main St</p></td>
<td><p>02139</p></td>
<td><p>22 May 1998</p></td>
</tr>
<tr class="row-odd"><td><p>Weltcher</p></td>
<td><p>Bob</p></td>
<td><p>13 July 1947</p></td>
<td><p>M</p></td>
<td><p>148, Gold Rd</p></td>
<td><p>02138</p></td>
<td><p>22 May 1998</p></td>
</tr>
<tr class="row-even"><td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
</tr>
</tbody>
</table>
<table class="colwidths-auto docutils align-default" id="tab-medical">
<caption><span class="caption-number">Table 6 </span><span class="caption-text">Hospital Dataset: this table represents an extract of the medical dataset. Note that this table does not contain any direct identifiers, such as surnames or social security numbers.</span><a class="headerlink" href="#tab-medical" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Id</p></th>
<th class="head"><p>Sex</p></th>
<th class="head"><p>Date of birth</p></th>
<th class="head"><p>ZIP code</p></th>
<th class="head"><p>Visit</p></th>
<th class="head"><p>Diagnosis</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>M</p></td>
<td><p>31 July 1945</p></td>
<td><p>02138</p></td>
<td><p>9 May 1996</p></td>
<td><p>Diabetes</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p><span style='background:#00bfec'>  M  </span></p></td>
<td><p><span style='background:#00bfec'>  31 July 1945  </span></p></td>
<td><p><span style='background:#00bfec'>   02139 </span></p></td>
<td><p>18 May 1996</p></td>
<td><p>Stroke</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>F</p></td>
<td><p>31 July 1945</p></td>
<td><p>02138</p></td>
<td><p>18 May 1996</p></td>
<td><p>Osteoporosis</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>F</p></td>
<td><p>31 July 1945</p></td>
<td><p>02139</p></td>
<td><p>23 May 1996</p></td>
<td><p>Stroke</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>M</p></td>
<td><p>4 July 1945</p></td>
<td><p>02138</p></td>
<td><p>5 June 1996</p></td>
<td><p>Diabetes</p></td>
</tr>
<tr class="row-even"><td><p>6</p></td>
<td><p>M</p></td>
<td><p>13 June 1945</p></td>
<td><p>02139</p></td>
<td><p>9 June 1996</p></td>
<td><p>Arthritis</p></td>
</tr>
<tr class="row-odd"><td><p>7</p></td>
<td><p>F</p></td>
<td><p>5 April 1945</p></td>
<td><p>02139</p></td>
<td><p>4 July 1996</p></td>
<td><p>Hypertension</p></td>
</tr>
<tr class="row-even"><td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
</tr>
</tbody>
</table>
<p>In <a class="reference internal" href="#tab-vote"><span class="std std-numref">Table 5</span></a> and <a class="reference internal" href="#tab-medical"><span class="std std-numref">Table 6</span></a> we can see a simplified version of Sweeney’s attack.
Looking at the tables singularly, no <em>sensitive information</em> (i.e., the diagnosis) about the Governor William Floyd Weld can be inferred. However, from <a class="reference internal" href="#tab-vote"><span class="std std-numref">Table 5</span></a> we gain access to the date of birth and ZIP code of Governor. Then, we can search for the persons born on July 31, 1945 in <a class="reference internal" href="#tab-medical"><span class="std std-numref">Table 6</span></a>, finding patients number 1, 2, 3 and 4. However, ids 3 and 4 correspond to women, so we should consider only individuals 1 and 2. Finally, we can look at the ZIP code in <a class="reference internal" href="#tab-medical"><span class="std std-numref">Table 6</span></a>: the patient number 1 lives in a different area, so we only have one patient (the number 2) that can be the Governor. In brief, we can see that, matching the information colored in <span style='background:#00bfec'>blue</span> from the two tables, there is no other possibility for Governor Weld but to be a patient suffering from a stroke. This was a clear breach of the privacy of the Governor, as the public statement about his health differed from the actual cause of hospitalization.</p>
<p>Sweeney conducted similar attacks in a more structured and generalised experiment, finding that 87% of the United States population was uniquely or nearly uniquely identified by the combination of ZIP code, gender, and date of birth <span id="id4">[<a class="reference internal" href="TAILOR.html#id2812">2</a>]</span>. This leaded Sweeney to theorize the k-anonymity principle, and call the attributes used for the re-identification process <em>quasi-identifier</em>.</p>
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p id="id5"><dl class="citation">
<dt class="label" id="id10"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>European Parliament &amp; Council. General data protection regulation. 2016. L119, 4/5/2016, p. 1–88.</p>
</dd>
<dt class="label" id="id2814"><span class="brackets"><a class="fn-backref" href="#id4">2</a></span></dt>
<dd><p>Latanya Sweeney. K-anonymity: a model for protecting privacy. <em>Int. J. Uncertain. Fuzziness Knowl.-Based Syst.</em>, 10(5):557–570, oct 2002. URL: <a class="reference external" href="https://doi.org/10.1142/S0218488502001648">https://doi.org/10.1142/S0218488502001648</a>, <a class="reference external" href="https://doi.org/10.1142/S0218488502001648">doi:10.1142/S0218488502001648</a>.</p>
</dd>
<dt class="label" id="id2812"><span class="brackets"><a class="fn-backref" href="#id2">3</a></span></dt>
<dd><p>Daniel C. Barth-Jones. The 're-identification' of governor william weld's medical information: a critical re-examination of health data identification risks and privacy protections, then and now. 2012. <a class="reference external" href="https://doi.org/http://dx.doi.org/10.2139/ssrn.2076397">doi:http://dx.doi.org/10.2139/ssrn.2076397</a>.</p>
</dd>
<dt class="label" id="id2813"><span class="brackets"><a class="fn-backref" href="#id3">4</a></span></dt>
<dd><p>Latanya Sweeney. Weaving technology and policy together to maintain confidentiality. <em>The Journal of Law, Medicine &amp; Ethics</em>, 25(2-3):98–110, 1997. PMID: 11066504. URL: <a class="reference external" href="https://doi.org/10.1111/j.1748-720X.1997.tb01885.x">https://doi.org/10.1111/j.1748-720X.1997.tb01885.x</a>, <a class="reference external" href="https://arxiv.org/abs/https://doi.org/10.1111/j.1748-720X.1997.tb01885.x">arXiv:https://doi.org/10.1111/j.1748-720X.1997.tb01885.x</a>, <a class="reference external" href="https://doi.org/10.1111/j.1748-720X.1997.tb01885.x">doi:10.1111/j.1748-720X.1997.tb01885.x</a>.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Comandé et al. Elgar Encyclopedia of Law and Data Science. Edward Elgar Publishing (2022) ISBN: 978 1 83910 458 9</em> by Francesca Pratesi, Roberto Pellungrini, and Anna Monreale.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
</div>
</div>
<span id="document-Societal_and_Environmental_Wellbeing/Societal_and_Environmental_Wellbeing"></span><div class="tex2jax_ignore mathjax_ignore section" id="societal-and-environmental-wellbeing">
<h3>Societal and Environmental Wellbeing<a class="headerlink" href="#societal-and-environmental-wellbeing" title="Permalink to this headline">¶</a></h3>
<!-- Sustainability-->
<div class="section" id="in-brief">
<h4>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h4>
<p>Sustainability is an ethical aspect studied in the <a href="https://tailor-network.eu/" target=_blank>TAILOR project</a>.
Sustainability in AI refers to the application of AI technology (from the power consumption of the hardware to the processing and storage of a very large dataset) while considering problems related to sustainable development.
The sustainability in AI can be divided into two branches that should be addressed: AI for Sustainability and Sustainability of AI <span id="id1">[<a class="reference internal" href="#id2823">1</a>]</span>. The first branch is the most developed branch whose aim is to achieve the United Nations Sustainable Development Goals (SDGs) and where not-for-profit organisation such as <a href="https://ai4good.org/" target=_blank>AI4Good</a> or <a href="https://www.forclimate.ai/" target=_blank>AI for Climate</a> are active to address the scope. In this branch we consider AI applications whose objective is to use the algorithm for environmental purposes such as reducing the energy cost of large data centres or better predicting the weather forecast to maximise renewable energy production. The second branch regards the sustainable development of AI/ML itself, concerning how to measure the sustainability of developing and utilising AI models such as the computational cost of training AI algorithms or the amount of CO2 emission.</p>
</div>
<div class="section" id="abstract">
<h4>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">¶</a></h4>
<p>In this part we will cover the main elements that define sustainability in AI systems. Some of them are related to the aspects of AI for sustainability and can be common to computer systems in general, others are related to the sustainability of AI. We will focus on sustainability in AI with more attention to new AI-specific issues and challenges because they are less covered in the literature. We include a taxonomic organisation of terms in the area of AI sustainability and their definition.</p>
</div>
<div class="section" id="motivation-and-background">
<h4>Motivation and Background<a class="headerlink" href="#motivation-and-background" title="Permalink to this headline">¶</a></h4>
<p>Given the increasing capabilities and widespread use of artificial intelligence, there is a growing concern about its impact on the environment related to the carbon footprints and the power consumption needed for training, store and developing AI models and algorithms. There is a wide literature regarding the dangers of climate change and the need of modifying the habits of use of the technology by consumers and industries. Plans such as the European Green Deal promulgated by the European Commission has the aim to tackle climate change. AI has the potential to accelerate the efforts of protecting the planet with many applications such as the use of machine learning to optimise the energy consumption efficiency, reducing the CO2 emission, monitoring quality of the air, the water, the biodiversity changes, the vegetation, the forest cover, and preventing natural disasters.</p>
</div>
<div class="section" id="guidelines">
<h4>Guidelines<a class="headerlink" href="#guidelines" title="Permalink to this headline">¶</a></h4>
<p>The guidelines include common elements that should be considered in the design and build of a piece of technology while using ML and other emerging technologies.
In 2015 the United Nations General Assembly set up the Sustainable Development Goals, a list of 17 interlinked global goals that are intended to be achieved by the year 2030. Not-for-profit organisations such as AI4Good also described a list of guidelines like “Guidelines on the Implementation of Eco-friendly Criteria for AI and other Emerging Technologies” and “Guidelines on the environmental efficiency of machine learning processes in supply chain management” (both guidelines can be found <a href="https://www.itu.int/en/ITU-T/focusgroups/ai4ee/Pages/WG3deliverables.aspx" target=_blank>here</a>) with the scope of addressing very important aspects on the use of AI and its development.</p>
</div>
<div class="section" id="taxonomic-organisation-of-terms">
<h4>Taxonomic Organisation of Terms<a class="headerlink" href="#taxonomic-organisation-of-terms" title="Permalink to this headline">¶</a></h4>
<p>The Sustainable Development Goals (SDGs) are a universal call to action to end poverty, protect the planet, and ensure that by 2030 all people enjoy peace and prosperity. They were adopted by the United Nations in 2015 as. They are divided into 17 goals -they recognize that action in one area will affect outcomes in others and that development must balance social, economic and environmental sustainability (see <a href="https://www.undp.org/sustainable-development-goals#:~:text=The%20Sustainable%20Development%20Goals%20(SDGs)%2C%20also%20known%20as%20the,people%20enjoy%20peace%20and%20prosperity" target=_blank>here</a> for further details).</p>
<p>AI is involved in many of these 17 Global Goals, including <a href="https://www.undp.org/sustainable-development-goals#sustainable-cities-and-communities" target=_blank>Sustainable cities and communities</a> and <a href="https://www.undp.org/sustainable-development-goals#responsible-consumption-and-production" target=_blank>Responsible consumption and production</a> with areas like production optimization, smart cities and green computing.</p>
<div class="figure align-center" id="t3-6taxonomy36">
<a class="reference internal image-reference" href="_images/UN_Sustainable_Development_Goals_pillars.jpg"><img alt="_images/UN_Sustainable_Development_Goals_pillars.jpg" src="_images/UN_Sustainable_Development_Goals_pillars.jpg" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 28 </span><span class="caption-text">A possible taxonomy about Sustainability Goals <span id="id2">[<a class="reference internal" href="#id2824">2</a>]</span>.</span><a class="headerlink" href="#t3-6taxonomy36" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="main-keywords">
<h4>Main Keywords<a class="headerlink" href="#main-keywords" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/sustenaible_AI"><span class="doc">Sustainable AI</span></a>: The goal of <em>Sustainable AI</em> is to reduce energy consumption for a more sustainable AI and also find a way in which AI could contribute to solve some of the big sustainability challenges that face humanity today (e.g., climate change).</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/greenAI"><span class="doc">Green AI</span></a>: The goal of Green AI (also known as <em>Green IT</em>, <em>Green Computing</em>, or <em>ICT Sustainability</em>) is to minimise the negative aspects of IT operations on the environment. To do so, computers and IT products can be designed, manufactured and disposed of in an environmentally-friendly manner.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/power_aware"><span class="doc">Power-aware Computing</span></a>: Power-aware computing (also called <em>Energy-aware Computing</em> and <em>Energy-efficient Computing</em>) is part of the Green IT. Power-aware design strategies strive to maximise performance in high-performance systems within power dissipation and power consumption constraints. Reduced power utilisation on a node is one way to reduce the amount of energy required to compute. Lowering the frequency at which the CPU works on one approach to do this. Reduced clock speed, on the other hand, increases the time to solution, posing a potential compromise.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/cloud_computing"><span class="doc">Cloud Computing</span></a>: Cloud computing (or <em>Mesh computing</em>) is the provision of computing resources (storage and processing power) on demand, without direct user intervention. A large cloud often includes multiple data centres, each housing a different set of functions. The cloud computing model aims to achieve core economies of scale through sharing of resources, taking advantage of a “pay-as-you-go” model that can decrease capital expenditures, but can also result in unforeseen operating expenses for users who are unaware of the concept.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/edge_computing"><span class="doc">Edge Computing</span></a>: Edge Computing (or <em>Fog Computing</em>) is a distributed computing paradigm in which processing and data storage are brought closer to the data sources. This should increase response times while also conserving bandwidth. Rather than referring to a single technology, the phrase refers to an architecture.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/data_centre"><span class="doc">Data Centre</span></a>: A data centre is a structure, a specialised area inside a structure, or a collection of structures used to house computer systems and related components such as telecommunications and storage systems. Because IT operations are so important for business continuity, they usually incorporate redundant or backup components and infrastructure for power, data transmission connections, environmental control (such as air conditioning and fire suppression), and other security systems. A huge data centre is a large-scale activity that consumes the same amount of power as a small town.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/cradle_to_cradle"><span class="doc">Cradle-to-cradle Design</span></a>: Cradle-to-cradle Design (also known as <em>2CC2</em>, <em>C2C</em>, <em>cradle 2 cradle</em>, or <em>regenerative design</em>) is a biomimetic approach to product and system design that mimics natural processes, in which materials are considered as nutrients flowing in healthy, safe metabolisms.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/resource_prediction"><span class="doc">Resource Prediction</span></a>: Resource prediction (also called <em>Workload Prediction</em> or <em>Workload Forecast</em>) is the estimation of the resources a customer will require in the future to complete his tasks.  This concept has a wide variety of application and it is particularly studied in the context of data centres management. When these forecasts are generated, historical and current data are utilised to predict how many resource units, which tools and operative systems and the number of requests are required to accomplish a task.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/resource_allocation"><span class="doc">Resource Allocation</span></a>: Cloud computing provides a computing environment where businesses, clients, and projects can lease resources on demand. Both cloud users and providers want to allocate cloud resources efficiently and profitably. These resources are typically scarce, therefore cloud providers must make the best use of them while staying within the confines of the cloud environment and meeting the demands of cloud apps so that they may perform their jobs. The distribution of resources is one of the most important aspects of cloud computing. Its efficiency has a direct impact on the overall performance of the cloud environment. Cost efficiency, reaction time, reallocation, computing performance, and job scheduling are all key difficulties in resource allocation. Cloud computing users want to do task for the least amount of money feasible.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/social_impact"><span class="doc">Social Impact of AI Systems</span></a>: Artificial intelligence (AI) is quickly changing the way we work and the way we live. Indeed, the <strong>societal impact of AI</strong> is on most people’s minds.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/human_interaction"><span class="doc">AI human interaction</span></a>: As AI develops to learn and adapt, it is increasingly being perceived as human-like in both its appearance and intelligence. This ability to imitate human behavior and interactions creates anthropomorphic cues which cause users to form emotional bonds <span id="id3">[<a class="reference internal" href="TAILOR.html#id2825">1</a>]</span>. As AI becomes more commonplace in our daily lives, being integrated into virtual assistants, recommender systems etc. it is important to assess how the system’s anthropomorphic cues affect our interaction with it. In addition, potential risks resulting from AI-human interaction should be assessed prior to any implementation of AI as previous research has demonstrated the unpredictability of AI.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/self-identification"><span class="doc">Self-identification of AI</span></a>: AI systems which are presented to be both high in intelligence and have a good quality design tend to cause humans to anthropomorphize the system; i.e. assign human-like traits to it which can lead to emotional bonding with the system <span id="id4">[<a class="reference internal" href="TAILOR.html#id2825">1</a>]</span>. This is dangerous, especially for vulnerable groups such as older generations or those less acquainted with the mechanisms behind the AI. In addition to the threat of emotional attachment, AI systems have been found to frequently provide false or partially false information and portrait it as fact, due to a phenomenon referred to as AI hallucinations. As such it is important that users can clearly identify an artificial intelligence as such, in order to understand the system’s limitations and prevent emotional attachment and misinformation. The remaining part of this chapter will first explain which AI systems are required to be clearly identifiable and then elaborate on how these criteria should be met.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/emotional_impact"><span class="doc">Emotional Impact</span></a>: Due to the fast development of AI systems, their behavior, in some aspects, has become akin to humans. Especially with the introduction of large language models like chatGPT, it has become increasingly difficult to differentiate human and AI generated language. When coupled with human-like design, this cognitive ability has been found to lead to emotional attachment by users. While emotional impact cannot be prevented entirely, it is important to assess the extent to which the AI encourages human attachment and ensure clear signaling to the end-user that they are interacting with an AI.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/workforce_impact"><span class="doc">AI Impact on the Workforce</span></a>: The impact of Artificial Intelligence on the workforce can be considered from several perspectives. The most common consideration is the loss of jobs or possibility of deskilling of the workforce due to the introduction of AI systems. A second consideration is the increased demand for workers with a computer-science skill set in order to operate and maintain implemented AI systems. A final consideration is the effect of AI-triggered changes for stakeholders. Prior to the introduction of an AI system, risks and consequences in these areas should be assessed and steps to counter negative outcomes should be considered.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/society_and_democracy"><span class="doc">Society and Democracy</span></a>: The rapid increase of technological development is impacting society in new and partially unpredictable ways. With AI being integrated into all facets of life, ranging from recommender systems for movies to automatic passport control and facial recognition in airports, it is important to understand the negative ramifications this technology may have on society and democracy as a whole. Previously, the development of technologies was frequently guided by the interests of tech giants with profit motivating research <span id="id5">[<a class="reference internal" href="TAILOR.html#id2827">2</a>]</span>. While this line of development has produced some impressive systems, there is little consideration for the protection of citizens and the regulation of these technologies for the greater societal good. In order to combat this trend, the European Union introduced the AI act, stipulating legislation for new AI systems depending on the level or risk posed by the system. However, with this increase in legislation, fears are rising about Europe falling behind other, non-legislated countries, in the development of new software. It is such that legislators need to find the balance between regulation and development, to protect democracy and society without adversely affecting the development of the European AI research sector.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/social_scoring"><span class="doc">AI for social scoring</span></a>: Social Scoring is the concept that all daily actions taken by individuals are monitored and scored for their benefit to society as a whole. Based on this scoring system, you are assigned a given value which determines your access to education, healthcare and other public goods. AI lends itself to social scoring as it excels at facial recognition and movement identification. However this poses several technical and ethical problems. The obvious ethical issue is that the goodness of actions is subjective and the ability of individuals to access goods and services necessary for survival should not depend on their perceived ability to contribute to society. Furthermore, the technical issue with social scoring is that the quality of AI evaluation of movement hinges on the quality of its training data, with biased training data leading to biased evaluation by the AI. Social scoring has currently only been proposed by the Chinese government but other countries including the United Kingdom are using automatic facial recognition in surveillance tapes to search for fugitives.</p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/propaganda"><span class="doc">AI for propaganda</span></a>: With the onset of large language models, the mass production of text has become considerably cheaper and faster, requiring less human involvement. With the addition of automatic image generators such as Dall-E, the same is now possible for pictures. Previous work has found that GPT-3, the antecedent to ChatGPT is capable of creating text equally persuasive as content from existing covert propaganda <span id="id6">[<a class="reference internal" href="TAILOR.html#id2829">1</a>]</span>. This causes a rising concern regarding the ease at which a large number of influential texts can be circulated using online platforms. To combat mass-propaganda, it is important that online and offline platforms have effective controls to identify AI-generated materials in their publications.</p></li>
</ul>
</div>
<div class="section" id="bibliography">
<h4>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h4>
<!-- :style: unsrtalpha -->
<p id="id7"><dl class="citation">
<dt class="label" id="id2823"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Aimee van Wynsberghe. Sustainable AI: AI for sustainability and the sustainability of AI. <em>AI and Ethics</em>, 1(3):213–218, 2021.</p>
</dd>
<dt class="label" id="id2824"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Department of Economic United Nations and Social Affair. Sustainable development. URL: <a class="reference external" href="https://sdgs.un.org/goals">https://sdgs.un.org/goals</a> (visited on 2022-05-02).</p>
</dd>
<dt class="label" id="id2830"><span class="brackets">3</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id4">2</a>)</span></dt>
<dd><p>Joohee Kim and Il Im. Anthropomorphic response: understanding interactions between humans and artificial intelligence agents. <em>Computers in Human Behavior</em>, 139:107512, 2023.</p>
</dd>
<dt class="label" id="id2831"><span class="brackets"><a class="fn-backref" href="#id5">4</a></span></dt>
<dd><p>Paul Nemitz. Constitutional democracy and technology in the age of artificial intelligence. <em>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</em>, 376(2133):20180089, 2018.</p>
</dd>
<dt class="label" id="id2832"><span class="brackets"><a class="fn-backref" href="#id6">5</a></span></dt>
<dd><p>Josh A Goldstein, Jason Chao, Shelby Grossman, Alex Stamos, and Michael Tomz. How persuasive is ai-generated propaganda? <em>PNAS nexus</em>, 3(2):pgae034, 2024.</p>
</dd>
</dl>
</p>
</div>
<div class="section" id="further-recommended-reading">
<h4>Further Recommended Reading<a class="headerlink" href="#further-recommended-reading" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>van Wynsberghe, Aimee. “Sustainable AI: AI for sustainability and the sustainability of AI.” AI and Ethics 1.3 (2021): 213-218.</p></li>
<li><p>Vinuesa, Ricardo, et al. “The role of artificial intelligence in achieving the Sustainable Development Goals.” Nature communications 11.1 (2020): 1-10.</p></li>
<li><p>Graybill, Robert, and Rami Melhem, eds. Power aware computing. Springer Science &amp; Business Media, 2013.</p></li>
<li><p>Schoormann, Thorsten, et al. “Achieving Sustainability with Artificial Intelligence—A Survey of Information Systems Research.” (2021).</p></li>
<li><p>Khakurel, Jayden, et al. “The rise of artificial intelligence under the lens of sustainability.” Technologies 6.4 (2018): 100.</p></li>
<li><p>Nishant, Rohit, Mike Kennedy, and Jacqueline Corbett. “Artificial intelligence for sustainability: Challenges, opportunities, and a research agenda.” International Journal of Information Management 53 (2020): 102104.</p></li>
<li><p>Wu, Carole-Jean, et al. “Sustainable ai: Environmental implications, challenges and opportunities.” arXiv preprint arXiv:2111.00364 (2021).</p></li>
<li><p>Fisher, Douglas H. “Computing and AI for a Sustainable Future.” IEEE intelligent systems 26.6 (2011): 14-18.</p></li>
<li><p>Pedemonte, C. “AI for Sustainability: an overview of AI and the SDGs to contribute to European policy-making.” (2021).</p></li>
<li><p>Galaz, Victor, et al. “Artificial intelligence, systemic risks, and sustainability.” Technology in Society 67 (2021): 101741.</p></li>
</ul>
<blockquote>
<div><p>This entry was written by Andrea Rossi, Andrea Visentin and Barry O’Sullivan.</p>
</div></blockquote>
</div>
<div class="toctree-wrapper compound">
<span id="document-Societal_and_Environmental_Wellbeing/sustenaible_AI"></span><div class="tex2jax_ignore mathjax_ignore section" id="sustainable-ai">
<h4>Sustainable AI<a class="headerlink" href="#sustainable-ai" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p>The goal of <strong>Sustainable AI</strong> is to reduce energy consumption for a more sustainable AI and also find a way in which AI could contribute to solve some of the big sustainability challenges that face humanity today (e.g., climate change).</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>AI systems are responsible of most of energy consumption nowadays.</p>
<p>Referring to UN sustainability goals highlighted in <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/Societal_and_Environmental_Wellbeing"><span class="doc">Societal and Environmental Wellbeing</span></a>, it is important to ensure:</p>
<ul class="simple">
<li><p><a href="https://www.undp.org/sustainable-development-goals/affordable-and-clean-energy" target=_blank>Goal 7: Affordable and clean energy</a>: along with the growth of the world population and the demand for cheap energy, we need to expand our infrastructures and upgrade technologies to provide clean and more efficient energy.</p></li>
<li><p><a href="https://www.undp.org/sustainable-development-goals/sustainable-cities-and-communities" target=_blank>Goal 11: Sustainable cities and communities</a>: even if this goal is mainly related with the development of urban areas, the general principle is that sustainable development cannot be achieved without significantly transforming the way we build and manage our resources.</p></li>
<li><p><a href="https://www.undp.org/sustainable-development-goals/responsible-consumption-and-production" target=_blank>Goal 12: Responsible consumption and production</a>: this goal requires that we urgently reduce our ecological footprint by changing the way we produce and consume goods and resources.</p></li>
</ul>
<p>Indeed, it is important to use clean energy and wisely manage our resources but also avoid to waste energy for deprecabile AI computations.</p>
<p>For this reason, we explore concepts such as <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/greenAI"><span class="doc">Green AI</span></a>, <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/edge_computing"><span class="doc">Edge Computing</span></a>, <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/data_centre"><span class="doc std std-doc">efficient use of data centres</span></a>, and <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/resource_allocation"><span class="doc">Resource Allocation</span></a>.</p>
<blockquote>
<div><p>This entry was written by Francesca Pratesi.</p>
</div></blockquote>
</div>
<div class="toctree-wrapper compound">
<span id="document-Societal_and_Environmental_Wellbeing/greenAI"></span><div class="tex2jax_ignore mathjax_ignore section" id="green-ai">
<h5>Green AI<a class="headerlink" href="#green-ai" title="Permalink to this headline">¶</a></h5>
<p><em>Synonyms</em>: Green IT, Green Computing, ICT sustainability.</p>
<!-- <span style='background:red;color:white'></span> -->
<div class="section" id="in-brief">
<h6>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p>The goal of <strong>Green AI</strong> is to minimise the negative aspects of IT operations on the environment. To do so, computers and IT products can be designed, manufactured and disposed of in an environmentally-friendly manner.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>The concept of Green computing practises came into prominence in 1992, when the Environmental Protection Agency (EPA) launched the Energy Star program. The goals of green computing includes the maximisation of energy efficiency, the reduction of energy consumption.
There are four aspects that are addressed by green IT:</p>
<ul class="simple">
<li><p><em>Green use</em>: use of computers in an eco-friend;y and energy-efficient way by minimising their electricity usage and using their peripheral devices.</p></li>
<li><p><em>Green disposal</em>: disposing of obsolete electronic equipment in an environmentally safe manner by repurposing it or recycling it.</p></li>
<li><p><em>Green design</em>: computers and other devices designed with reduced energy consumption.</p></li>
<li><p><em>Green manufacturing</em>: making computers and other systems with as little waste as possible in order to reduce their environmental impact.</p></li>
</ul>
<p>Government regulatory agencies are also actively working to promote green computing principles by enacting a number of voluntary initiatives and rules. The following strategies can be used by average computer users to make their computing more environmentally friendly:</p>
<ul class="simple">
<li><p>When you are gone from your computer for a lengthy amount of time, use hibernation or sleep mode.</p></li>
<li><p>Instead of desktop PCs, get energy-efficient laptop computers.</p></li>
<li><p>Make sufficient measures for the safe disposal of electronic trash.</p></li>
<li><p>At the end of the day, turn off computers.</p></li>
<li><p>Rather than buying new printer cartridges, refill them.</p></li>
<li><p>Instead of buying a new computer, consider renovating one that already exists.</p></li>
<li><p>Activate the power management options to keep an eye on your energy usage.</p></li>
</ul>
<blockquote>
<div><p>This entry was written by Andrea Rossi, Andrea Visentin and Barry O’Sullivan.</p>
</div></blockquote>
</div>
<div class="toctree-wrapper compound">
<span id="document-Societal_and_Environmental_Wellbeing/power_aware"></span><div class="tex2jax_ignore mathjax_ignore section" id="power-aware-computing">
<h6>Power-aware Computing<a class="headerlink" href="#power-aware-computing" title="Permalink to this headline">¶</a></h6>
<p><em>Synonyms</em>: Energy-aware computing, Energy-efficient computing.</p>
<div class="section" id="in-brief">
<h7>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h7>
<p><strong>Power-aware computing</strong> is part of the <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/greenAI"><span class="doc">Green AI</span></a>. Power-aware design strategies strive to maximise performance in high-performance systems within power dissipation and power consumption constraints. Reduced power utilisation on a node is one way to reduce the amount of energy required to compute. Lowering the frequency at which the CPU works on one approach to do this. Reduced clock speed, on the other hand, increases the time to solution, posing a potential compromise.</p>
</div>
<div class="section" id="more-in-detail">
<h7>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h7>
<p>Low-power design strategies attempt to decrease power or energy consumption in portable equipment in order to fulfil a particular performance or throughput objective. All of the energy used by a system ultimately dissipates and is converted to heat. <strong>Power dissipation</strong> and related thermal issues have an impact on performance, packaging, reliability, environmental impact, and heat removal costs; <strong>power and energy consumption</strong> have an impact on power delivery costs, performance, and reliability, and they are directly related to portable device size and battery life.</p>
<p>Power-aware computing’s major purpose is to save energy while routing communications from source to destination. The contemporary period is characterised by wireless networks, in which nodes connect with one another over many hops. In this technology, many data transfer protocols are employed. Different routing techniques will draw more power from the battery. In power-aware computing, power-aware routing metrics have played an essential role.</p>
<blockquote>
<div><p>This entry was written by Andrea Rossi, Andrea Visentin and Barry O’Sullivan.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
<span id="document-Societal_and_Environmental_Wellbeing/cloud_computing"></span><div class="tex2jax_ignore mathjax_ignore section" id="cloud-computing">
<h5>Cloud Computing<a class="headerlink" href="#cloud-computing" title="Permalink to this headline">¶</a></h5>
<p><em>Synonyms</em>: Mesh Computing.</p>
<div class="section" id="in-brief">
<h6>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p><strong>Cloud computing</strong> is the provision of computing resources (storage and processing power) on demand, without direct user intervention. A large cloud often includes multiple data centres, each housing a different set of functions. The cloud computing model aims to achieve core economies of scale through sharing of resources, taking advantage of a “pay-as-you-go” model that can decrease capital expenditures, but can also result in unforeseen operating expenses for users who are unaware of the concept.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>With cloud computing, users are able to utilise any or all of these technologies, without having to know a great deal about them or being an expert in them. As a result, cloud computing reduces overhead and lets users focus on expanding their business rather than on IT problems.</p>
<p>Virtualization is the primary enabling technology for cloud computing. By separating a physical computing device into multiple virtual ones, each of which can easily be managed and used for computing operations, virtualization software allows companies to reduce costs and increase efficiency. By using operating system-level virtualization, idle computing resources can be allocated more efficiently and turned into a scalable system of independent computing devices. Increasing utilisation of the infrastructure through virtualization reduces costs and speeds up IT operations.</p>
<p>In autonomous computing, users can provision resources on-demand by automating a number of steps. Through automation, processes become more efficient, labour costs are reduced, and the possibility of human error is reduced. In the context of <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/greenAI"><span class="doc">Green AI</span></a>, cloud computing addresses two major challenges - energy consumption and resource consumption.</p>
<p>Cloud computing can significantly lower carbon emissions and energy use thanks to <strong>virtualization</strong>, <strong>dynamic provisioning environment</strong>, <strong>multi-tenancy</strong>, and <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/data_centre"><span class="doc std std-doc"><strong>green data centre technologies</strong></span></a>. Certain on-premises applications can be moved into the cloud by large enterprises and small businesses in order to reduce their energy consumption and carbon emissions. People can buy products and services online without having to travel to physical stores, reducing greenhouse gas emissions associated with travel. One example would be online shopping, where they purchase products online without having to drive and waste fuel to reach the physical stores.</p>
<blockquote>
<div><p>This entry was written by Andrea Rossi, Andrea Visentin and Barry O’Sullivan.</p>
</div></blockquote>
</div>
</div>
<span id="document-Societal_and_Environmental_Wellbeing/edge_computing"></span><div class="tex2jax_ignore mathjax_ignore section" id="edge-computing">
<h5>Edge Computing<a class="headerlink" href="#edge-computing" title="Permalink to this headline">¶</a></h5>
<p><em>Synonyms</em>: Fog Computing.</p>
<div class="section" id="in-brief">
<h6>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p><strong>Edge computing</strong> is a distributed computing paradigm in which processing and data storage are brought closer to the data sources. This should increase response times while also conserving bandwidth. Rather than referring to a single technology, the phrase refers to an architecture.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>Edge computing is defined as any form of computer software that provides reduced latency closer to the requests. It can be defined as any computing outside the cloud that happens at the network’s edge, and more particularly in applications where real-time data processing is necessary. While cloud computing works with huge data, edge computing works with immediate data or data created in real-time by sensors or users. Virtualization technology may be used in edge computing to make it easier to deploy and execute a wide range of applications on edge servers.</p>
<p>Edge computing has its roots in content distributed networks, which were developed in the late 1990s to provide web and video content from edge servers located near consumers. In The early 2000s, these networks expanded to host applications and application components at edge servers, leading to the first commercial edge computing services, which hosted applications including dealer locators, shopping carts, real-time data aggregators, and ad insertion engines. Edge and fog computing are examples of new technologies that can help reduce energy use. These technologies enable redistribution computing closer to the user, lowering network energy costs. Furthermore, having fewer data centres reduces the amount of energy consumed in operations such as refrigeration and maintenance.</p>
<blockquote>
<div><p>This entry was written by Andrea Rossi, Andrea Visentin and Barry O’Sullivan.</p>
</div></blockquote>
</div>
</div>
<span id="document-Societal_and_Environmental_Wellbeing/data_centre"></span><div class="tex2jax_ignore mathjax_ignore section" id="data-centre">
<h5>Data Centre<a class="headerlink" href="#data-centre" title="Permalink to this headline">¶</a></h5>
<div class="section" id="in-brief">
<h6>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p>A <strong>data centre</strong> is a structure, a specialised area inside a structure, or a collection of structures used to house computer systems and related components such as telecommunications and storage systems. Because IT operations are so important for business continuity, they usually incorporate redundant or backup components and infrastructure for power, data transmission connections, environmental control (such as air conditioning and fire suppression), and other security systems. A huge data centre is a large-scale activity that consumes the same amount of power as a small town.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>Data centres in the field of enterprise IT are meant to serve business applications and operations such as email communication and file sharing, applications for productivity, management of customer relationships, databases and enterprise resource planning, machine learning, AI and big data, communications and collaboration services, as well as virtual desktops. Routers, switches, firewalls, storage systems, servers, and application delivery controllers are all part of the data centre design.</p>
<p><em>Data centre security</em> is crucial in data centre architecture because these components hold and handle business-critical data and applications. They provide services such as computing resources and network and storage infrastructure. Data centre facilities are energy guzzlers, accounting for between 1.1 and 1.5 percent of total global energy consumption in 2010 <span id="id1">[<a class="reference internal" href="#id2821">1</a>]</span>.</p>
<p>Data centre facilities use to 100 to 200 times more energy than ordinary office buildings, according to the US Department of Energy <span id="id2">[<a class="reference internal" href="#id2822">2</a>]</span>. From the IT equipment to the HVAC (heating, ventilation and air conditioning) equipment to the actual location, configuration, and construction of the building, an energy efficient data centre design should handle all the energy usage issues contained in a data centre.</p>
<p>The US department of energy has identified five major arrears where energy efficient data centre architecture best practises should be focused:</p>
<ul class="simple">
<li><p>IT systems,</p></li>
<li><p>environmental conditions,</p></li>
<li><p>air quality control,</p></li>
<li><p>refrigeration systems,</p></li>
<li><p>and systems involving electricity.</p></li>
</ul>
<p>On-site electricity production and waste heat recycling are two more energy-efficient design options recommended.</p>
<p>Data centre design that is energy efficient should assist to better use a data centre’s space while also increasing performance and efficiency.</p>
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p id="id3"><dl class="citation">
<dt class="label" id="id2821"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Edward Curry, Bill Guyon, Charles Sheridan, and Brian Donnellan. Developing a sustainable it capability: lessons from Intel's journey. <em>MIS Quarterly Executive</em>, 11(2):61–74, 2012.</p>
</dd>
<dt class="label" id="id2822"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>VanGeet, Otto, W. Lintner, and B. Tschudi. FEMP best practises guide for energy-efficient data centre design. 2011. National Renewable Energy Laboratory.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Andrea Rossi, Andrea Visentin and Barry O’Sullivan.</p>
</div></blockquote>
</div>
</div>
<span id="document-Societal_and_Environmental_Wellbeing/cradle_to_cradle"></span><div class="tex2jax_ignore mathjax_ignore section" id="cradle-to-cradle-design">
<h5>Cradle-to-cradle Design<a class="headerlink" href="#cradle-to-cradle-design" title="Permalink to this headline">¶</a></h5>
<p><em>Synonyms</em>: 2CC2, C2C, cradle 2 cradle, regenerative design.</p>
<div class="section" id="in-brief">
<h6>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p><strong>Cradle-to-cradle Design</strong>  is a biomimetic approach to product and system design that mimics natural processes, in which materials are considered as nutrients flowing in healthy, safe metabolisms.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>Cradle to Cradle is a philosophy that views rubbish as an infinite resource and encourages people to do the right thing from the start. It’s about making community and product development work like a healthy ecological system, where all resources are utilised efficiently and in a cyclical manner. Industry, according to C2C, should safeguard and enhance ecosystems and nature’s biological metabolism. It is a comprehensive economic, industrial, and social framework aimed at creating jobs that are efficient and waste-free. The concept may be applied to many facets of human civilization, including urban landscapes, buildings, economy, adn social systems.</p>
<blockquote>
<div><p>This entry was written by Andrea Rossi, Andrea Visentin and Barry O’Sullivan.</p>
</div></blockquote>
</div>
</div>
<span id="document-Societal_and_Environmental_Wellbeing/resource_prediction"></span><div class="tex2jax_ignore mathjax_ignore section" id="resource-prediction">
<h5>Resource Prediction<a class="headerlink" href="#resource-prediction" title="Permalink to this headline">¶</a></h5>
<p><em>Synonyms</em>: Workload Prediction, Workload Forecast.</p>
<div class="section" id="in-brief">
<h6>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p><strong>Resource prediction</strong> is the estimation of the resources a customer will require in the future to complete his tasks.  This concept has a wide variety of application and it is particularly studied in the context of data centres management. When these forecasts are generated, historical and current data are utilised to predict how many resource units, which tools and operative systems and the number of requests are required to accomplish a task.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>A resource prediction ensures that the resource pool is proportionate to the workload. To accomplish efficient work scheduling and load balancing in cloud computing, accurate resource requests forecast is required. It is vital for a competitive service to ensure that resources are available to fulfil demand as it arises. Cloud computing companies want to preconfigure computers ahead of time in order to deliver a high <strong>Quality of Service</strong> (QoS), which includes low latency, high availability and high dependability. If demand can be precisely forecast, suppliers may expect greater resource usage and a reduction on pre configured but idle computers, in addition to a high Quality of Service.</p>
<p>Cloud service demand, on the other hand, is difficult to forecast due to factors such as variety, size, burst and uncertainty. Service providers would be able to make a principled trade-off between QoS and resource cost if they had an accurate model of demand variance across time. The topic is widely studied in the literature and includes many algorithms and techniques applied over more than a decade, from statistical models to Machine Learning and Deep Learning models.</p>
<p>The benefits of predicting the future demands include better resource utilisation and a reduction of the overall location with the opportunity of serving more customers, which leads to an increase in profit and an overall reduction of energy and maintenance costs, with a possible improvement in environmental impact.</p>
<blockquote>
<div><p>This entry was written by Andrea Rossi, Andrea Visentin and Barry O’Sullivan.</p>
</div></blockquote>
</div>
</div>
<span id="document-Societal_and_Environmental_Wellbeing/resource_allocation"></span><div class="tex2jax_ignore mathjax_ignore section" id="resource-allocation">
<h5>Resource Allocation<a class="headerlink" href="#resource-allocation" title="Permalink to this headline">¶</a></h5>
<div class="section" id="in-brief">
<h6>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p>Cloud computing provides a computing environment where businesses, clients, and projects can lease resources on demand. Both cloud users and providers want to allocate cloud resources efficiently and profitably. These resources are typically scarce, therefore cloud providers must make the best use of them while staying within the confines of the cloud environment and meeting the demands of cloud apps so that they may perform their jobs. The distribution of resources is one of the most important aspects of cloud computing. Its efficiency has a direct impact on the overall performance of the cloud environment. Cost efficiency, reaction time, reallocation, computing performance, and job scheduling are all key difficulties in resource allocation. Cloud computing users want to do task for the least amount of money feasible.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>The provision of services and storage space for certain tasks specified by users is referred to as resource allocation. Different resource allocation mechanisms are used to accomplish this. Resource allocation techniques entail combining cloud provider operations when assigning and utilising scarce cloud resources, as well as addressing the demands of cloud applications so that they may accomplish their goals. The two stakeholders in a cloud computing system, cloud consumers and cloud providers, have distinct objectives. Cloud providers encourage customers to utilise as much of their resources as possible in order to increase earnings, whereas consumers have the opposite purpose in mind. They aim to reduce their cloud computing costs while maintaining their performance requirements. There are a variety of ways for achieving a balance between resource allocation and cost. The measures assist in avoiding:</p>
<ul class="simple">
<li><p><em>Over-provisioning</em>: happens when the amount of cloud resources available exceeds the amount of resources needed.</p></li>
<li><p><em>Under-provisioning</em>: occurs when the provided resources are insufficient to meet the demand.</p></li>
<li><p><em>Resource fragmentation</em>: is a problem that occurs when a system’s resources are unavailable. The available resources are unable to distribute themselves to the needed users.</p></li>
<li><p><em>Resource contention</em>: when two or more applications in the cloud system want to utilise the same computational resource in the same instance.
Cloud computing technology continues to be used by businesses for a variety of purposes, including enhancing productivity, lowering cloud costs, ensuring data security and storing unlimited data. Knowledge of resource allocation is becoming increasingly important for cloud customers that seek to reduce their cloud use expenses. While there are several techniques for distributing resource in the cloud, the most successful method ison that optimises cloud service provider revenu while also ensuring cloud user pleasure.</p></li>
</ul>
<blockquote>
<div><p>This entry was written by Andrea Rossi, Andrea Visentin and Barry O’Sullivan.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
<span id="document-Societal_and_Environmental_Wellbeing/social_impact"></span><div class="tex2jax_ignore mathjax_ignore section" id="social-impact-of-ai-systems">
<h4>Social Impact of AI Systems<a class="headerlink" href="#social-impact-of-ai-systems" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p>Artificial intelligence (AI) is quickly changing the way we work and the way we live. Indeed, the <strong>societal impact of AI</strong> is on most people’s minds.</p>
</div>
<div class="section" id="more-in-detail">
<h5>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h5>
<p>AI development has increased rapidly in recent years, leading to the onset of what experts now consider the AI revolution. Similar to previous periods of rapid development such as the industrial revolution, the AI revolution is expected to impact society in a myriad of ways, altering the current state of the art in many domains. While some consequences of the AI revolution are considered to be well-understood such as the impact on certain repetitive, easily-automated tasks, others are still unknown. This is especially important as experts consider the AI revolution to still be in infancy and expect it to continue to rapidly develop throughout the next 100 years <span id="id1">[<a class="reference internal" href="#id2105">1</a>]</span>. In order to gauge the potential harm of any new AI system, the European Union has proposed a risk-based system of assessing the safety of new AI models to ensure that potential negative consequences are known and targeted prior to the implementation of the system. This system is designed to protect society from the potential harms of AI and risk levels are designated based on the perceived harm of society. Each level of risk and the associated AI models are discussed in greater detail below. In addition we recommend a self-assessment tool to gauge the level of risk of a given AI system.</p>
<div class="section" id="spam-filters-and-video-games-minimal-risk-ai">
<h6>SPAM Filters and Video Games: Minimal Risk AI<a class="headerlink" href="#spam-filters-and-video-games-minimal-risk-ai" title="Permalink to this headline">¶</a></h6>
<p>At the lowest risk category, “Minimal Risk”, the European Union targets systems which pose virtually no harm to the privacy, safety or rights of users. These systems are subject to no regulation as they are considered to be harmless to end-users and can therefore operate freely in the European market. This risk category includes SPAM filters such as those commonplace in most email providers and video games, which may use AI for procedural-content generation throughout the game, adjusting ambient conditions depending on user action. However, some expert groups expect harsher regulation in these areas in the future as AI holds the potential to generate highly-realistic harmful content, which may endanger the safety and privacy of citizens.</p>
</div>
<div class="section" id="generative-models-and-chatbots-limited-risk-ai">
<h6>Generative Models and ChatBots: Limited Risk AI<a class="headerlink" href="#generative-models-and-chatbots-limited-risk-ai" title="Permalink to this headline">¶</a></h6>
<p>Legislation becomes relevant at the second lowest risk-tier “Limited Risk”, where AI systems are considered to pose minimal risk to user safety, rights and privacy but nonetheless require user awareness. As such, developers are obligated to ensure that users are aware that they are interacting with an AI in order to prevent misunderstandings as well as potential unhealthy attachments (See also <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/human_interaction"><span class="doc std std-doc">AI human interaction</span></a>} and <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/self-identification"><span class="doc std std-doc">Self-identification of AI</span></a>). At this tier of risk, generative models such as DALL-E are found, which utilize AI to generate content. In addition, chat bots including ChatGPT are considered to pose Limited Risk and therefore need to be clearly identifiable as AI by end users.</p>
</div>
<div class="section" id="self-driving-cars-and-medicine-high-risk-ai">
<h6>Self-Driving Cars and Medicine: High Risk AI<a class="headerlink" href="#self-driving-cars-and-medicine-high-risk-ai" title="Permalink to this headline">¶</a></h6>
<p>At the highest acceptable tier of Risk, the European Union designates systems, which may under proper legislation be beneficial to end users but pose considerable harm to the privacy, security and rights of society. As such, strict legislation is implemented for these systems, requiring thorough risk assessments of the AI, implemented safeguards to prevent bias and protect privacy and human oversight to prevent misuse. Additionally, the AI needs to be transparent in order to make it auditable and gauge the cause of potentially harmful decisions. At this level, developments such as self-driving cars are found, as malfunction within the AI could cause considerable harm to society and, as such, rigorous testing is necessary before the product can be implemented. In addition, medical AI mechanisms are considered high risk as they could lead to discrimination or oversight if not properly tested and used alongside human supervision.</p>
</div>
<div class="section" id="social-scoring-and-exploitation-unacceptable-risk-ai">
<h6>Social Scoring and Exploitation: Unacceptable Risk AI<a class="headerlink" href="#social-scoring-and-exploitation-unacceptable-risk-ai" title="Permalink to this headline">¶</a></h6>
<p>At the highest tier, classed ‘Unacceptable Risk’, AI systems which pose non-justifiable risk to society are found. These AI systems are outlawed by the European Union as they are considered to pose unacceptable risk to the rights, privacy and security of citizens without yielding a corresponding positive outcome. In this category, systems implementing social scoring are found (see also Section \ref{section: AI for social scoring}). In addition, systems which exploit vulnerable groups such as AI generated targeted ads for gambling are considered unacceptable.</p>
</div>
<div class="section" id="altai-self-assessment-of-ai-risk">
<h6>ALTAI: Self-Assessment of AI Risk<a class="headerlink" href="#altai-self-assessment-of-ai-risk" title="Permalink to this headline">¶</a></h6>
<p>When designing a new AI system, developers must assess the level of risk they are operating at and implement the corresponding safety measures as designated by the European Union. In order to ease this process, a self-assessment questionnaire was designed that allows developers to easily determine the level of risk of their AI. The questionnaire can be found here: <a href="https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment" target=_blank>Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment</a>.</p>
</div>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id2"><dl class="citation">
<dt class="label" id="id2105"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Spyros Makridakis. The forthcoming artificial intelligence (ai) revolution: its impact on society and firms. <em>Futures</em>, 90:46–60, 2017.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Nicola Rossberg and Andrea Visentin.</p>
</div></blockquote>
</div>
<div class="toctree-wrapper compound">
<span id="document-Societal_and_Environmental_Wellbeing/human_interaction"></span><div class="tex2jax_ignore mathjax_ignore section" id="ai-human-interaction">
<h5>AI human interaction<a class="headerlink" href="#ai-human-interaction" title="Permalink to this headline">¶</a></h5>
<div class="section" id="in-brief">
<h6>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p>As AI develops to learn and adapt, it is increasingly being perceived as human-like in both its appearance and intelligence. This ability to imitate human behavior and interactions creates anthropomorphic cues which cause users to form emotional bonds <span id="id1">[<a class="reference internal" href="TAILOR.html#id2825">1</a>]</span>. As AI becomes more commonplace in our daily lives, being integrated into virtual assistants, recommender systems etc. it is important to assess how the system’s anthropomorphic cues affect our interaction with it. In addition, potential risks resulting from AI-human interaction should be assessed prior to any implementation of AI as previous research has demonstrated the unpredictability of AI.</p>
</div>
<div class="section" id="more-in-detail">
<h6>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h6>
<p>While AI systems have been implemented in industry applications such as price forecasting for many years, they have only recently been piloted for end-user interaction. This was primarily enabled by the onset of large language models, which have allowed for competent AI based virtual assistants to become commonplace on a variety of platforms. This section will assess the factors which impact AI-human interaction and how both human and AI traits can affect human perception of a AI. Next, potential adverse outcomes resulting from poor AI-human interaction identified in the literature are assessed. Finally, recommendations for AI design choices leading to successful AI-human interactions are provided.</p>
<div class="section" id="factors-of-ai-human-interaction">
<h7>Factors of AI-Human Interaction<a class="headerlink" href="#factors-of-ai-human-interaction" title="Permalink to this headline">¶</a></h7>
<p>Three factors of an AI are found to affect the anthropomorphic response of humans to AI interactions: appearance, cognitive intelligence and emotional intelligence <span id="id2">[<a class="reference internal" href="TAILOR.html#id2825">1</a>]</span>. Cognitive intelligence is found to be the most vital factor in triggering an anthropomorphic response but must co-occur with high quality design in order to be perceived as human-like. As such, systems which exceed in both design and intelligence are found to be perceived as most human-like by users and are hence most likely to cause the formation of emotional bonds. Based on this, developers should consider the impact of their systems design and function on the human anthropomorphic response before implementation in order to gauge any potential problems caused by the system. This is especially important in regard to vulnerable groups, which may form maladaptive attachments to AI which could cause harm. Consequentially the impact of the system on AI-human interaction must be assessed prior to implementation and steps to allow self-identification and regulate emotional impact must be taken.</p>
</div>
<div class="section" id="consequences-of-ai-human-interaction">
<h7>Consequences of AI-Human Interaction<a class="headerlink" href="#consequences-of-ai-human-interaction" title="Permalink to this headline">¶</a></h7>
<p>The implementation of AI to communicate with end-users has been seen in several different fields. Here we will be assessing the usage of AI in medicine. For each application, the current usage will be briefly elaborated and the potential risks and benefits discussed.</p>
<p>In the medical field, AI holds the potential to be implemented as a medical chatbot. These bots may be implemented for mental health counseling to support practitioners and provide low-cost assistance to patients. To produce human-level text, these bots utilize natural language processing, which allows the analysis of user utterances, which may allow the detection of sentiment changes. Based on this, machine learning algorithms may be able to detect patient outcomes and behavior changes. Through this technology, AI holds the potential to alleviate the substantial load on mental health professionals and provide support for patients in need. In addition, AI chatbots may support those unable to communicate with mental health providers due to stigmatization. Finally, previous research has demonstrated potential positive effects of AI chatbots over traditional therapy-alternatives <span id="id3">[<a class="reference internal" href="#id2832">2</a>]</span>.</p>
<p>However, despite these positive implications of AI in healthcare, the technology still presents major limitations, which block its successful implementation. First, the AI may not be capable of expressing the empathy and understanding required by an end-user leading to drop-out and decreased success rates. In addition, a self-learning AI bot may develop new conversational rules which deviate from existing protocol and could present harm to end-users. Finally, chatbots based on LLMs require large amounts of training data, which is difficult to acquire in the medical domain. It is important that chat bots are trained on domain-specific data as generic data may lead to adverse responses which may endanger users.</p>
</div>
<div class="section" id="guidelines">
<h7>Guidelines<a class="headerlink" href="#guidelines" title="Permalink to this headline">¶</a></h7>
<p>The current EU guidelines for AI focus on a risk-based approach to assessing AI safety. Based on this, human-AI interaction falls under the label of limited risk. However, depending on the target domain, this may change as medical chatbots would be considered high risk as they pose a risk of harm to people’s health. As such, developers should consider the potential outcomes of their developed model prior to implementation and consider the according regulations. The regulations applying to limited and high risk applications are briefly summarized below.</p>
<p><strong>Limited Risk</strong>
Limited risk AI systems are those systems which are considered to pose minimal risk of harm to user’s privacy, safety and fundamental rights. Limited risk applications are lightly regulated as they are considered to not pose major harm to end-users. However, systems must make it explicit to the end-user that they are communicating with an AI in order to avoid misunderstandings.</p>
<p><strong>High Risk</strong>
High-risk AI systems, as defined by the EU AI Act, are those that present a significant potential threat to people’s safety, well-being, and fundamental rights. These systems are subject to stricter regulations to mitigate these risks.
Developers of high-risk AI systems must conduct thorough risk assessments, ensure human oversight to prevent misuse, and implement robust data management practices to minimize bias and privacy risks. For these systems, there is a greater emphasis on transparency in decision-making processes, which helps identify and address potential biases or errors.
Classifying AI systems as high-risk aims to proactively manage potential dangers and promote the responsible development and deployment of AI technology. This approach seeks to ensure that AI benefits society while protecting people’s well-being and fundamental rights.
As such, AI systems which interact with users in a medical domain and may have frequent interactions with vulnerable audiences must undergo stringent testing as well as prove to be transparent in their decisions in order to ensure user safety[^secureprivacy].</p>
</div>
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p id="id4"><dl class="citation">
<dt class="label" id="id2827"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>)</span></dt>
<dd><p>Joohee Kim and Il Im. Anthropomorphic response: understanding interactions between humans and artificial intelligence agents. <em>Computers in Human Behavior</em>, 139:107512, 2023.</p>
</dd>
<dt class="label" id="id2832"><span class="brackets"><a class="fn-backref" href="#id3">2</a></span></dt>
<dd><p>Kerstin Denecke, Alaa Abd-Alrazaq, and Mowafa Househ. Artificial intelligence for chatbots in mental health: opportunities and challenges. <em>Multiple perspectives on artificial intelligence in healthcare: Opportunities and challenges</em>, pages 115–128, 2021.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Nicola Rossberg and Andrea Visentin.</p>
</div></blockquote>
<p>[^secureprivacy] <a href="https://secureprivacy.ai/\_2024" target=_blank>https://secureprivacy.ai/_2024</a>.</p>
</div>
<div class="toctree-wrapper compound">
<span id="document-Societal_and_Environmental_Wellbeing/self-identification"></span><div class="tex2jax_ignore mathjax_ignore section" id="self-identification-of-ai">
<h6>Self-identification of AI<a class="headerlink" href="#self-identification-of-ai" title="Permalink to this headline">¶</a></h6>
<div class="section" id="in-brief">
<h7>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h7>
<p>AI systems which are presented to be both high in intelligence and have a good quality design tend to cause humans to anthropomorphize the system; i.e. assign human-like traits to it which can lead to emotional bonding with the system <span id="id1">[<a class="reference internal" href="#id2825">1</a>]</span>. This is dangerous, especially for vulnerable groups such as older generations or those less acquainted with the mechanisms behind the AI. In addition to the threat of emotional attachment, AI systems have been found to frequently provide false or partially false information and portrait it as fact, due to a phenomenon referred to as AI hallucinations. As such it is important that users can clearly identify an artificial intelligence as such, in order to understand the system’s limitations and prevent emotional attachment and misinformation. The remaining part of this chapter will first explain which AI systems are required to be clearly identifiable and then elaborate on how these criteria should be met.</p>
</div>
<div class="section" id="more-in-details">
<h7>More in Details<a class="headerlink" href="#more-in-details" title="Permalink to this headline">¶</a></h7>
<!--\subsubsection{Applicable Guidelines}-->
<p>Under the EU AI act, guidelines for the identification of AI are provided for developers. Systems which are classified to pose at least limited risk to the user, must be clearly identifiable as AI. This aims to prevent both misunderstandings and accidental misinformation due to AI hallucinations. Any system which interacts with humans is considered limited risk. In addition, systems including recommender systems, generative models and emotion-recognition systems are also considered to be limited risk and must consequently be identifiable as AI. The <a href="https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment" target=_blank>ALTAI self-assessment checklist</a> can be used to assess the risk level posed by an algorithm and the consequential precautions which should be implemented.</p>
<div class="section" id="making-the-system-identifiable">
<h8>Making the system identifiable<a class="headerlink" href="#making-the-system-identifiable" title="Permalink to this headline">¶</a></h8>
<p>If a system is considered to pose at least limited risk, it must be clearly identifiable as AI by the end-user. In order to meet this criterion, developers should consult the following points and ensure that the system is in compliance with all:</p>
<ol class="simple">
<li><p>The AI system must be identifiable as an AI system.</p>
<ul class="simple">
<li><p>This should not be hidden in jargon such as ‘large language model’ but communicated in simple language</p></li>
</ul>
</li>
<li><p>When needed to comply with fundamental rights, the option to decide against AI interaction in favor of human interaction should be provided.</p></li>
<li><p>The AI system’s capabilities and limitations should be communicated to AI practitioners or end-users in a manner appropriate to the use case at hand</p>
<ul class="simple">
<li><p>This may include the AI system’s level of accuracy and its limitations</p></li>
</ul>
</li>
</ol>
</div>
<div class="section" id="guidelines">
<h8>Guidelines<a class="headerlink" href="#guidelines" title="Permalink to this headline">¶</a></h8>
<p>Under the EU AI act, guidelines for the identification of AI are provided for developers. Systems which are classified to pose at least limited risk to the user, must be clearly identifiable as AI. This is aimed to prevent both misunderstandings and accidental misinformation due to AI hallucinations. Any system which interacts with humans is considered limited risk. In addition, systems including recommender systems, generative models and emotion-recognition systems are also considered to be limited risk and must consequently be identifiable as AI. The <a href="https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment" target=_blank>ALTAI self-assessment checklist</a> can be used to assess the risk level posed by an algorithm and the consequential precautions which should be implemented.</p>
</div>
<div class="section" id="key-words">
<h8>Key Words<a class="headerlink" href="#key-words" title="Permalink to this headline">¶</a></h8>
<ul class="simple">
<li><p><em>AI Hallucinations</em>: AI hallucinations refer to instances where artificial intelligence systems, particularly generative models like GPT-4, produce incorrect, misleading, or nonsensical information that is presented as factual.</p></li>
<li><p><em>Recommender Systems</em>: Recommender systems are a type of information filtering system that seeks to predict the preferences or interests of users and make personalized suggestions accordingly. Commonly used in online platforms such as Netflix, Amazon, and Spotify, these systems analyze user behavior and preferences to recommend movies, products, music, or other content.</p></li>
<li><p><em>Generative Models</em>: Generative models are a class of machine learning models that can generate new data instances similar to a given set of training data. These models learn the underlying patterns and distribution of the training data and can create new content, such as text, images, or audio, that mimics the original data.</p></li>
<li><p><em>Emotion-Recognition Systems</em>: Emotion-recognition systems are AI systems designed to identify and interpret human emotions from various data sources, such as facial expressions, voice tones, text, or physiological signals.</p></li>
<li><p><em>Large Language Model</em>: A large language model is a type of artificial intelligence model that has been trained on vast amounts of text data to understand and generate human language. These models, like GPT-4, use deep learning techniques to learn the complexities of language, including grammar, context, and nuances, enabling them to perform tasks such as translation, summarizing, and conversation generation with a high degree of fluency and coherence.</p></li>
</ul>
</div>
</div>
<div class="section" id="bibliography">
<h7>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h7>
<p id="id2"><dl class="citation">
<dt class="label" id="id2825"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Joohee Kim and Il Im. Anthropomorphic response: understanding interactions between humans and artificial intelligence agents. <em>Computers in Human Behavior</em>, 139:107512, 2023.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Nicola Rossberg and Andrea Visentin.</p>
</div></blockquote>
</div>
</div>
<span id="document-Societal_and_Environmental_Wellbeing/emotional_impact"></span><div class="tex2jax_ignore mathjax_ignore section" id="emotional-impact">
<h6>Emotional Impact<a class="headerlink" href="#emotional-impact" title="Permalink to this headline">¶</a></h6>
<div class="section" id="in-brief">
<h7>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h7>
<p>Due to the fast development of AI systems, their behavior, in some aspects, has become akin to humans. Especially with the introduction of large language models like chatGPT, it has become increasingly difficult to differentiate human and AI generated language. When coupled with human-like design, this cognitive ability has been found to lead to emotional attachment by users. While emotional impact cannot be prevented entirely, it is important to assess the extent to which the AI encourages human attachment and ensure clear signaling to the end-user that they are interacting with an AI.</p>
</div>
<div class="section" id="more-in-details">
<h7>More in Details<a class="headerlink" href="#more-in-details" title="Permalink to this headline">¶</a></h7>
<p>With the rapid development of AI in recent years, especially in the realm of human-language generation, it has become increasingly difficult to distinguish human and AI generated text. Furthermore, with AI becoming increasingly integrated into daily life, in the form of chat-bots and virtual assistants (such as Siri or Alexa), humans are regularly confronted with artificial intelligence agents mimicking human interaction. Due to these factors, it is important to consider the potential emotional ramifications of the integration of AI into everyday life. Special attention should be paid to potentially vulnerable populations who may not understand the capabilities of AI. The following will discuss previous research on emotional attachment of AI and introduce the European Union guidelines relating to the clear identification of AI systems to avoid emotional attachment.</p>
<div class="section" id="current-state-of-research">
<h8>Current State of Research<a class="headerlink" href="#current-state-of-research" title="Permalink to this headline">¶</a></h8>
<p>Past research has shown that high quality human-like design (e.g. the different voices available for Siri) coupled with high cognitive intelligence can cause users to assign human-like traits to the system and form emotional bonds. Depending on the anthropomorphism tendencies (AT) of the user, systems with good design (for users with high AT) or good cognitive intelligence (for users with low AT) are considered especially human-like. While the formation of attachments may be favorable in certain areas (e.g. when integrating care-robots into hospitals and social-care facilities) it also bears dangers as it may make users more susceptible to potentially harmful suggestions from the system. One prominent example is the tragic suicide of a young man whose attachment to an AI agent resulted in severe anxiety and depression which drove him to take his own life <span id="id1">[<a class="reference internal" href="#id2831">1</a>]</span>. Based on such cases, it is important to clearly communicate the nature of AI agents to users as well as clarify the shortcomings of these agents (see also <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/self-identification"><span class="doc">Self-identification of AI</span></a>.</p>
</div>
<div class="section" id="guidelines-relating-to-emotional-attachments">
<h8>Guidelines Relating to Emotional Attachments<a class="headerlink" href="#guidelines-relating-to-emotional-attachments" title="Permalink to this headline">¶</a></h8>
<p>Emotional attachments are most likely to be formed during direct contact between the AI and the end-user. This scenario takes place most frequently during user interactions with chat-bots such as ChatGPT. These types of systems are classified as posing ‘limited risk’ by the European Union. As such, legislation dictates that systems must be clearly identifiable as AI in order to prevent emotional attachment or misinformation. Prior to the piloting of such an AI system, the following two questions should be assessed in the respective field:</p>
<ol class="simple">
<li><p>Did you assess whether the AI system encourages humans to develop attachment and empathy towards the system?</p></li>
<li><p>Did you ensure that the AI system clearly signals that its social interaction is simulated and that it has no capacities of “understanding” and “feeling”?</p></li>
</ol>
<p>While most AI systems which may encourage emotional attachment are considered limited risk, it is important to note that this may not necessarily always be the case. Developers should always assess the risk of their developed system prior to implementation and follow legislation to ensure user safety. While not all attachment can be prevented, partially due to the high quality and fast development of AI, measures must be taken to prevent unhealthy and dangerous attachment.</p>
</div>
</div>
<div class="section" id="bibliography">
<h7>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h7>
<p id="id2"><dl class="citation">
<dt class="label" id="id2831"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Lauren Walker. Belgian man dies by suicide following exchanges with chatbot. <em>Brussel Times</em>, 2023. URL: <a class="reference external" href="https://www.brusselstimes.com/430098/belgian-man-commits-suicide-following-exchanges-with-chatgpt">https://www.brusselstimes.com/430098/belgian-man-commits-suicide-following-exchanges-with-chatgpt</a>.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Nicola Rossberg and Andrea Visentin.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
<span id="document-Societal_and_Environmental_Wellbeing/workforce_impact"></span><div class="tex2jax_ignore mathjax_ignore section" id="ai-impact-on-the-workforce">
<h5>AI Impact on the Workforce<a class="headerlink" href="#ai-impact-on-the-workforce" title="Permalink to this headline">¶</a></h5>
<div class="section" id="in-brief">
<h6>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p>The impact of Artificial Intelligence on the workforce can be considered from several perspectives. The most common consideration is the loss of jobs or possibility of deskilling of the workforce due to the introduction of AI systems. A second consideration is the increased demand for workers with a computer-science skill set in order to operate and maintain implemented AI systems. A final consideration is the effect of AI-triggered changes for stakeholders. Prior to the introduction of an AI system, risks and consequences in these areas should be assessed and steps to counter negative outcomes should be considered.</p>
</div>
<div class="section" id="more-in-details">
<h6>More in Details<a class="headerlink" href="#more-in-details" title="Permalink to this headline">¶</a></h6>
<p>Similar to previous ages of rapid development, such as the industrial revolution, the AI revolution is expected to have large and unforeseeable impacts on the structure of the workforce. While some changes, such as an increase of demand for workers with a computer-science skill set are currently expected, many alterations to the workforce remain unknown. The remainder of this text will first discuss the expected changes of the AI revolution based on previous revolutions’ trends. It will then elaborate on the considerations developers should make during AI development. Finally, the relevant EU guidelines will briefly be discussed.</p>
<div class="section" id="the-ai-revolution-and-past-trends">
<h7>The AI revolution and past trends<a class="headerlink" href="#the-ai-revolution-and-past-trends" title="Permalink to this headline">¶</a></h7>
<p>With the growing capabilities of AI, concerns about rising unemployment due to AI replacing workers are growing. In previous industrial revolutions, the medium-skilled industry (e.g. factory workers) generally suffered most as their tasks were repetitive and easily automated. However with the advent of AI this is likely to change as systems may replace both low-skilled workers and high-skilled workers (e.g., journalists). However, following the trends of past industrial revolutions, it has been observed that the number of jobs created based on the new technology tends to exceed the number of jobs lost, albeit with a significant time lag between job loss and job creation <span id="id1">[<a class="reference internal" href="#id2824">1</a>]</span>. In the case of the AI revolution, experts are expecting a rise in demand for workers with a computer-science skill set to supervise and maintain novel AI systems. As such, a reasonable expectation would be an initial series of lay-offs in both low-skilled and high-skilled jobs, followed by re-education programs to allow workers to supervise and maintain the systems.</p>
</div>
<div class="section" id="considerations-for-ai-developers">
<h7>Considerations for AI developers<a class="headerlink" href="#considerations-for-ai-developers" title="Permalink to this headline">¶</a></h7>
<p>With this brief background in mind, developers should consider the impact of any proposed AI system on the workforce. Specifically, the impact on all economic stakeholders should be considered including workers who may lose their job, increased demand for workers capable of maintaining the proposed system, potential re-skilling programs as well as the stakeholders of the target industry. If negative outcomes are identified during this assessment, potential ramifications should be presented alongside the system in order to protect the workforce.</p>
</div>
<div class="section" id="eu-guidelines">
<h7>EU Guidelines<a class="headerlink" href="#eu-guidelines" title="Permalink to this headline">¶</a></h7>
<p>Systems which may affect the labor market could operate at all levels of the risk-assessment scale proposed by the European Union. As such, developers should gauge the risk of their developed system and implement the associated safety precautions accordingly. However, it is recommended that alongside the legally required precautions, developers also consider the impact of the developed system on the job market. While a given development may not pose a direct risk to user’s health, the system’s impact on the job market may have adverse consequences, which should be considered prior to implementation. If the system is found to pose a high risk of job loss and consequential security to society, developers should present potential counteracting mechanisms alongside the system.</p>
</div>
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p id="id2"><dl class="citation">
<dt class="label" id="id2824"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Georgios Petropoulos. The impact of artificial intelligence on employment. <em>Praise for Work in the Digital Age</em>, 119:121, 2018.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Nicola Rossberg and Andrea Visentin.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
<span id="document-Societal_and_Environmental_Wellbeing/society_and_democracy"></span><div class="tex2jax_ignore mathjax_ignore section" id="society-and-democracy">
<h4>Society and Democracy<a class="headerlink" href="#society-and-democracy" title="Permalink to this headline">¶</a></h4>
<div class="section" id="in-brief">
<h5>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h5>
<p>The rapid increase of technological development is impacting society in new and partially unpredictable ways. With AI being integrated into all facets of life, ranging from recommender systems for movies to automatic passport control and facial recognition in airports, it is important to understand the negative ramifications this technology may have on society and democracy as a whole. Previously, the development of technologies was frequently guided by the interests of tech giants with profit motivating research <span id="id1">[<a class="reference internal" href="#id2827">2</a>]</span>. While this line of development has produced some impressive systems, there is little consideration for the protection of citizens and the regulation of these technologies for the greater societal good. In order to combat this trend, the European Union introduced the AI act, stipulating legislation for new AI systems depending on the level or risk posed by the system. However, with this increase in legislation, fears are rising about Europe falling behind other, non-legislated countries, in the development of new software. It is such that legislators need to find the balance between regulation and development, to protect democracy and society without adversely affecting the development of the European AI research sector.</p>
</div>
<div class="section" id="more-in-details">
<h5>More in Details<a class="headerlink" href="#more-in-details" title="Permalink to this headline">¶</a></h5>
<p>While there is currently only limited research on the impact of AI on society and democracy, many experts are concerned about potential adverse outcomes if AI is not properly legislated. The AI act introduced by the European Union is the first of its kind to regulate AI in order to protect both end-users and society as a whole. The AI act takes a risk-based approach to the classification of AI, gauging the risk posed by a proposed system to determine the suitable legislation for the system. The following will first briefly discuss the levels of risk proposed by the AI act. In addition, we introduce two sets of supplementary considerations proposed by Catelijne Muller, one of the members of the EU High Level Expert Group on Artificial Intelligence. The first are a series of human rights established by the European Union which should not be endangered by any proposed system. The second consideration is a series of red-lines which should in no event be crossed by a proposed application. These considerations may be used as a supplement to the AI act in order to ensure further system safety. However it is important to note that the AI act must be consulted and abided by in all scenarios.</p>
<p>First, a risk assessment of the developed system must be conducte. <a href="https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment" target=_blank>Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment</a> provides a tool which allows a self-assessment of the expected risk of the algorithm. Depending on the assessed risk-level different legislation may apply to the AI system, as reported in the <span class="xref std std-doc">./../main/Ethical\_Legal\_Framework/AI\_ACT</span> entry.</p>
<div class="section" id="human-right-considerations">
<h6>Human-Right Considerations<a class="headerlink" href="#human-right-considerations" title="Permalink to this headline">¶</a></h6>
<p>Next to the official EU guidelines, developers should consider the impact of the developed technology in the context of its proposed application. If the current guidelines do not cover this area of deployment but nonetheless a high risk is found to exist with the deployment of the AI, developers should collaborate with local legislators to ensure that the algorithm can be safely implemented. The following human rights might be considered in the assessment of AI safety in addition to the EU guidelines (Source: Catelijne Muller, one of the members of the EU High Level Expert Group on Artificial Intelligence)</p>
<ul class="simple">
<li><p>A right to <a class="reference internal" href="TAILOR.html#document-Human_Agency_and_Oversight/Human_Agency_and_Oversight"><span class="doc std std-doc">human autonomy, agency and oversight over AI</span></a>.</p></li>
<li><p>A right to <a class="reference internal" href="TAILOR.html#document-Transparency/Transparency"><span class="doc std std-doc">transparency</span></a> / (explainability)(../Transparency/XAI.md) of AI outcomes, including the right to an explanation of how the AI functions, what logic it follows, and how its use affects the interests of the individual concerned, even if the AI-system does not process personal data, in which case there is already a right to such information under the <a href="https://gdpr-info.eu/" target=_blank>GDPR</a> <span id="id2">[<a class="reference internal" href="TAILOR.html#id8">1</a>]</span>.</p></li>
<li><p>A separate right to physical, psychological and moral Integrity in light of AI profiling, affect recognition.</p></li>
<li><p>A strengthened <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/Privacy_and_Data_Governance"><span class="doc std std-doc">right to privacy</span></a> to protect against AI-driven mass surveillance.</p></li>
<li><p>Adapting the <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/Privacy_and_Data_Governance"><span class="doc std std-doc">right to data privacy</span></a> to protect against indiscriminate, society-wide online tracking of individuals, using personal and non-personal data (which often serves as a proxy for personal identification) Diverging from these rights in exceptional circumstances such as for security purposes should only be allowed under strict conditions and in a proportionate manner.</p></li>
</ul>
</div>
<div class="section" id="red-lines">
<h6>Red-Lines<a class="headerlink" href="#red-lines" title="Permalink to this headline">¶</a></h6>
<p>Finally, all developers should be aware of red lines in the development and deployment of AI. While moral and ethical standards may differ between cultures and individuals, the following considerations are proposed by Catelijne Muller, one of the members of the EU High Level Expert Group on Artificial Intelligence:</p>
<ul class="simple">
<li><p>Indiscriminate use of facial recognition and other forms of bio-metric recognition either by state actors or by private actors.</p></li>
<li><p>AI-powered mass surveillance (using facial/bio-metric recognition but also other forms of AI tracking and/or identification such as through location services, online behaviour, etc.).</p></li>
<li><p>Personal, physical or mental tracking, assessment, profiling, scoring and nudging through bio-metric and behaviour recognition.</p></li>
<li><p>AI-enabled Social Scoring.</p></li>
<li><p>Covert AI systems and deep fakes.</p></li>
<li><p>Human-AI interfaces 20 Exceptional use of such technologies, such as for national security purposes or medical treatment or diagnosis, should be evidence based, necessary and proportionate and only be allowed in controlled environments and (if applicable) for limited periods of time.</p></li>
</ul>
</div>
</div>
<div class="section" id="bibliography">
<h5>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h5>
<p id="id3"><dl class="citation">
<dt class="label" id="id8"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>European Parliament &amp; Council. General data protection regulation. 2016. L119, 4/5/2016, p. 1–88.</p>
</dd>
<dt class="label" id="id2827"><span class="brackets"><a class="fn-backref" href="#id1">2</a></span></dt>
<dd><p>Paul Nemitz. Constitutional democracy and technology in the age of artificial intelligence. <em>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</em>, 376(2133):20180089, 2018.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Nicola Rossberg and Andrea Visentin.</p>
</div></blockquote>
</div>
<div class="toctree-wrapper compound">
<span id="document-Societal_and_Environmental_Wellbeing/social_scoring"></span><div class="tex2jax_ignore mathjax_ignore section" id="ai-for-social-scoring">
<h5>AI for social scoring<a class="headerlink" href="#ai-for-social-scoring" title="Permalink to this headline">¶</a></h5>
<div class="section" id="in-brief">
<h6>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p>Social Scoring is the concept that all daily actions taken by individuals are monitored and scored for their benefit to society as a whole. Based on this scoring system, you are assigned a given value which determines your access to education, healthcare and other public goods. AI lends itself to social scoring as it excels at facial recognition and movement identification. However this poses several technical and ethical problems. The obvious ethical issue is that the goodness of actions is subjective and the ability of individuals to access goods and services necessary for survival should not depend on their perceived ability to contribute to society. Furthermore, the technical issue with social scoring is that the quality of AI evaluation of movement hinges on the quality of its training data, with biased training data leading to biased evaluation by the AI. Social scoring has currently only been proposed by the Chinese government but other countries including the United Kingdom are using automatic facial recognition in surveillance tapes to search for fugitives.</p>
</div>
<div class="section" id="more-in-details">
<h6>More in Details<a class="headerlink" href="#more-in-details" title="Permalink to this headline">¶</a></h6>
<p>Within the EU legislation, AI systems for social scoring are labeled as marking ‘clear harm’ and are banned within the European Union. Social Scoring, as employed by the Chinese government, is a wide-ranging method of evaluating and assigning value to citizen’s daily actions. Through this, citizens are assigned a social score which determines their access to vital goods such as health care. This poses several obvious ethical problems and has been outlawed as part of the EU AI act. As such, social scoring to this degree will not be further discussed in this section. However to a lesser degree, social scoring exists within daily western life already. Apps like Uber use scoring to evaluate customer and driver behavior and create black-lists of users. Insurance companies in New York are allowed to adjust prices based on information about their users they have gathered from social media. While these systems operate on a smaller scale and are at times beneficial to e.g. protect Uber drivers from customers who have behaved poorly in the past, social scoring becomes dangerous when automated through AI.</p>
<div class="section" id="problems-with-social-scoring-and-ai">
<h7>Problems with Social Scoring and AI<a class="headerlink" href="#problems-with-social-scoring-and-ai" title="Permalink to this headline">¶</a></h7>
<p>While AI systems excel at pattern recognition and identification and Large Multimodal Models especially allow for the successful identification of actions and behavior based on video footage, AI systems only perform as well as the data they are trained on. However, due to biased collection procedures and the difficulty of collecting data sets large enough to train data-hungry AI models, it is difficult to obtain a non-biased training dataset. As a consequence the AI mechanisms trained on this data develop biased classification mechanisms, which systematically disadvantage certain groups, leading to wide-reaching negative consequences. As such, next to the ethical and moral concerns about the implementation of social scoring systems, AI is also not suitable for this task as it would most likely further perpetuate already present biases.</p>
</div>
<div class="section" id="guidelines">
<h7>Guidelines<a class="headerlink" href="#guidelines" title="Permalink to this headline">¶</a></h7>
<p>State-run social scoring falls under the ‘Unacceptable’ level of risk proposed by the EU AI act and as such is outlawed by the European Union. However smaller-level social scoring such as the ratings of customers by apps may be classed as high risk as it determines access to services and benefits. As such, someone with a poor rating on an app like Uber may lose access to the app’s transportation services. High risk applications are strictly legislated by the EU and require thorough assessment prior to implementation. This includes assessing potential risks posed by the app as well as the implementation of transparency to allow auditing of decision mechanisms by the algorithm. If an algorithm is auditable, justifications for given decisions can be provided, and users can understand why they may have been e.g. blacklisted for a given service.</p>
<blockquote>
<div><p>This entry was written by Nicola Rossberg and Andrea Visentin.</p>
</div></blockquote>
</div>
</div>
</div>
<span id="document-Societal_and_Environmental_Wellbeing/propaganda"></span><div class="tex2jax_ignore mathjax_ignore section" id="ai-for-propaganda">
<h5>AI for propaganda<a class="headerlink" href="#ai-for-propaganda" title="Permalink to this headline">¶</a></h5>
<div class="section" id="in-brief">
<h6>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h6>
<p>With the onset of large language models, the mass production of text has become considerably cheaper and faster, requiring less human involvement. With the addition of automatic image generators such as Dall-E, the same is now possible for pictures. Previous work has found that GPT-3, the antecedent to ChatGPT is capable of creating text equally persuasive as content from existing covert propaganda <span id="id1">[<a class="reference internal" href="#id2829">1</a>]</span>. This causes a rising concern regarding the ease at which a large number of influential texts can be circulated using online platforms. To combat mass-propaganda, it is important that online and offline platforms have effective controls to identify AI-generated materials in their publications.</p>
</div>
<div class="section" id="more-in-details">
<h6>More in Details<a class="headerlink" href="#more-in-details" title="Permalink to this headline">¶</a></h6>
<p>Propaganda as a tool for political persuasion is not novel and has been used to spread political information for centuries. However, with the onset of the AI revolution, the generation and publication of propaganda has been greatly eased. As a consequence, there is a rise in concerns about the impact and negative ramifications of AI-generated political messaging. The following first discusses the use of Large Language Models in the generation of propaganda and their perception by the public. We then discuss the use of recommender systems to create a political echo chamber. Finally we outline the section of the EU AI act which may be applicable to AI-generated propaganda.</p>
<div class="section" id="large-language-models">
<h7>Large Language Models<a class="headerlink" href="#large-language-models" title="Permalink to this headline">¶</a></h7>
<p>The high quality of text produced by large language models such as ChatGPT has caused rising concerns about an increasing amount of AI-generated content circulating online. While careful analysis and machine learning models may allow for the differentiation of AI and Human generated text, this may not be possible for end-users \cite{liao2023differentiating, herbold2023large}. Additionally, some preliminary research has found that given the correct prompts and some baseline human scanning, large language models are capable of producing text equally persuasive to human-generated covert propaganda campaigns <span id="id2">[<a class="reference internal" href="#id2829">1</a>]</span>. Consequently it is vital that AI-generated texts and images be identifiable in both online and offline forums. While generated text may not be intentionally biased or aimed to be propaganda, it may still hold implicit biases, which could be influential all the same if not prefaced properly. As such, any AI which is designed to generate content that directly interacts with the end-user should clearly identify the content which is AI generated. Furthermore, providers of online and offline forums, which may contain AI-generated materials should set in-place validated mechanisms for evaluating published content for AI-generated materials and mark the content appropriately.</p>
</div>
<div class="section" id="recommender-systems">
<h7>Recommender Systems<a class="headerlink" href="#recommender-systems" title="Permalink to this headline">¶</a></h7>
<p>Recommender Systems are AI algorithms which aim to show users content that is consistent with their previous interactions. Such algorithms are commonplace on platforms such as Netflix and Instagram, where recommended content is based on the user’s interaction with previous posts. These algorithms can become instrumental for political propaganda in two ways. The first is through amplification. Here the algorithm reinforces extremist political tendencies by presenting content consistent with these ideals, leading to a metaphorical echo chamber. As a consequence, users’ views are reinforced, leading to increasing extremism and a lack of confrontation with opposing views. Alternatively, recommender systems may cause persuasion through repeatedly exposing the user to certain content. During this process, neutral users are increasingly recommended extremist content, despite no previous interaction with this material. As a result users may become persuaded by the content, leading to involuntary opinion change. The presentation of extremist content through recommender systems may result from system bias or purposeful manipulation. Research has found that some recommender systems appear to be more prone to these biases than others, indicating that training data and user base may impact the algorithm’s function <span id="id3">[<a class="reference internal" href="#id2834">2</a>]</span>.</p>
</div>
<div class="section" id="guidelines">
<h7>Guidelines<a class="headerlink" href="#guidelines" title="Permalink to this headline">¶</a></h7>
<p>The current AI act guidelines do not explicitly stipulate regulations relating to the AI-based generation of propaganda. However, we argue that this should be placed in the unacceptable risk level bracket as it may exploit vulnerable groups through targeted advertisement and automatically generated content. As such, the automatic generation and circulation of propaganda should be outlawed under EU legislation. However, the enforcement of such rules are questionable due to the intricacies of propaganda. As such, more research in the identification and reporting of AI generated and circulated propaganda is required.</p>
</div>
</div>
<div class="section" id="bibliography">
<h6>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h6>
<p id="id4"><dl class="citation">
<dt class="label" id="id2829"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>)</span></dt>
<dd><p>Josh A Goldstein, Jason Chao, Shelby Grossman, Alex Stamos, and Michael Tomz. How persuasive is ai-generated propaganda? <em>PNAS nexus</em>, 3(2):pgae034, 2024.</p>
</dd>
<dt class="label" id="id2834"><span class="brackets"><a class="fn-backref" href="#id3">2</a></span></dt>
<dd><p>Joe Whittaker, Seán Looney, Alastair Reed, Fabio Votta, and others. Recommender systems and the amplification of extremist content. <em>Internet Policy Review</em>, 2021.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was written by Nicola Rossberg and Andrea Visentin.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<span id="document-TAILOR_project"></span><div class="tex2jax_ignore mathjax_ignore section" id="the-tailor-project">
<h2>The TAILOR project<a class="headerlink" href="#the-tailor-project" title="Permalink to this headline">¶</a></h2>
<div class="section" id="in-brief">
<h3>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h3>
<p><a href="https://tailor-network.eu" terget=_blank>TAILOR</a> is an EU project with the aim build the capacity to provide the scientific foundations for Trustworthy AI in Europe. TAILOR develops a network of research excellence centers, leveraging and combining learning, optimization, and reasoning (LOR) with the key concepts of Trustworthy AI (TAI).</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/VW5RsxxLagc?si=i8tLlr2fMGp6f5Wt" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>
<div class="section" id="more-in-detail">
<h3>More in detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">¶</a></h3>
<p>The purpose of the EU Project TAILOR is to build the capacity to provide the scientific foundations for Trustworthy AI in Europe by developing a network of research excellence centers leveraging and combining learning, optimization, and reasoning. These systems are meant to provide descriptive, predictive, and prescriptive systems integrating data-driven and knowledge-based approaches.
Artificial Intelligence (AI) has grown unprecedentedly in the last ten years. It has been applied to many industrial and service sectors, becoming ubiquitous in our everyday life. More and more often, AI systems are used to suggest decisions to human experts, to propose actions, and to provide predictions. Because these systems might influence our lives and significantly impact how we decide, they need to be trustworthy. How can a radiologist trust an AI system analyzing medical images? How can a financial broker trust an AI system providing stock price predictions? How can a passenger trust a self-driving car?</p>
<p>These are fundamental questions that require deep analysis and fundamental research activity, as well as a new generation of AI talents who are skilled in the scientific foundations of Trustworthy AI, who know how to assess, and how to design, trustworthy AI systems. Some of the current issues related to lack of trust in AI systems are a direct consequence of the massive use of black-box methods relying only on data. One needs to define the foundations of a new generation of AI systems not only relying on data-driven approaches but also on the whole set of AI techniques, including symbolic AI methods, optimization, reasoning, and planning.</p>
<div class="figure align-center" id="fig-tailor-goals">
<a class="reference internal image-reference" href="_images/TAILOR_goals.png"><img alt="_images/TAILOR_goals.png" src="_images/TAILOR_goals.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 29 </span><span class="caption-text">The main objectives of the TAILOR project</span><a class="headerlink" href="#fig-tailor-goals" title="Permalink to this image">¶</a></p>
</div>
<p>The project is articulated into five research areas:</p>
<ul class="simple">
<li><p><a class="reference internal" href="TAILOR.html#document-TAILOR"><span class="doc std std-doc">Trustworthy AI</span></a> to develop the foundation of Trustworthy AI.</p></li>
<li><p>Paradigms and representations to combine and integrate learning, reasoning, and optimization.</p></li>
<li><p>Acting, learning, and reasoning to plan, act, and monitor behavior.</p></li>
<li><p>Social AI, learning, and reasoning for multi-agent interactions and human AI collaboration.</p></li>
<li><p>Auto AI to automate the development and deployment of AI systems, and democratize the access to state-of-the-art technologies.</p></li>
</ul>
<p>More information about the project can be found at the <a href="https://tailor-network.eu" terget=_blank>TAILOR website</a>.</p>
<blockquote>
<div><p>This entry was re-adapted from the TAILOR project website by Francesca Pratesi and Umberto Straccia.</p>
</div></blockquote>
<!--
<a href="https://tailor-network.eu" terget=_blank>TAILOR</a> is an EU-funded ICT-48 Network (GA 952215) with the purpose of building the capacity of providing the scientific foundations for Trustworthy AI in Europe by developing a network of research excellence centres leveraging and combining learning, optimization and reasoning.

- TAILOR is a network of research excellence centres across all of Europe on the Foundations of Trustworthy AI based on four powerful instruments (a strategic roadmap committee, basic research program to address grand challenges, a connectivity fund for active dissemination to the larger AI community, and network collaboration activities promoting research exchanges, training materials and events, and joint PhD supervision.
- TAILOR is developing an ambitious research and innovation roadmap for the foundation of Trustworthy AI leveraging Europe’s strengths and opportunities, across multiple disciplines, maturity levels, and geographical location. Seeds for its implementation are proposed: challenges regarding both the basic research themes and application use-cases; a PhD program favouring immersion of PhDs in industry.
- TAILOR launched and executes five basic research programs validating the operation of the network and performing ground-breaking basic research integrating learning, optimisation and reasoning in key areas for providing the scientific foundations for Trustworthy AI.
- TAILOR is developing and building on new mechanisms to step up AI outreach, harmonize training curricula, and significantly strengthen European capacities in AI research on Trustworthy AI.
- TAILOR brings together leading AI research centres from learning, optimisation and reasoning together with major European companies representing important industry sectors into a single scientific network to reduce the fragmentation, boost the collaboration, and increase the AI research capacity of Europe as well as attracting and retaining talents in Europe.

TAILOR, like all the research projects, is based on Work Pakages (WPs).
WP3 (Trustworthy AI) aims at advancing knowledge on the six dimensions and putting each of them in relationships with foundation themes.
-->
</div>
</div>
<span id="document-authors"></span><div class="tex2jax_ignore mathjax_ignore section" id="complete-list-of-contributors">
<h2>Complete List of Contributors<a class="headerlink" href="#complete-list-of-contributors" title="Permalink to this headline">¶</a></h2>
<p>Coordinators:</p>
<ul class="simple">
<li><p>Umberto Straccia - Institute of Information Science and Technologies “A. Faedo” of the National Research Council of Italy (ISTI-CNR), Via G. Moruzzi, 1, 56124 Pisa, Italy</p></li>
<li><p>Francesca Pratesi - Institute of Information Science and Technologies “A. Faedo” of the National Research Council of Italy (ISTI-CNR), Via G. Moruzzi, 1, 56124 Pisa, Italy</p></li>
</ul>
<blockquote>
<div><p>The complete list of authors is (in alphabetical order):</p>
<ul class="simple">
<li><p>Riccardo Albertoni - Istituto di Matematica Applicata e Tecnologie Informatiche “Enrico Magenes”, Consiglio Nazionale delle Ricerche (IMATI-CNR), Via De Marini, 6, 16149 Genova, Italy <!-- T3.4 --></p></li>
<li><p>Tristan Allard - University of Rennes, CNRS, IRISA, 35000 Rennes, France <!-- T3.5 --></p></li>
<li><p>Guilherme Alves, University of Lorraine, CNRS, Inria, LORIA, 54000 Nancy, France <!-- T3.3 --></p></li>
<li><p>George Ashwin - Faculty of Electrical Engineering Mathematics and Computer Science, Delft University of Technology, Delft, The Netherlands <!-- T3.4 --></p></li>
<li><p>Alejandra Bringas Colmenarejo, School of Law, University of Southampton, SO17 1BJ, United Kingdom   <!-- T3.3 --></p></li>
<li><p>Stefan Buijsman - Delft University of Technology, Jaffalaan 5, 2628 BX, Delft, The Netherlands <!-- T3.3 --></p></li>
<li><p>Pablo A M Casares - VRAIN, Universitat Politècnica de València <!-- T3.2 --></p></li>
<li><p>Sara Colantonio - Institute of Information Science and Technologies “A. Faedo” of the National Research Council of Italy (ISTI-CNR), Via G. Moruzzi, 1, 56124 Pisa, Italy <!-- T3.4 --></p></li>
<li><p>Miguel Couceiro - Université de Lorraine, CNRS, Inria, LORIA, 54000 Nancy, France <!-- T3.3 --></p></li>
<li><p>Santiago Escobar - VRAIN, Universitat Politècnica de València <!-- T3.2 --></p></li>
<li><p>Alessandro Fabris, MPI-SP, Max Planck Institute for Security and Privacy, 44799 Bochum, Germany <!-- T3.3 --></p></li>
<li><p>Peter Flach, University of Bristol, United Kingdom <!-- T3.2 --></p></li>
<li><p>Gabriel Gonzalez-Castañé - University College Cork, Cork, Ireland <!-- T3.3 --></p></li>
<li><p>Riccardo Guidotti - University of Pisa, Department of Computer Sciences, Largo B. Pontecorvo, 3, 56127 Pisa, Italy <!-- T3.1 --></p></li>
<li><p>Fredrik Heintz - Linköping University, Department of Computer and Information Sciences, 58 183 Linköping, Sweden  <!-- T3.3 --></p></li>
<li><p>Jose Hernandez-Orallo - VRAIN, Universitat Politècnica de València <!-- T3.2 --></p></li>
<li><p>Sietze Kuilman - Faculty of Electrical Engineering Mathematics and Computer Science, Delft University of Technology, Delft, The Netherlands <!-- T3.4 --></p></li>
<li><p>Karima Makhlouf, Inria, Ecole Polytechnique, IPP, 91120, Paris, France <!-- T3.3 --></p></li>
<li><p>Marta Marchiori Manerba - University of Pisa, Department of Computer Sciences, Largo B. Pontecorvo, 3, 56127 Pisa, Italy <!-- T3.3 --></p></li>
<li><p>Fernando Martinez-Plumed - VRAIN, Universitat Politècnica de València <!-- T3.2 --></p></li>
<li><p>Anna Monreale - University of Pisa, Department of Computer Sciences, Largo B. Pontecorvo, 3, 56127 Pisa, Italy <!-- T3.5 --></p></li>
<li><p>Roberto Pellungrini - University of Pisa, Department of Computer Sciences, Largo B. Pontecorvo, 3, 56127 Pisa, Italy <!-- T3.5 --></p></li>
<li><p>Miquel Perello Nieto, University of Bristol, United Kingdom <!-- T3.2 --></p></li>
<li><p>Francesca Pratesi - Institute of Information Science and Technologies “A. Faedo” of the National Research Council of Italy (ISTI-CNR), Via G. Moruzzi, 1, 56124 Pisa, Italy <!-- T3.1, T3.4, T3.5 --></p></li>
<li><p>Resmi Ramachandran Pillai - Linköping University, Department of Computer and Information Sciences, 58 183 Linköping, Sweden  <!-- T3.3 --></p></li>
<li><p>Nicola Rossberg - School of Computer Science &amp; IT, University College Cork, Cork, Ireland <!-- T3.6 --></p></li>
<li><p>Andrea Rossi - SFI Centre for Research Training in Artificial Intelligence, University College Cork <!-- T3.6 --></p></li>
<li><p>Marie-Christine Rousset - University of Grenoble Alpes, Grenoble, France <!-- T3.5 --></p></li>
<li><p>Salvatore Ruggieri - University of Pisa, Department of Computer Sciences, Largo B. Pontecorvo, 3, 56127 Pisa, Italy  <!-- T3.3 --></p></li>
<li><p>Luciano C Siebert - Faculty of Electrical Engineering Mathematics and Computer Science, Delft University of Technology, Delft, The Netherlands <!-- T3.4 --></p></li>
<li><p>Piotr Skrzypczyński - Institute of Robotics and Machine Intelligence, Poznań University of Technology, ul. Piotrowo 3A, 60-965 Poznań, Poland <!-- T3.4 --></p></li>
<li><p>Kacper Sokol, ETH Zurich, Switzerland <!-- T3.1 --></p></li>
<li><p>Jerzy Stefanowski - Institute of Computing Science, Poznań University of Technology, ul. Piotrowo 2, 60-965 Poznań, Poland <!-- T3.4 --></p></li>
<li><p>Barry O’Sullivan - School of Computer Science &amp; IT, University College Cork, Cork, Ireland <!-- T3.6 --></p></li>
<li><p>Andrea Visentin - School of Computer Science &amp; IT, University College Cork, Cork, Ireland <!-- T3.6 --></p></li>
<li><p>Arkady Zgonnikov - Faculty of Mechanical, Maritime and Materials Engineering, Delft University of Technology, Delft, The Netherlands <!-- T3.4 --></p></li>
<li><p>Sami Zhioua, Inria, Ecole Polytechnique, IPP, 91120, Paris, France <!-- T3.3 --></p></li>
</ul>
</div></blockquote>
</div>
<span id="document-main/AnalyticalIndex"></span><div class="tex2jax_ignore mathjax_ignore section" id="index">
<h2>Index<a class="headerlink" href="#index" title="Permalink to this headline">¶</a></h2>
<p>Here, you can find the list of entries in alphabetical order.</p>
<ul class="simple">
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/2CC2"><span class="doc">2CC2</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Accountability/Accountability"><span class="doc">Accountability</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/propaganda"><span class="doc">AI for propaganda</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/social_scoring"><span class="doc">AI for social scoring</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/human_interaction"><span class="doc">AI human interaction</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/workforce_impact"><span class="doc">AI Impact on the Workforce</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Alignment"><span class="doc">Alignment</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Adversarial attack"><span class="doc">Adversarial Attack</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Adversarial example"><span class="doc">Adversarial Example</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Adversarial input"><span class="doc">Adversarial Input</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Indistinguishability"><span class="doc">Anonymity by Indistinguishability</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Ante-hoc Explanation"><span class="doc">Ante-hoc Explanation</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Assessment"><span class="doc">Assessment</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Attacks Anonym"><span class="doc">Attacks on Anonymization Schema</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Attacks on Pseudonymised Data"><span class="doc">Attacks on Pseudonymised Data</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/Auditing"><span class="doc">Auditing</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/Bias"><span class="doc">Bias</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/bias_factors"><span class="doc">Bias Conducive Factors</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/bias_lmm"><span class="doc">Bias and Fairness in LLMs</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Black-box Explanations"><span class="doc">Black-box Explanation</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Brittleness"><span class="doc">Brittleness</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/C2C"><span class="doc">C2C</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Human_Agency_and_Oversight/Causal_responsibility"><span class="doc">Causal Responsibility</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Cloud Computing"><span class="doc">Cloud Computing</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Accountability/Continuous monitoring"><span class="doc">Continuous Performance Monitoring</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Counterexemplar"><span class="doc">Counterexemplars</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Counterfactual"><span class="doc">Counterfactuals</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/cradle 2 cradle"><span class="doc">Cradle 2 cradle</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Cradle"><span class="doc">Cradle-to-cradle Design</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Data Anonymization"><span class="doc">Data Anonymization</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Data Center"><span class="doc">Data Center</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Data Poisoning"><span class="doc">Data Poisoning</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Dependability"><span class="doc">Dependability</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Differential Privacy models"><span class="doc">Differential Privacy Models</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/epsilon_delta-differential_privacy"><span class="doc std std-doc">Differential Privacy - (<span class="math notranslate nohighlight">\(\epsilon\)</span>,<span class="math notranslate nohighlight">\(\delta\)</span>)-Differential Privacy</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Epsilon-differential_privacy"><span class="doc std std-doc">Differential Privacy - <span class="math notranslate nohighlight">\(\epsilon\)</span>-Differential Privacy</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Epsilon-indist"><span class="doc std std-doc">Differential Privacy - <span class="math notranslate nohighlight">\(\epsilon\)</span>-Indistinguishability</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Data Shift"><span class="doc">Distributional Shift</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Dimensions of Explanations"><span class="doc">Dimensions of Explanations</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/Discrimination"><span class="doc">Grounds of Discrimination</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Distributional Shift"><span class="doc">Distributional Shift</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Direct"><span class="doc">Direct Behaviour</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Edge Computing"><span class="doc">Edge Computing</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/emotional_impact"><span class="doc">Emotional Impact</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Energy Aware"><span class="doc">Energy-aware Computing</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Energy Efficient"><span class="doc">Energy-efficient Computing</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Evaluation"><span class="doc">Evaluation</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/Equity"><span class="doc">Equity</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Exemplars"><span class="doc">Exemplars</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Explainable AI"><span class="doc">Explainable AI</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Explanation by Design"><span class="doc">Explanation by Design</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/Fair Machine Learning"><span class="doc">Fair Machine Learning</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/Fairness"><span class="doc">Fairness</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Features Importance"><span class="doc">Feature Importance</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Federated"><span class="doc">Federated Learning</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Accountability/Frame"><span class="doc">The Frame Problem</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Fog Computing"><span class="doc">Fog Computing</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Generalizable XAI"><span class="doc">Model Agnostic</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Global Explanations"><span class="doc">Global Explanations</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Green AI"><span class="doc">Green AI</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Green Computing"><span class="doc">Green Computing</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Green IT"><span class="doc">Green IT</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/ICT sustainability"><span class="doc">ICT sustainability</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Intended"><span class="doc">Intended Behaviour</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/intersectionality"><span class="doc">Intersectionality</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/Justice"><span class="doc">Justice</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/K-Anonymity"><span class="doc">K-anonymity</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/L_diversity"><span class="doc">l-diversity</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Linking Attack"><span class="doc">Linking Attack</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Lore"><span class="doc">Local Rule-based Explanation</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Local Explanations"><span class="doc">Local Explanations</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Human_Agency_and_Oversight/Meaningful_human_control"><span class="doc">Meaningful Human Control</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Measurement"><span class="doc">Measurement</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Misdirect"><span class="doc">Misdirect Behaviour</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Model-Agnostic"><span class="doc">Model Agnostic</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Model-Specific"><span class="doc">Model Specific</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Not Generalizable XAI"><span class="doc">Model Specific</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Negative Side Effects"><span class="doc">Negative Side Effects</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Perturbation"><span class="doc">Achiving Differential Privacy</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Post-hoc Explanations"><span class="doc">Post-hoc Explanation</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Privacy model"><span class="doc">Privacy models</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Accountability/Problem_of_many_hands"><span class="doc">Problem of Many Hands</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Prototypes"><span class="doc">Prototypes</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Accountability/Provenance"><span class="doc">Provenance Tracking</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Pseudonymised Data"><span class="doc">Pseudonymization</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Randomization"><span class="doc">Randomization Methods</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/Re-identification Attack"><span class="doc">Re-identification Attack</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Regenerative Design"><span class="doc">Regenerative Design</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Reliability"><span class="doc">Reliability</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Accountability/Repeatability"><span class="doc">Repeatability</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Accountability/Replicability"><span class="doc">Replicability</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Accountability/Reproducibility"><span class="doc">Reproducibility</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Resource Allocation"><span class="doc">Resource Allocation</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Resource Prediction"><span class="doc">Resource Prediction</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Resource Scheduling"><span class="doc">Resource Scheduling</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Robustness"><span class="doc">Robustness</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Rules"><span class="doc">Rules List and Rules Set</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Saliency Maps"><span class="doc">Saliency Maps</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Security"><span class="doc">Security</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Diversity_Non-Discrimination_and_Fairness/Segregation"><span class="doc">Segregation</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/self-identification"><span class="doc">Self-identification of AI</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Single Tree Approximation"><span class="doc">Single Tree Approxiamation</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/social_impact"><span class="doc">Social Impact of AI Systems</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/society_and_democracy"><span class="doc">Society and Democracy</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Privacy_and_Data_Governance/T_closeness"><span class="doc">t-closeness</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Testing"><span class="doc">Testing</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Accountability/Traceability"><span class="doc">Traceability</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/Transparency"><span class="doc">Transparency</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Technical_Robustness_and_Safety/Unintended"><span class="doc">Unintended Behaviour</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Transparency/XAI"><span class="doc">XAI</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Accountability/Wicked"><span class="doc">Wicked Problems</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Workload Forecast"><span class="doc">Workload Forecast</span></a></p></li>
<li><p><a class="reference internal" href="TAILOR.html#document-index/Societal_and_Environmental_Wellbeing/Workload Prediction"><span class="doc">Workload Prediction</span></a></p></li>
</ul>
<div class="toctree-wrapper compound">
<span id="document-index/Societal_and_Environmental_Wellbeing/2CC2"></span><div class="tex2jax_ignore mathjax_ignore section" id="cc2">
<h3>2CC2<a class="headerlink" href="#cc2" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms:</em> Crade-to-cradle design, C2C, cradle 2 cradle, regenerative design.</p>
<p><strong>2CC2</strong> is a biomimetic approach to product and system design that mimics natural processes, in which materials are considered as nutrients flowing in healthy, safe metabolisms.</p>
<p>You can find futher information about 2CC2 <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/cradle_to_cradle"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Accountability/Accountability"></span><div class="tex2jax_ignore mathjax_ignore section" id="accountability">
<h3>Accountability<a class="headerlink" href="#accountability" title="Permalink to this headline">¶</a></h3>
<p><strong>Accountability</strong> is an ethical aspect studied in the <a href="https://tailor-network.eu/" target=_blank>TAILOR project</a> to ensure that a given actor or actors can render an account of the actions of an AI system. The accountability concept is strictly related to the concept of responsibility.</p>
<p>You can find futher information about Accountability term <a class="reference internal" href="TAILOR.html#document-Accountability/L2.Accountability"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/propaganda"></span><div class="tex2jax_ignore mathjax_ignore section" id="ai-for-propaganda">
<h3>AI for propaganda<a class="headerlink" href="#ai-for-propaganda" title="Permalink to this headline">¶</a></h3>
<p>With the onset of large language models, the mass production of text has become considerably cheaper and faster, requiring less human involvement. With the addition of automatic image generators such as Dall-E, the same is now possible for pictures. Previous work has found that GPT-3, the antecedent to ChatGPT is capable of creating text equally persuasive as content from existing covert propaganda. This causes a rising concern regarding the ease at which a large number of influential texts can be circulated using online platforms. To combat mass-propaganda, it is important that online and offline platforms have effective controls to identify AI-generated materials in their publications.</p>
<p>You can find futher information about AI for propaganda <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/propaganda"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/social_scoring"></span><div class="tex2jax_ignore mathjax_ignore section" id="ai-for-social-scoring">
<h3>AI for social scoring<a class="headerlink" href="#ai-for-social-scoring" title="Permalink to this headline">¶</a></h3>
<p>Social Scoring is the concept that all daily actions taken by individuals are monitored and scored for their benefit to society as a whole. Based on this scoring system, you are assigned a given value which determines your access to education, healthcare and other public goods. AI lends itself to social scoring as it excels at facial recognition and movement identification. However this poses several technical and ethical problems. The obvious ethical issue is that the goodness of actions is subjective and the ability of individuals to access goods and services necessary for survival should not depend on their perceived ability to contribute to society. Furthermore, the technical issue with social scoring is that the quality of AI evaluation of movement hinges on the quality of its training data, with biased training data leading to biased evaluation by the AI. Social scoring has currently only been proposed by the Chinese government but other countries including the United Kingdom are using automatic facial recognition in surveillance tapes to search for fugitives.</p>
<p>You can find futher information about AI for Social Scoring <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/social_scoring"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/human_interaction"></span><div class="tex2jax_ignore mathjax_ignore section" id="ai-human-interaction">
<h3>AI human interaction<a class="headerlink" href="#ai-human-interaction" title="Permalink to this headline">¶</a></h3>
<p>As AI develops to learn and adapt, it is increasingly being perceived as human-like in both its appearance and intelligence. This ability to imitate human behavior and interactions creates anthropomorphic cues which cause users to form emotional bonds. As AI becomes more commonplace in our daily lives, being integrated into virtual assistants, recommender systems etc. it is important to assess how the system’s anthropomorphic cues affect our interaction with it. In addition, potential risks resulting from AI-human interaction should be assessed prior to any implementation of AI as previous research has demonstrated the unpredictability of AI.</p>
<p>You can find futher information about AI human interaction <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/human_interaction"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/workforce_impact"></span><div class="tex2jax_ignore mathjax_ignore section" id="ai-impact-on-the-workforce">
<h3>AI Impact on the Workforce<a class="headerlink" href="#ai-impact-on-the-workforce" title="Permalink to this headline">¶</a></h3>
<p>The impact of Artificial Intelligence on the workforce can be considered from several perspectives. The most common consideration is the loss of jobs or possibility of deskilling of the workforce due to the introduction of AI systems. A second consideration is the increased demand for workers with a computer-science skill set in order to operate and maintain implemented AI systems. A final consideration is the effect of AI-triggered changes for stakeholders. Prior to the introduction of an AI system, risks and consequences in these areas should be assessed and steps to counter negative outcomes should be considered.</p>
<p>You can find futher information about AI Impact on the Workforce <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/workforce_impact"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Alignment"></span><div class="tex2jax_ignore mathjax_ignore section" id="alignment">
<h3>Alignment<a class="headerlink" href="#alignment" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: (Mis)directed behaviour, (Un)intended behaviour</p>
<p>The goal of AI <strong>alignment</strong> is to ensure that AI systems are aligned with human intentions and values. This first requires determining the normative question of what values or principles we have and what humans really want, collectively or individually, and second, the technical question of how to imbue AI systems with these values and goals.</p>
<p>You can find futher information about Alignment <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/alignment"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Adversarial attack"></span><div class="tex2jax_ignore mathjax_ignore section" id="adversarial-attack">
<h3>Adversarial Attack<a class="headerlink" href="#adversarial-attack" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Adversarial Input, Adversarial Example.</p>
<p>An <strong>adversarial attack</strong> is any perturbation of the input features or observations of a system (sometimes imperceptible to both humans and the own system) that makes the system fail or take the system to a dangerous state. A prototypical case of an adversarial situation happens with machine learning models, when an external agent maliciously modify input data –often in imperceptible ways– to induce them into misclassification or incorrect prediction. For instance, by undetectably altering a few pixels on a picture, an adversarial attacker can mislead a model into generating an incorrect output (like identifying a panda as a gibbon or a ‘stop’ sign as a ‘speed limit’ sign) with an extremely high confidence. While a good amount of attention has been paid to the risks that adversarial attacks pose in deep learning applications like computer vision, these kinds of perturbations are also effective across a vast range of machine learning techniques and uses such as spam filtering and malware detection. A different but related type of adversarial attack is called Data Poisoning, but this involves a malicious compromise of data sources (used for training or testing) at the point of collection and pre-processing.</p>
<p>You can find futher information about Adversarial Attack <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/adversarial_attack"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Adversarial example"></span><div class="tex2jax_ignore mathjax_ignore section" id="adversarial-example">
<h3>Adversarial Example<a class="headerlink" href="#adversarial-example" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Adversarial Input, Adversarial Attack.</p>
<p>An <strong>adversarial attack</strong> is any perturbation of the input features or observations of a system (sometimes imperceptible to both humans and the own system) that makes the system fail or take the system to a dangerous state. A prototypical case of an adversarial situation happens with machine learning models, when an external agent maliciously modify input data –often in imperceptible ways– to induce them into misclassification or incorrect prediction. For instance, by undetectably altering a few pixels on a picture, an adversarial attacker can mislead a model into generating an incorrect output (like identifying a panda as a gibbon or a ‘stop’ sign as a ‘speed limit’ sign) with an extremely high confidence. While a good amount of attention has been paid to the risks that adversarial attacks pose in deep learning applications like computer vision, these kinds of perturbations are also effective across a vast range of machine learning techniques and uses such as spam filtering and malware detection. A different but related type of adversarial attack is called Data Poisoning, but this involves a malicious compromise of data sources (used for training or testing) at the point of collection and pre-processing.</p>
<p>You can find futher information about Adversarial Example <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/adversarial_attack"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Adversarial input"></span><div class="tex2jax_ignore mathjax_ignore section" id="adversarial-input">
<h3>Adversarial Input<a class="headerlink" href="#adversarial-input" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Adversarial Attack, Adversarial Example.</p>
<p>An <strong>adversarial input</strong> is any perturbation of the input features or observations of a system (sometimes imperceptible to both humans and the own system) that makes the system fail or take the system to a dangerous state. A prototypical case of an adversarial situation happens with machine learning models, when an external agent maliciously modify input data –often in imperceptible ways– to induce them into misclassification or incorrect prediction. For instance, by undetectably altering a few pixels on a picture, an adversarial attacker can mislead a model into generating an incorrect output (like identifying a panda as a gibbon or a ‘stop’ sign as a ‘speed limit’ sign) with an extremely high confidence. While a good amount of attention has been paid to the risks that adversarial attacks pose in deep learning applications like computer vision, these kinds of perturbations are also effective across a vast range of machine learning techniques and uses such as spam filtering and malware detection. A different but related type of adversarial attack is called Data Poisoning, but this involves a malicious compromise of data sources (used for training or testing) at the point of collection and pre-processing.</p>
<p>You can find futher information about Adversarial Input <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/adversarial_attack"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/Indistinguishability"></span><div class="tex2jax_ignore mathjax_ignore section" id="anonymity-by-indistinguishability">
<h3>Anonymity by Indistinguishability<a class="headerlink" href="#anonymity-by-indistinguishability" title="Permalink to this headline">¶</a></h3>
<p>The family of <strong>anonymity by indistinguishability</strong> models is based on comparison among individuals present in data, and it aims to make each individual so similar as to be indistinguishable from others. They aims to produce anonymity sets, i.e., equivalence classes, having specific properties.</p>
<p>You can find futher information about Anonymity by Indistinguishability <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.indistinguishability"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Ante-hoc Explanation"></span><div class="tex2jax_ignore mathjax_ignore section" id="ante-hoc-explanation">
<h3>Ante-hoc Explanation<a class="headerlink" href="#ante-hoc-explanation" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Explanation by Design, Transparent model.</p>
<p><strong>Ante-hoc explanation</strong> means to rely, by design, on a transparent model, instead of providing explanations of an AI model.</p>
<p>You can find futher information about Ante-hoc Explanation <a class="reference internal" href="TAILOR.html#document-Transparency/blackbox_transparent"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Assessment"></span><div class="tex2jax_ignore mathjax_ignore section" id="assessment">
<h3>Assessment<a class="headerlink" href="#assessment" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Evaluation, Testing, Measurement.</p>
<p><strong>AI assessment</strong> is any activity that estimates attributes as measures— of an AI system or some of its components, abstractly or in particular contexts of operation. These attributes, if well estimated, can be used to explain and predict the behaviour of the system. This can stem from an engineering perspective, trying to understand whether a particular AI system meets the specifications or the intention of their designers, known respectively as <strong>verification</strong> and <strong>validation</strong>. Under this perspective, AI measurement is close to computer systems <strong>testing</strong> (hardware and/or software) and other evaluation procedures in engineering. However, in AI there is an extremely complex adaptive behaviour, and in many cases, with a lack of a written and operational specification. What the systems has to do depends on some constraints and utility functions that have to be optimised, is specified by example (from which the system has to learn a model) or ultimately depends on feedback from the user or the environment (e.g., in the form of rewards).</p>
<p>You can find futher information about Assessment <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/evaluation"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/Attacks Anonym"></span><div class="tex2jax_ignore mathjax_ignore section" id="attacks-on-anonymization-schema">
<h3>Attacks on Anonymization Schema<a class="headerlink" href="#attacks-on-anonymization-schema" title="Permalink to this headline">¶</a></h3>
<p>There are a variety of attacks that involve data privacy. Some of them are very context-specific (for example, there exists attacks on partition-based algorithms, such as deFinetti Attack or Minimality Attack), while other are more general. For example, we can have the <em>Re-identification Attack</em>, where the aim is to link a identity in the data with a real identity; <em>Membership Attack</em>, where the goal is to understand whether or not a particular individual is present in the considered dataset; <em>Reconstruction Attack</em>, where one wants to reconstruct (even partially) a private dataset from public aggregate information.</p>
<p>You can find futher information about <span class="math notranslate nohighlight">\(\epsilon\)</span>-Differential Privacy <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L1.attacks"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/Attacks on Pseudonymised Data"></span><div class="tex2jax_ignore mathjax_ignore section" id="attacks-on-pseudonymised-data">
<h3>Attacks on Pseudonymised Data<a class="headerlink" href="#attacks-on-pseudonymised-data" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Re-identification Attack, Linking Attack.</p>
<p>A <strong>Attack on Pseudonymised Data</strong> aims to link a certain set of data related to an individual in a dataset (which does not contain direct identifiers) to a real identity, relying on additional information.</p>
<p>You can find futher information about Attacks on Pseudonymised Data <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.reidentification"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Diversity_Non-Discrimination_and_Fairness/Auditing"></span><div class="tex2jax_ignore mathjax_ignore section" id="auditing">
<h3>Auditing<a class="headerlink" href="#auditing" title="Permalink to this headline">¶</a></h3>
<p><strong>Auditing AI</strong> aims to identify and address possible risks and impacts while ensuring robust and trustworthy accountability.</p>
<p>You can find futher information about Auditing <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/auditing"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Diversity_Non-Discrimination_and_Fairness/Bias"></span><div class="tex2jax_ignore mathjax_ignore section" id="bias">
<h3>Bias<a class="headerlink" href="#bias" title="Permalink to this headline">¶</a></h3>
<p><strong>Bias</strong> refers to an inclination towards or against a particular individual, group, or sub-groups. AI models may inherit biases from training data or introduce new forms of bias.</p>
<p>You can find futher information about Bias <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/bias"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Diversity_Non-Discrimination_and_Fairness/bias_factors"></span><div class="tex2jax_ignore mathjax_ignore section" id="bias-conducive-factors">
<h3>Bias Conducive Factors<a class="headerlink" href="#bias-conducive-factors" title="Permalink to this headline">¶</a></h3>
<p><strong>Bias conducive factors</strong> are aspects of individuals and institutions that lead to biases in data-driven models by influencing data and tech development. This entry presents a selection of bias conducive factors in algorithmic hiring.</p>
<p>You can find futher information about Discrimination <span class="xref myst">here</span></p>
</div>
<span id="document-index/Diversity_Non-Discrimination_and_Fairness/bias_lmm"></span><div class="tex2jax_ignore mathjax_ignore section" id="bias-and-fairness-in-llms">
<h3>Bias and Fairness in LLMs<a class="headerlink" href="#bias-and-fairness-in-llms" title="Permalink to this headline">¶</a></h3>
<p>Within the Natural Language Processing (NLP) field, bias manifests in several forms, possibly leading to harms and unfairness. We review here intrinsic, lantent bias; extrinsic harms, and data selection bias in Large Language Models (LLMs).</p>
<p>You can find futher information about Discrimination <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/bias_lmm"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Black-box Explanations"></span><div class="tex2jax_ignore mathjax_ignore section" id="black-box-explanation">
<h3>Black-box Explanation<a class="headerlink" href="#black-box-explanation" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Post-hoc Explanations.</p>
<p>With a <strong>black-box explanation</strong> we pair the black box model with an interpretation the black box decisions or model, instead of relying, by design, on a transparent model.</p>
<p>You can find futher information about Black-box Explanation <a class="reference internal" href="TAILOR.html#document-Transparency/blackbox_transparent"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Brittleness"></span><div class="tex2jax_ignore mathjax_ignore section" id="brittleness">
<h3>Brittleness<a class="headerlink" href="#brittleness" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Robustness.</p>
<p><strong>Brittleness</strong> is the degree in which an AI system functions reliably and accurately under harsh conditions. These conditions may include adversarial intervention, implementer error, or skewed goal-execution by an automated learner (in reinforcement learning applications). The measure of robustness is therefore the strength of a system’s integrity and the soundness of its operation in response to difficult conditions, adversarial attacks, perturbations, data poisoning, and undesirable reinforcement learning behaviour.</p>
<p>You can find futher information about Brittleness <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/robustness"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/C2C"></span><div class="tex2jax_ignore mathjax_ignore section" id="c2c">
<h3>C2C<a class="headerlink" href="#c2c" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms</em>: 2CC2, Cradle-to-cradle Design, cradle 2 cradle, regenerative design.</p>
<p><strong>C2C</strong>  is a biomimetic approach to product and system design that mimics natural processes, in which materials are considered as nutrients flowing in healthy, safe metabolisms.</p>
<p>You can find futher information about C2C <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/cradle_to_cradle"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Human_Agency_and_Oversight/Causal_responsibility"></span><div class="tex2jax_ignore mathjax_ignore section" id="causal-responsibility">
<h3>Causal Responsibility<a class="headerlink" href="#causal-responsibility" title="Permalink to this headline">¶</a></h3>
<p>Causal responsibility is the notion of responsibility that is concerned with actual causation.</p>
<p>You can find futher information about Causal Responsibility <a class="reference internal" href="TAILOR.html#document-Human_Agency_and_Oversight/Causal_responsibility"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Cloud Computing"></span><div class="tex2jax_ignore mathjax_ignore section" id="cloud-computing">
<h3>Cloud Computing<a class="headerlink" href="#cloud-computing" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms:</em> Mesh Computing</p>
<p><strong>Cloud Computing</strong> is the provision of computing resources (storage and processing power) on demand, without direct user intervention. A large cloud often includes multiple data centres, each housing a different set of functions. The cloud computing model aims to achieve core economies of scale through sharing of resources, taking advantage of a “pay-as-you-go” model that can decrease capital expenditures, but can also result in unforeseen operating expenses for users who are unaware of the concept.</p>
<p>You can find futher information about Cloud Computing <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/cloud_computing"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Accountability/Continuous monitoring"></span><div class="tex2jax_ignore mathjax_ignore section" id="continuous-performance-monitoring">
<h3>Continuous Performance Monitoring<a class="headerlink" href="#continuous-performance-monitoring" title="Permalink to this headline">¶</a></h3>
<p><strong>Continuous performance monitoring</strong> is the activity to track, log and monitor over time the behaviour and the performance of Artificial Intelligence and Machine Learning models. This activity is particularly relevant after in-production deployment in order to detect any performance drifts and outages of the model.</p>
<p>You can find futher information about Continuous Performance Monitoring term <a class="reference internal" href="TAILOR.html#document-Accountability/L3.Continuous_performance_monitoring"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Counterexemplar"></span><div class="tex2jax_ignore mathjax_ignore section" id="counterexemplars">
<h3>Counterexemplars<a class="headerlink" href="#counterexemplars" title="Permalink to this headline">¶</a></h3>
<p>A <strong>counterexemplar explanation</strong> shows what should have been different to change the decision of an AI system. For example, a counterexemplars explanation could be a local explaination of a certain istance by providing the nearest istances that lead to a different decision or describing a small change in the input of the model that lead to a change in the outcome of the model.</p>
<p>You can find futher information about Counterexemplars term <a class="reference internal" href="TAILOR.html#document-Transparency/counterfactual"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Counterfactual"></span><div class="tex2jax_ignore mathjax_ignore section" id="counterfactuals">
<h3>Counterfactuals<a class="headerlink" href="#counterfactuals" title="Permalink to this headline">¶</a></h3>
<p>A <strong>counterfactual explanation</strong> shows what should have been different to change the decision of an AI system. For example, a counterfactual explanation could be a local explaination of a certain istance by providing the nearest istances that lead to a different decision or describing a small change in the input of the model that lead to a change in the outcome of the model.</p>
<p>You can find futher information about Counterfactual term <a class="reference internal" href="TAILOR.html#document-Transparency/counterfactual"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/cradle 2 cradle"></span><div class="tex2jax_ignore mathjax_ignore section" id="cradle-2-cradle">
<h3>Cradle 2 cradle<a class="headerlink" href="#cradle-2-cradle" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms</em>: 2CC2, C2C, Cradle-to-cradle Design, regenerative design.</p>
<p><strong>Cradle 2 cradle</strong> is a biomimetic approach to product and system design that mimics natural processes, in which materials are considered as nutrients flowing in healthy, safe metabolisms.</p>
<p>You can find futher information about cradle 2 cradle <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/cradle_to_cradle"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Cradle"></span><div class="tex2jax_ignore mathjax_ignore section" id="cradle-to-cradle-design">
<h3>Cradle-to-cradle Design<a class="headerlink" href="#cradle-to-cradle-design" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms</em>: 2CC2, C2C, cradle 2 cradle, regenerative design.</p>
<p><strong>Cradle-to-cradle Design</strong> (also known as <em>2CC2</em>, <em>C2C</em>, <em>cradle 2 cradle</em>, or <em>regenerative design</em>) is a biomimetic approach to product and system design that mimics natural processes, in which materials are considered as nutrients flowing in healthy, safe metabolisms.</p>
<p>You can find futher information about Cradle-to-cradle Design <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/cradle_to_cradle"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/Data Anonymization"></span><div class="tex2jax_ignore mathjax_ignore section" id="data-anonymization">
<h3>Data Anonymization<a class="headerlink" href="#data-anonymization" title="Permalink to this headline">¶</a></h3>
<p>A data subject is considered anonymous if it is reasonably hard to attribute his personal data to him/her.</p>
<p>You can find futher information about Data Anonymization <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L1.anonymization"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Data Center"></span><div class="tex2jax_ignore mathjax_ignore section" id="data-center">
<h3>Data Center<a class="headerlink" href="#data-center" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p>A <strong>data centre</strong> is a structure, a specialised area inside a structure, or a collection of structures used to house computer systems and related components such as telecommunications and storage systems. Because IT operations are so important for business continuity, they usually incorporate redundant or backup components and infrastructure for power, data transmission connections, environmental control (such as air conditioning and fire suppression), and other security systems. A huge data centre is a large-scale activity that consumes the same amount of power as a small town.</p>
<p>You can find futher information about Data Centers <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/data_centre"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Data Poisoning"></span><div class="tex2jax_ignore mathjax_ignore section" id="data-poisoning">
<h3>Data Poisoning<a class="headerlink" href="#data-poisoning" title="Permalink to this headline">¶</a></h3>
<p><strong>Data poisoning</strong> occurs when an adversary modifies or manipulates part of the dataset upon which a model will be trained, validated, or tested. By altering a selected subset of training inputs, a poisoning attack can induce a trained AI system into curated misclassification, systemic malfunction, and poor performance. An especially concerning dimension of targeted data poisoning is that an adversary may introduce a ‘backdoor’ into the infected model whereby the trained system functions normally until it processes maliciously selected inputs that trigger error or failure. Data poisoning is possible because data collection and procurement often involves potentially unreliable or questionable sources. When data originates in uncontrollable environments like the internet, social media, or the Internet of Things, many opportunities present themselves to ill-intentioned attackers, who aim to manipulate training examples. Likewise, in third-party data curation processes (such as ‘crowdsourced’ labelling, annotation, and content identification), attackers may simply handcraft malicious inputs.</p>
<p>You can find futher information about Data Poisoning <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/data_poisoning"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Dependability"></span><div class="tex2jax_ignore mathjax_ignore section" id="dependability">
<h3>Dependability<a class="headerlink" href="#dependability" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Reliability.</p>
<p>The objective of <strong>dependability</strong> is that an AI system behaves exactly as its designers intended and anticipated, over time. A reliable system adheres to the specifications it was programmed to carry out at any time. Reliability is therefore a measure of consistency of operation and can establish confidence in the safety of a system based upon the dependability with which it operationally conforms to its intended functionality.</p>
<p>You can find futher information about Dependability <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/reliability"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/Differential Privacy models"></span><div class="tex2jax_ignore mathjax_ignore section" id="differential-privacy-models">
<h3>Differential Privacy Models<a class="headerlink" href="#differential-privacy-models" title="Permalink to this headline">¶</a></h3>
<p><strong>Differential privacy</strong> implies that adding or deleting a single record does not significantly affect the result of any analysis.</p>
<p>You can find futher information about Differential Privacy Models <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.differential_privacy"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/emotional_impact"></span><div class="tex2jax_ignore mathjax_ignore section" id="emotional-impact">
<h3>Emotional Impact<a class="headerlink" href="#emotional-impact" title="Permalink to this headline">¶</a></h3>
<p>Due to the fast development of AI systems, their behavior, in some aspects, has become akin to humans. Especially with the introduction of large language models like chatGPT, it has become increasingly difficult to differentiate human and AI generated language. When coupled with human-like design, this cognitive ability has been found to lead to emotional attachment by users. While emotional impact cannot be prevented entirely, it is important to assess the extent to which the AI encourages human attachment and ensure clear signaling to the end-user that they are interacting with an AI.</p>
<p>You can find futher information about emotional impact of AI <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/emotional_impact"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/epsilon_delta-differential_privacy"></span><div class="tex2jax_ignore mathjax_ignore section" id="epsilon-delta-differential-privacy">
<h3>(<span class="math notranslate nohighlight">\(\epsilon\)</span>,<span class="math notranslate nohighlight">\(\delta\)</span>)-Differential Privacy<a class="headerlink" href="#epsilon-delta-differential-privacy" title="Permalink to this headline">¶</a></h3>
<p>A relaxed version of <em>Differential Privacy</em>, named <strong>(<span class="math notranslate nohighlight">\(\epsilon\)</span>,<span class="math notranslate nohighlight">\(\delta\)</span>)-Differential Privacy</strong>, allows a little privacy loss (<span class="math notranslate nohighlight">\(\delta\)</span>) due to a variation in the output distribution for the privacy mechanism.</p>
<p>You can find futher information about (<span class="math notranslate nohighlight">\(\epsilon\)</span>,<span class="math notranslate nohighlight">\(\delta\)</span>)-Differential Privacy <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L3.epsilon_delta_DP"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/Epsilon-differential_privacy"></span><div class="tex2jax_ignore mathjax_ignore section" id="epsilon-differential-privacy">
<h3><span class="math notranslate nohighlight">\(\epsilon\)</span>-Differential Privacy<a class="headerlink" href="#epsilon-differential-privacy" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: <span class="math notranslate nohighlight">\(\epsilon\)</span>-indistinguishability.</p>
<p><strong><span class="math notranslate nohighlight">\(\epsilon\)</span>-Differential Privacy</strong> is the simpler form of <em>Differential Privacy</em>, where <span class="math notranslate nohighlight">\(\epsilon\)</span> represents the level of privacy guarantee.</p>
<p>You can find futher information about <span class="math notranslate nohighlight">\(\epsilon\)</span>-Differential Privacy <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L3.epsilon_DP"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/Epsilon-indist"></span><div class="tex2jax_ignore mathjax_ignore section" id="epsilon-indistinguishability">
<h3><span class="math notranslate nohighlight">\(\epsilon\)</span>-Indistinguishability<a class="headerlink" href="#epsilon-indistinguishability" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: <span class="math notranslate nohighlight">\(\epsilon\)</span> Differential Privacy.</p>
<p><strong><span class="math notranslate nohighlight">\(\epsilon\)</span>-Indistinguishability</strong> is the simpler form of <em>Differential Privacy</em>, where <span class="math notranslate nohighlight">\(\epsilon\)</span> represents the level of privacy guarantee.</p>
<p>You can find futher information about <span class="math notranslate nohighlight">\(\epsilon\)</span>-Indistinguishability <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L3.epsilon_DP"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Data Shift"></span><div class="tex2jax_ignore mathjax_ignore section" id="distributional-shift">
<h3>Distributional Shift<a class="headerlink" href="#distributional-shift" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Distributional Shift.</p>
<p>Once trained, most machine learning systems operate on static models of the world that have been built from historical data which have become fixed in the systems’ parameters. This freezing of the model before it is released ‘into the wild’ makes its accuracy and reliability especially vulnerable to changes in the underlying distribution of data. When the historical data that have crystallised into the trained model’s architecture cease to reflect the population concerned, the model’s mapping function will no longer be able to accurately and reliably transform its inputs into its target output values. These systems can quickly become prone to error in unexpected and harmful ways. In all cases, the system and the operators must remain vigilant to the potentially rapid concept drifts that may occur in the complex, dynamic, and evolving environments in which your AI project will intervene. Remaining aware of these transformations in the data is crucial for safe AI.</p>
<p>You can find futher information about Data Shift <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/distributional_shift"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Dimensions of Explanations"></span><div class="tex2jax_ignore mathjax_ignore section" id="dimensions-of-explanations">
<h3>Dimensions of Explanations<a class="headerlink" href="#dimensions-of-explanations" title="Permalink to this headline">¶</a></h3>
<p><strong>Dimensions of explanations</strong> are useful to analyze the interpretability of AI systems and to classify the explanation method.</p>
<p>You can find futher information about Dimensions of Explanations <a class="reference internal" href="TAILOR.html#document-Transparency/XAI_dimensions"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Diversity_Non-Discrimination_and_Fairness/Discrimination"></span><div class="tex2jax_ignore mathjax_ignore section" id="grounds-of-discrimination">
<h3>Grounds of Discrimination<a class="headerlink" href="#grounds-of-discrimination" title="Permalink to this headline">¶</a></h3>
<p>International and national laws prohibit <strong>discriminating on some explicitly defined grounds</strong>, such as race, sex, religion, etc. They can be considered in isolation, or interacting, giving rise to multiple discrimination and intersectional discrimination.</p>
<p>You can find futher information about Discrimination <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/discrimination"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Distributional Shift"></span><div class="tex2jax_ignore mathjax_ignore section" id="distributional-shift">
<h3>Distributional Shift<a class="headerlink" href="#distributional-shift" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Data Shift.</p>
<p>Once trained, most machine learning systems operate on static models of the world that have been built from historical data which have become fixed in the systems’ parameters. This freezing of the model before it is released ‘into the wild’ makes its accuracy and reliability especially vulnerable to changes in the underlying distribution of data. When the historical data that have crystallised into the trained model’s architecture cease to reflect the population concerned, the model’s mapping function will no longer be able to accurately and reliably transform its inputs into its target output values. These systems can quickly become prone to error in unexpected and harmful ways. In all cases, the system and the operators must remain vigilant to the potentially rapid concept drifts that may occur in the complex, dynamic, and evolving environments in which your AI project will intervene. Remaining aware of these transformations in the data is crucial for safe AI.</p>
<p>You can find futher information about Distributional Shift <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/distributional_shift"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Direct"></span><div class="tex2jax_ignore mathjax_ignore section" id="direct-behaviour">
<h3>Direct Behaviour<a class="headerlink" href="#direct-behaviour" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Aligment, (Un)intended behaviour</p>
<p>The goal of AI <strong>direct behaviour</strong> is to ensure that AI systems are aligned with human intentions and values. This first requires determining the normative question of what values or principles we have and what humans really want, collectively or individually, and second, the technical question of how to imbue AI systems with these values and goals.</p>
<p>You can find futher information about Direct Behaviour <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/alignment"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Edge Computing"></span><div class="tex2jax_ignore mathjax_ignore section" id="edge-computing">
<h3>Edge Computing<a class="headerlink" href="#edge-computing" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms</em>: Fog Computing.</p>
<p><strong>Edge Computing</strong> is a distributed computing paradigm in which processing and data storage are brought closer to the data sources. This should increase response times while also conserving bandwidth. Rather than referring to a single technology, the phrase refers to an architecture.</p>
<p>You can find futher information about Edge Computing <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/edge_computing"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Energy Aware"></span><div class="tex2jax_ignore mathjax_ignore section" id="energy-aware-computing">
<h3>Energy-aware Computing<a class="headerlink" href="#energy-aware-computing" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms</em>: Power-aware computing, Energy-efficient computing.</p>
<p><strong>Energy-aware computing</strong> is part of the Green IT. Power-aware design strategies strive to maximise performance in high-performance systems within power dissipation and power consumption constraints. Reduced power utilisation on a node is one way to reduce the amount of energy required to compute. Lowering the frequency at which the CPU works on one approach to do this. Reduced clock speed, on the other hand, increases the time to solution, posing a potential compromise.</p>
<p>You can find futher information about Energy-aware Computing <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/power_aware"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Energy Efficient"></span><div class="tex2jax_ignore mathjax_ignore section" id="energy-efficient-computing">
<h3>Energy-efficient Computing<a class="headerlink" href="#energy-efficient-computing" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms</em>: Power-aware computing, Energy-aware computing.</p>
<p><strong>Energy-efficient computing</strong> is part of the Green IT. Power-aware design strategies strive to maximise performance in high-performance systems within power dissipation and power consumption constraints. Reduced power utilisation on a node is one way to reduce the amount of energy required to compute. Lowering the frequency at which the CPU works on one approach to do this. Reduced clock speed, on the other hand, increases the time to solution, posing a potential compromise.</p>
<p>You can find futher information about Energy-efficient Computing <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/power_aware"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Evaluation"></span><div class="tex2jax_ignore mathjax_ignore section" id="evaluation">
<h3>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Assessment, Testing, Measurement.</p>
<p><strong>AI evaluation</strong> is any activity that estimates attributes as measures— of an AI system or some of its components, abstractly or in particular contexts of operation. These attributes, if well estimated, can be used to explain and predict the behaviour of the system. This can stem from an engineering perspective, trying to understand whether a particular AI system meets the specifications or the intention of their designers, known respectively as <strong>verification</strong> and <strong>validation</strong>. Under this perspective, AI measurement is close to computer systems <strong>testing</strong> (hardware and/or software) and other evaluation procedures in engineering. However, in AI there is an extremely complex adaptive behaviour, and in many cases, with a lack of a written and operational specification. What the systems has to do depends on some constraints and utility functions that have to be optimised, is specified by example (from which the system has to learn a model) or ultimately depends on feedback from the user or the environment (e.g., in the form of rewards).</p>
<p>You can find futher information about Evaluation <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/evaluation"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Diversity_Non-Discrimination_and_Fairness/Equity"></span><div class="tex2jax_ignore mathjax_ignore section" id="equity">
<h3>Equity<a class="headerlink" href="#equity" title="Permalink to this headline">¶</a></h3>
<p>Forms of bias that count as discrimination against social groups or individuals should be avoided, both from legal and ethical perspectives. Discrimination can be direct or indirect, intentional or unintentional.</p>
<p>You can find futher information about Equity <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/equity"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Exemplars"></span><div class="tex2jax_ignore mathjax_ignore section" id="exemplars">
<h3>Exemplars<a class="headerlink" href="#exemplars" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Prototypes.</p>
<p><strong>Exemplars-based explanation</strong> methods return as explanation a selection of particular instances of the dataset for locally explaining the behavior of the AI system</p>
<p>You can find futher information about Exemplars-based explanation term <a class="reference internal" href="TAILOR.html#document-Transparency/prototypes"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Explainable AI"></span><div class="tex2jax_ignore mathjax_ignore section" id="explainable-ai">
<h3>Explainable AI<a class="headerlink" href="#explainable-ai" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: XAI.</p>
<p><strong>Explainable AI</strong> (often shortened to <strong>XAI</strong>) is one of the ethical dimensions described in the General Data Protection Regulation (GDPR).
Indeed, the GDPR mentions the right to explanation, as a suitable safeguard to ensure fair and transparent processing in respect of data subjects. It is defined as the right “to obtain an explanation of the decision reached after profiling”.</p>
<p>You can find futher information about Explainable AI term <a class="reference internal" href="TAILOR.html#document-Transparency/XAI"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Explanation by Design"></span><div class="tex2jax_ignore mathjax_ignore section" id="explanation-by-design">
<h3>Explanation by Design<a class="headerlink" href="#explanation-by-design" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Ante-hoc Explanation, Transparent model.</p>
<p><strong>Explanation by Design</strong> means to rely, by design, on a transparent model, instead of providing explanations of an AI model.</p>
<p>You can find futher information about Explanation by Design <a class="reference internal" href="TAILOR.html#document-Transparency/blackbox_transparent"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Diversity_Non-Discrimination_and_Fairness/Fair Machine Learning"></span><div class="tex2jax_ignore mathjax_ignore section" id="fair-machine-learning">
<h3>Fair Machine Learning<a class="headerlink" href="#fair-machine-learning" title="Permalink to this headline">¶</a></h3>
<p><strong>Fair Machine Learning</strong> models take into account the issues of bias and fairness. Approaches can be categorized as pre-processig, which transform the input data, as in-processing, which modify the learning algorithm, and post-processing, which alter models’ internals or their decisions.</p>
<p>You can find futher information about Fair Machine Learning <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/fair_ML"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Diversity_Non-Discrimination_and_Fairness/Fairness"></span><div class="tex2jax_ignore mathjax_ignore section" id="fairness">
<h3>Fairness<a class="headerlink" href="#fairness" title="Permalink to this headline">¶</a></h3>
<p>The term <strong>fairness</strong> is defined as the quality or state of being fair; or a lack of favoritism towards one side. The notions of fairness, and quantitative measures of them (fairness metrics), can be distinguished based on the focus on individuals, groups and sub-groups.</p>
<p>You can find futher information about Fairness <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/fairness"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Features Importance"></span><div class="tex2jax_ignore mathjax_ignore section" id="feature-importance">
<h3>Feature Importance<a class="headerlink" href="#feature-importance" title="Permalink to this headline">¶</a></h3>
<p>The <strong>feature importance</strong> technique provides a score, representing the “importance”, for all the input features for a given AI model, i.e., a higher importance means that the corresponding feature will have a larger effect on the model.</p>
<p>You can find futher information about Feature Importance <a class="reference internal" href="TAILOR.html#document-Transparency/feature_importance"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/Federated"></span><div class="tex2jax_ignore mathjax_ignore section" id="federated-learning">
<h3>Federated Learning<a class="headerlink" href="#federated-learning" title="Permalink to this headline">¶</a></h3>
<p><strong>Federated Learning</strong> is a paradigm of distributed processing, where models instead of data are shared among peers.</p>
<p>You can find futher information about Federated Learning <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.federated"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Accountability/Frame"></span><div class="tex2jax_ignore mathjax_ignore section" id="the-frame-problem">
<h3>The Frame Problem<a class="headerlink" href="#the-frame-problem" title="Permalink to this headline">¶</a></h3>
<p>The <strong>frame problem</strong> is the challenge of knowing and modeling the relevant features and context of situations, and getting an agent to act on those without consideration all the irrelevant facts as well.</p>
<p>You can find futher information about The Frame Problem term <a class="reference internal" href="TAILOR.html#document-Accountability/L3.The_frame_problem"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Fog Computing"></span><div class="tex2jax_ignore mathjax_ignore section" id="fog-computing">
<h3>Fog Computing<a class="headerlink" href="#fog-computing" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms</em>: Edge Computing.</p>
<p><strong>Fog Computing</strong> is a distributed computing paradigm in which processing and data storage are brought closer to the data sources. This should increase response times while also conserving bandwidth. Rather than referring to a single technology, the phrase refers to an architecture.</p>
<p>You can find futher information about Fog Computing <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/edge_computing"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Generalizable XAI"></span><div class="tex2jax_ignore mathjax_ignore section" id="model-agnostic">
<h3>Model Agnostic<a class="headerlink" href="#model-agnostic" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Model Agnostic Explanation.</p>
<p>We distinguish between <strong>model-specific</strong> or <strong>model-agnostic</strong> explanation method depending on whether the technique adopted to retrieve the explanation acts on a particular model adopted by an AI system, or can be used on any type of AI.</p>
<p>You can find futher information about Model Agnostic term <a class="reference internal" href="TAILOR.html#document-Transparency/model_specific"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Global Explanations"></span><div class="tex2jax_ignore mathjax_ignore section" id="global-explanations">
<h3>Global Explanations<a class="headerlink" href="#global-explanations" title="Permalink to this headline">¶</a></h3>
<p><strong>Global explanation</strong> is an explanation that allows understanding the whole logic of a model used by an AI system.</p>
<p>You can find futher information about Global Explanations <a class="reference internal" href="TAILOR.html#document-Transparency/global_local"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Green AI"></span><div class="tex2jax_ignore mathjax_ignore section" id="green-ai">
<h3>Green AI<a class="headerlink" href="#green-ai" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms</em>: Green IT, Green Computing, ICT sustainability.</p>
<p>The goal of <strong>Green AI</strong> is to minimise the negative aspects of IT operations on the environment. To do so, computers and IT products can be designed, manufactured and disposed of in an environmentally-friendly manner.</p>
<p>You can find futher information about Green AI <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/greenAI"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Green Computing"></span><div class="tex2jax_ignore mathjax_ignore section" id="green-computing">
<h3>Green Computing<a class="headerlink" href="#green-computing" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms</em>: Green IT, Green AI, ICT sustainability.</p>
<p>The goal of <strong>Green Computing</strong> is to minimise the negative aspects of IT operations on the environment. To do so, computers and IT products can be designed, manufactured and disposed of in an environmentally-friendly manner.</p>
<p>You can find futher information about Green Computing <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/greenAI"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Green IT"></span><div class="tex2jax_ignore mathjax_ignore section" id="green-it">
<h3>Green IT<a class="headerlink" href="#green-it" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms</em>: Green AI, Green Computing, ICT sustainability.</p>
<p>The goal of <strong>Green IT</strong> is to minimise the negative aspects of IT operations on the environment. To do so, computers and IT products can be designed, manufactured and disposed of in an environmentally-friendly manner.</p>
<p>You can find futher information about Green IT <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/greenAI"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/ICT sustainability"></span><div class="tex2jax_ignore mathjax_ignore section" id="ict-sustainability">
<h3>ICT sustainability<a class="headerlink" href="#ict-sustainability" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms</em>: Green IT, Green Computing, Green AI.</p>
<p>The goal of <strong>ICT sustainability</strong> is to minimise the negative aspects of IT operations on the environment. To do so, computers and IT products can be designed, manufactured and disposed of in an environmentally-friendly manner.</p>
<p>You can find futher information about ICT sustainability <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/greenAI"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Intended"></span><div class="tex2jax_ignore mathjax_ignore section" id="intended-behaviour">
<h3>Intended Behaviour<a class="headerlink" href="#intended-behaviour" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: (Mis)directed behaviour, Alignement</p>
<p>The goal of AI <strong>intended behaviour</strong> is to ensure that AI systems are aligned with human intentions and values. This first requires determining the normative question of what values or principles we have and what humans really want, collectively or individually, and second, the technical question of how to imbue AI systems with these values and goals.</p>
<p>You can find futher information about Intended Behaviour <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/alignment"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Diversity_Non-Discrimination_and_Fairness/intersectionality"></span><div class="tex2jax_ignore mathjax_ignore section" id="intersectionality">
<h3>Intersectionality<a class="headerlink" href="#intersectionality" title="Permalink to this headline">¶</a></h3>
<p><strong>Intersectionality</strong> focuses on a specific type of bias due to the combination of sensitive factors. An individual might not be discriminated against based on race or based on gender only, but she might be discriminated against because of a combination of both. Black women are particularly prone to this type of discrimination.</p>
<p>You can find futher information about Discrimination <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/intersectionality"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Diversity_Non-Discrimination_and_Fairness/Justice"></span><div class="tex2jax_ignore mathjax_ignore section" id="justice">
<h3>Justice<a class="headerlink" href="#justice" title="Permalink to this headline">¶</a></h3>
<p><strong>Justice</strong> encompasses three different perspectives: (1) <em>fairness</em> understood as the fair treatment of people, (2) <em>rightness</em> as the quality of being fair or reasonable, and (3) a legal system, the scheme or system of law. Justice can be distinguished between <em>substantive</em> and <em>procedural</em>.</p>
<p>You can find futher information about Justice <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/justice"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/K-Anonymity"></span><div class="tex2jax_ignore mathjax_ignore section" id="k-anonymity">
<h3>K-anonymity<a class="headerlink" href="#k-anonymity" title="Permalink to this headline">¶</a></h3>
<p><strong>k-anonimity</strong> (and the whole family of anonymity by <strong>indistinguishability models</strong>) is based on comparison among individuals present in data, and it aims to make each individual so similar as to be indistinguishable from at least <em>k-1</em> others.</p>
<p>You can find futher information about K-anonymity <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.k_anonymity"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/L_diversity"></span><div class="tex2jax_ignore mathjax_ignore section" id="l-diversity">
<h3>l-diversity<a class="headerlink" href="#l-diversity" title="Permalink to this headline">¶</a></h3>
<p><strong>l-diversity</strong> aims to protect the diversity of sensitive attributes in the <strong>anonymity by indistinguishability</strong> paradigm. An anonymity set is <em>l-diverse</em> if contains at least <em>l</em> “well-represented” values for the sensitive attribute.</p>
<p>You can find futher information about <em>l</em>-diversity <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.l_diversity"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/Linking Attack"></span><div class="tex2jax_ignore mathjax_ignore section" id="linking-attack">
<h3>Linking Attack<a class="headerlink" href="#linking-attack" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Re-identification Attack, Attack on Pseudonymised Data.</p>
<p><strong>Linking Attack</strong> attack aims to link a certain set of data related to an individual in a dataset (which does not contain direct identifiers) to a real identity, relying on additional information.</p>
<p>You can find futher information about Linking Attack <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.reidentification"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Local Explanations"></span><div class="tex2jax_ignore mathjax_ignore section" id="local-explanations">
<h3>Local Explanations<a class="headerlink" href="#local-explanations" title="Permalink to this headline">¶</a></h3>
<p><strong>Local explanation</strong> is an explanation that refers to a specific case, i.e., only a single decision is interpretable.</p>
<p>You can find futher information about Local Explanations <a class="reference internal" href="TAILOR.html#document-Transparency/global_local"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Lore"></span><div class="tex2jax_ignore mathjax_ignore section" id="local-rule-based-explanation">
<h3>Local Rule-based Explanation<a class="headerlink" href="#local-rule-based-explanation" title="Permalink to this headline">¶</a></h3>
<p><strong>Local Rule-based Explanation</strong> aims to extract a rule that provides a local explaination for a certain instance of the model.</p>
<p>You can find futher information about Exemplars-based explanation term <a class="reference internal" href="TAILOR.html#document-Transparency/lore"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Human_Agency_and_Oversight/Meaningful_human_control"></span><div class="tex2jax_ignore mathjax_ignore section" id="meaningful-human-control">
<h3>Meaningful Human Control<a class="headerlink" href="#meaningful-human-control" title="Permalink to this headline">¶</a></h3>
<p><strong>Meaningful human control</strong> is the notion that aims to generalize the traditional concept of operational control over technological artifacts to artificial intelligent systems. It implies that artificial systems should not make morally consequential decisions on their own, without appropriate control from responsible humans.</p>
<p>You can find futher information about Meaningful Human Control <a class="reference internal" href="TAILOR.html#document-Human_Agency_and_Oversight/Meaningful_human_control"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Measurement"></span><div class="tex2jax_ignore mathjax_ignore section" id="measurement">
<h3>Measurement<a class="headerlink" href="#measurement" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Assessment, Testing, Evaluation.</p>
<p><strong>AI measurement</strong> is any activity that estimates attributes as measures— of an AI system or some of its components, abstractly or in particular contexts of operation. These attributes, if well estimated, can be used to explain and predict the behaviour of the system. This can stem from an engineering perspective, trying to understand whether a particular AI system meets the specifications or the intention of their designers, known respectively as <strong>verification</strong> and <strong>validation</strong>. Under this perspective, AI measurement is close to computer systems <strong>testing</strong> (hardware and/or software) and other evaluation procedures in engineering. However, in AI there is an extremely complex adaptive behaviour, and in many cases, with a lack of a written and operational specification. What the systems has to do depends on some constraints and utility functions that have to be optimised, is specified by example (from which the system has to learn a model) or ultimately depends on feedback from the user or the environment (e.g., in the form of rewards).</p>
<p>You can find futher information about Measurement <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/evaluation"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Mesh Computing"></span><div class="tex2jax_ignore mathjax_ignore section" id="mesh-computing">
<h3>Mesh Computing<a class="headerlink" href="#mesh-computing" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms</em>: Cloud Computing</p>
<p><strong>Mesh Computing</strong> is the provision of computing resources (storage and processing power) on demand, without direct user intervention. A large cloud often includes multiple data centres, each housing a different set of functions. The cloud computing model aims to achieve core economies of scale through sharing of resources, taking advantage of a “pay-as-you-go” model that can decrease capital expenditures, but can also result in unforeseen operating expenses for users who are unaware of the concept.</p>
<p>You can find futher information about Mesh Computing <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/cloud_computing"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Misdirect"></span><div class="tex2jax_ignore mathjax_ignore section" id="misdirect-behaviour">
<h3>Misdirect Behaviour<a class="headerlink" href="#misdirect-behaviour" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Aligment, (Un)intended behaviour</p>
<p>The goal of AI <strong>direct behaviour</strong> is to ensure that AI systems are aligned with human intentions and values. This first requires determining the normative question of what values or principles we have and what humans really want, collectively or individually, and second, the technical question of how to imbue AI systems with these values and goals.</p>
<p>You can find futher information about Misdirect Behaviour <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/alignment"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Model-Agnostic"></span><div class="tex2jax_ignore mathjax_ignore section" id="model-agnostic">
<h3>Model Agnostic<a class="headerlink" href="#model-agnostic" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Generalizable Explanation.</p>
<p>We distinguish between <strong>model-specific</strong> or <strong>model-agnostic</strong> explanation method depending on whether the technique adopted to retrieve the explanation acts on a particular model adopted by an AI system, or can be used on any type of AI.</p>
<p>You can find futher information about Model Agnostic term <a class="reference internal" href="TAILOR.html#document-Transparency/model_specific"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Model-Specific"></span><div class="tex2jax_ignore mathjax_ignore section" id="model-specific">
<h3>Model Specific<a class="headerlink" href="#model-specific" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Not Generalizable Explanation.</p>
<p>We distinguish between <strong>model-specific</strong> or <strong>model-agnostic</strong> explanation method depending on whether the technique adopted to retrieve the explanation acts on a particular model adopted by an AI system, or can be used on any type of AI.</p>
<p>You can find futher information about Model Specific term <a class="reference internal" href="TAILOR.html#document-Transparency/model_specific"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Negative Side Effects"></span><div class="tex2jax_ignore mathjax_ignore section" id="negative-side-effects">
<h3>Negative Side Effects<a class="headerlink" href="#negative-side-effects" title="Permalink to this headline">¶</a></h3>
<p><strong>Negative side effects</strong> are an important safety issue in AI system that considers all possible unintended harm that is caused as a secondary effect of the AI system’s operation. An agent can disrupt or break other systems around, or damage third parties, including humans, or can exhaust resources, or a combination of all this. This usually happens because many things the system should not do are not included in its specification. In the case of AI systems, this is even more poignant as written specifications are usually replaced by an optimisation or loss function, in which it is even more difficult to express these things the system should not do, as they frequently rely on ‘common sense’.</p>
<p>You can find futher information about Negative Side Effects <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/negative_side_effects"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Not Generalizable XAI"></span><div class="tex2jax_ignore mathjax_ignore section" id="model-specific">
<h3>Model Specific<a class="headerlink" href="#model-specific" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Not Generalizable Explanation.</p>
<p>We distinguish between <strong>model-specific</strong> or <strong>model-agnostic</strong> explanation method depending on whether the technique adopted to retrieve the explanation acts on a particular model adopted by an AI system, or can be used on any type of AI.</p>
<p>You can find futher information about Model Specific term <a class="reference internal" href="TAILOR.html#document-Transparency/model_specific"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/Perturbation"></span><div class="tex2jax_ignore mathjax_ignore section" id="achiving-differential-privacy">
<h3>Achiving Differential Privacy<a class="headerlink" href="#achiving-differential-privacy" title="Permalink to this headline">¶</a></h3>
<p>Differential privacy guarantees can be provided by <strong>perturbation mechanisms</strong> aim at randomizing the output distributions of functions in order to provide privacy guarantees.</p>
<p>You can find futher information about Achiving Differential Privacy <span class="xref myst">here</span></p>
</div>
<span id="document-index/Transparency/Post-hoc Explanations"></span><div class="tex2jax_ignore mathjax_ignore section" id="post-hoc-explanation">
<h3>Post-hoc Explanation<a class="headerlink" href="#post-hoc-explanation" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Black-box Explanations.</p>
<p>With a <strong>post-hoc explanation</strong> we pair the black box model with an interpretation the black box decisions or model, instead of relying, by design, on a transparent model.</p>
<p>You can find futher information about Post-hoc Explanation <a class="reference internal" href="TAILOR.html#document-Transparency/blackbox_transparent"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Power Aware"></span><div class="tex2jax_ignore mathjax_ignore section" id="power-aware-computing">
<h3>Power-aware Computing<a class="headerlink" href="#power-aware-computing" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms</em>: Energy-aware computing, Energy-efficient computing.</p>
<p><strong>Power-aware computing</strong> is part of the Green IT. Power-aware design strategies strive to maximise performance in high-performance systems within power dissipation and power consumption constraints. Reduced power utilisation on a node is one way to reduce the amount of energy required to compute. Lowering the frequency at which the CPU works on one approach to do this. Reduced clock speed, on the other hand, increases the time to solution, posing a potential compromise.</p>
<p>You can find futher information about Power-aware Computing <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/power_aware"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/Privacy model"></span><div class="tex2jax_ignore mathjax_ignore section" id="privacy-models">
<h3>Privacy models<a class="headerlink" href="#privacy-models" title="Permalink to this headline">¶</a></h3>
<p>There are essentially two families of models, based on different goals and mechanisms: <em>anonymity by randomization</em> (where the most recent paradigm is <em>Differential Privacy</em>) and <em>anonymity by indistinguishability</em> (whose most famous example is <em>k-anonymity</em>).</p>
<p>You can find futher information about Privacy models <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L1.privacy_model"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Accountability/Problem_of_many_hands"></span><div class="tex2jax_ignore mathjax_ignore section" id="problem-of-many-hands">
<h3>Problem of Many Hands<a class="headerlink" href="#problem-of-many-hands" title="Permalink to this headline">¶</a></h3>
<p>This challenge aims to understand who is morally responsible in a situation with several actors.</p>
<p>You can find futher information about the Problem of Many Hands <a class="reference internal" href="TAILOR.html#document-Accountability/L3.Problem_of_many_hands"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Prototypes"></span><div class="tex2jax_ignore mathjax_ignore section" id="prototypes">
<h3>Prototypes<a class="headerlink" href="#prototypes" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Exemplars.</p>
<p><strong>Prototype-based explanation</strong> methods return as explanation a selection of particular instances of the dataset for locally explaining the behavior of the AI system</p>
<p>You can find futher information about Prototype-based explanation term <a class="reference internal" href="TAILOR.html#document-Transparency/prototypes"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Accountability/Provenance"></span><div class="tex2jax_ignore mathjax_ignore section" id="provenance-tracking">
<h3>Provenance Tracking<a class="headerlink" href="#provenance-tracking" title="Permalink to this headline">¶</a></h3>
<p><strong>Provenance tracking</strong> represents the tracking of “information that describes the production process of an end product, which can be anything from a piece of data to a physical object. […] Essentially, provenance can be seen as meta-data that, instead of describing data, describes a production process.”</p>
<p>You can find futher information about Provenance Tracking term <a class="reference internal" href="TAILOR.html#document-Accountability/L3.Provenance_tracking"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/Pseudonymised Data"></span><div class="tex2jax_ignore mathjax_ignore section" id="pseudonymization">
<h3>Pseudonymization<a class="headerlink" href="#pseudonymization" title="Permalink to this headline">¶</a></h3>
<p><strong>Pseudonymisation</strong> aims to substitute one or more identifiers that link(s) the identity of an individual to its data with a surrogate value, called <strong>pseudonym</strong> or <strong>token</strong>.</p>
<p>You can find futher information about Data Pseudonymization <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.pseudonymization"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/Randomization"></span><div class="tex2jax_ignore mathjax_ignore section" id="randomization-methods">
<h3>Randomization Methods<a class="headerlink" href="#randomization-methods" title="Permalink to this headline">¶</a></h3>
<p><strong>Randomization methods</strong> are used to perturb data in order to preserve the privacy of sensitive information.</p>
<p>You can find futher information about randomization methods <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.randomization"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/Re-identification Attack"></span><div class="tex2jax_ignore mathjax_ignore section" id="re-identification-attack">
<h3>Re-identification Attack<a class="headerlink" href="#re-identification-attack" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Linking Attack, Attack on Pseudonymised Data.</p>
<p><strong>Re-identification</strong> attack aims to link a certain set of data related to an individual in a dataset (which does not contain direct identifiers) to a real identity, relying on additional information.</p>
<p>You can find futher information about Re-identification Attack <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.reidentification"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Regenerative Design"></span><div class="tex2jax_ignore mathjax_ignore section" id="regenerative-design">
<h3>Regenerative Design<a class="headerlink" href="#regenerative-design" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms</em>: 2CC2, C2C, cradle 2 cradle, Cradle-to-cradle Design.</p>
<p><strong>Regenerative Design</strong> is a biomimetic approach to product and system design that mimics natural processes, in which materials are considered as nutrients flowing in healthy, safe metabolisms.</p>
<p>You can find futher information about Regenerative Design <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/cradle_to_cradle"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Reliability"></span><div class="tex2jax_ignore mathjax_ignore section" id="reliability">
<h3>Reliability<a class="headerlink" href="#reliability" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Dependability.</p>
<p>The objective of <strong>reliability</strong> is that an AI system behaves exactly as its designers intended and anticipated, over time. A reliable system adheres to the specifications it was programmed to carry out at any time. Reliability is therefore a measure of consistency of operation and can establish confidence in the safety of a system based upon the dependability with which it operationally conforms to its intended functionality.</p>
<p>You can find futher information about Reliability <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/reliability"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Accountability/Repeatability"></span><div class="tex2jax_ignore mathjax_ignore section" id="repeatability">
<h3>Repeatability<a class="headerlink" href="#repeatability" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Reproducibility, Replicability.</p>
<p><strong>Repeatability</strong> is the ability of independent investigators to draw the same conclusions from an experiment by following the documentation shared by the original investigators.</p>
<p>You can find futher information about Repeatability <a class="reference internal" href="TAILOR.html#document-Accountability/L2.Reproducibility"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Accountability/Replicability"></span><div class="tex2jax_ignore mathjax_ignore section" id="replicability">
<h3>Replicability<a class="headerlink" href="#replicability" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Reproducibility, Repeatability.</p>
<p><strong>Replicability</strong> is the ability of independent investigators to draw the same conclusions from an experiment by following the documentation shared by the original investigators.</p>
<p>You can find futher information about Replicability <a class="reference internal" href="TAILOR.html#document-Accountability/L2.Reproducibility"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Accountability/Reproducibility"></span><!-- Three entries connected: Reproducibility, Replicability, Repeatability -->
<div class="tex2jax_ignore mathjax_ignore section" id="reproducibility">
<h3>Reproducibility<a class="headerlink" href="#reproducibility" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Repeatability, Replicability.</p>
<p><strong>Reproducibility</strong> is the ability of independent investigators to draw the same conclusions from an experiment by following the documentation shared by the original investigators.</p>
<p>You can find futher information about Reproducibility <a class="reference internal" href="TAILOR.html#document-Accountability/L2.Reproducibility"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Resource Allocation"></span><div class="tex2jax_ignore mathjax_ignore section" id="resource-allocation">
<h3>Resource Allocation<a class="headerlink" href="#resource-allocation" title="Permalink to this headline">¶</a></h3>
<p>Cloud computing provides a computing environment where businesses, clients, and projects can lease resources on demand. Both cloud users and providers want to allocate cloud resources efficiently and profitably. These resources are typically scarce, therefore cloud providers must make the best use of them while staying within the confines of the cloud environment and meeting the demands of cloud apps so that they may perform their jobs. The distribution of resources is one of the most important aspects of cloud computing. Its efficiency has a direct impact on the overall performance of the cloud environment. Cost efficiency, reaction time, reallocation, computing performance, and job scheduling are all key difficulties in resource allocation. Cloud computing users want to do task for the least amount of money feasible.</p>
<p>You can find futher information about Resource Allocation <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/resource_allocation"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Resource Prediction"></span><div class="tex2jax_ignore mathjax_ignore section" id="resource-prediction">
<h3>Resource Prediction<a class="headerlink" href="#resource-prediction" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms</em>: Workload Forecast, Workload Prediction.</p>
<p><strong>Resource prediction</strong> is the estimation of the resources a customer will require in the future to complete his tasks.  This concept has a wide variety of application and it is particularly studied in the context of data centres management. When these forecasts are generated, historical and current data are utilised to predict how many resource units, which tools and operative systems and the number of requests are required to accomplish a task.</p>
<p>You can find futher information about Resource Prediction <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/resource_prediction"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Resource Scheduling"></span><div class="tex2jax_ignore mathjax_ignore section" id="resource-scheduling">
<h3>Resource Scheduling<a class="headerlink" href="#resource-scheduling" title="Permalink to this headline">¶</a></h3>
<p>Cloud computing provides a computing environment where businesses, clients, and projects can lease resources on demand. Both cloud users and providers want to allocate cloud resources efficiently and profitably. These resources are typically scarce, therefore cloud providers must make the best use of them while staying within the confines of the cloud environment and meeting the demands of cloud apps so that they may perform their jobs. The distribution of resources is one of the most important aspects of cloud computing. Its efficiency has a direct impact on the overall performance of the cloud environment. Cost efficiency, reaction time, reallocation, computing performance, and job scheduling are all key difficulties in resource allocation. Cloud computing users want to do task for the least amount of money feasible.</p>
<p>You can find futher information about Resource Scheduling <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/resource_allocation"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Robustness"></span><div class="tex2jax_ignore mathjax_ignore section" id="robustness">
<h3>Robustness<a class="headerlink" href="#robustness" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Brittleness.</p>
<p><strong>Robustness</strong> is the degree in which an AI system functions reliably and accurately under harsh conditions. These conditions may include adversarial intervention, implementer error, or skewed goal-execution by an automated learner (in reinforcement learning applications). The measure of robustness is therefore the strength of a system’s integrity and the soundness of its operation in response to difficult conditions, adversarial attacks, perturbations, data poisoning, and undesirable reinforcement learning behaviour.</p>
<p>You can find futher information about Robustness <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/robustness"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Rules"></span><div class="tex2jax_ignore mathjax_ignore section" id="rules-list-and-rules-set">
<h3>Rules List and Rules Set<a class="headerlink" href="#rules-list-and-rules-set" title="Permalink to this headline">¶</a></h3>
<p>A <strong>decision rule</strong> is generally formed by a set of conditions and by a consequent, e.g., <em>if conditions, then consequent</em>.</p>
<p>You can find futher information about Rules List and Rules Set term <a class="reference internal" href="TAILOR.html#document-Transparency/rules"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Saliency Maps"></span><div class="tex2jax_ignore mathjax_ignore section" id="saliency-maps">
<h3>Saliency Maps<a class="headerlink" href="#saliency-maps" title="Permalink to this headline">¶</a></h3>
<p><strong>Saliency maps</strong> are explanations used on image classification tasks. A saliency map is an image where each pixel’s color represents a value modeling the importance of that pixel in the original image (i.e., the one given in input to the explainer) for the prediction.</p>
<p>You can find futher information about Saliency Maps <a class="reference internal" href="TAILOR.html#document-Transparency/saliency_maps"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Security"></span><div class="tex2jax_ignore mathjax_ignore section" id="security">
<h3>Security<a class="headerlink" href="#security" title="Permalink to this headline">¶</a></h3>
<p>The goal of <strong>security</strong> encompasses the protection of several operational dimensions of an AI system when confronted with possible attacks, trying to take control of the system or having access to design, operational or personal information. A secure system is capable of maintaining the integrity of the information that constitutes it. This includes protecting its architecture from the unauthorised modification or damage of any of its component parts. A secure system also keeps confidential and private information protected even under hostile or adversarial conditions.</p>
<p>You can find futher information about Security <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/security"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Diversity_Non-Discrimination_and_Fairness/Segregation"></span><div class="tex2jax_ignore mathjax_ignore section" id="segregation">
<h3>Segregation<a class="headerlink" href="#segregation" title="Permalink to this headline">¶</a></h3>
<p><strong>Social segregation</strong> refers to the separation of groups on the grounds of personal or cultural traits. Separation can be physical (e.g., in schools or neighborhoods) or virtual (e.g., in social networks).</p>
<p>You can find futher information about Segregation <a class="reference internal" href="TAILOR.html#document-Diversity_Non-Discrimination_and_Fairness/segregation"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/self-identification"></span><div class="tex2jax_ignore mathjax_ignore section" id="self-identification-of-ai">
<h3>Self-identification of AI<a class="headerlink" href="#self-identification-of-ai" title="Permalink to this headline">¶</a></h3>
<p>AI systems which are presented to be both high in intelligence and have a good quality design tend to cause humans to anthropomorphize the system; i.e. assign human-like traits to it which can lead to emotional bonding with the system. This is dangerous, especially for vulnerable groups such as older generations or those less acquainted with the mechanisms behind the AI. In addition to the threat of emotional attachment, AI systems have been found to frequently provide false or partially false information and portrait it as fact, due to a phenomenon referred to as AI hallucinations. As such it is important that users can clearly identify an artificial intelligence as such, in order to understand the system’s limitations and prevent emotional attachment and misinformation. The remaining part of this chapter will first explain which AI systems are required to be clearly identifiable and then elaborate on how these criteria should be met.</p>
<p>You can find futher information about Self-identification of AI <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/self-identification"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Single Tree Approximation"></span><div class="tex2jax_ignore mathjax_ignore section" id="single-tree-approxiamation">
<h3>Single Tree Approxiamation<a class="headerlink" href="#single-tree-approxiamation" title="Permalink to this headline">¶</a></h3>
<p>The <strong>single tree appoximation</strong> is an approach that aims at building a decision tree to approximate the behavior of a black box, typically a neural network.</p>
<p>You can find futher information about Single Tree Approxiamation <a class="reference internal" href="TAILOR.html#document-Transparency/single_tree"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/social_impact"></span><div class="tex2jax_ignore mathjax_ignore section" id="social-impact-of-ai-systems">
<h3>Social Impact of AI Systems<a class="headerlink" href="#social-impact-of-ai-systems" title="Permalink to this headline">¶</a></h3>
<p>Artificial intelligence (AI) is quickly changing the way we work and the way we live. Indeed, the <strong>societal impact of AI</strong> is on most people’s minds.</p>
<p>You can find futher information about AI Impact on the Social Impact of AI System <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/social_impact"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/society_and_democracy"></span><div class="tex2jax_ignore mathjax_ignore section" id="society-and-democracy">
<h3>Society and Democracy<a class="headerlink" href="#society-and-democracy" title="Permalink to this headline">¶</a></h3>
<p>The rapid increase of technological development is impacting society in new and partially unpredictable ways. With AI being integrated into all facets of life, ranging from recommender systems for movies to automatic passport control and facial recognition in airports, it is important to understand the negative ramifications this technology may have on society and democracy as a whole. Previously, the development of technologies was frequently guided by the interests of tech giants with profit motivating research. While this line of development has produced some impressive systems, there is little consideration for the protection of citizens and the regulation of these technologies for the greater societal good. In order to combat this trend, the European Union introduced the AI act, stipulating legislation for new AI systems depending on the level or risk posed by the system. However, with this increase in legislation, fears are rising about Europe falling behind other, non-legislated countries, in the development of new software. It is such that legislators need to find the balance between regulation and development, to protect democracy and society without adversely affecting the development of the European AI research sector.</p>
<p>You can find futher information about Society and Democracy <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/society_and_democracy"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Privacy_and_Data_Governance/T_closeness"></span><div class="tex2jax_ignore mathjax_ignore section" id="t-closeness">
<h3>t-closeness<a class="headerlink" href="#t-closeness" title="Permalink to this headline">¶</a></h3>
<p><strong>t-closeness</strong> aims to maintain the distribution of sensitive attributes in the <strong>anonymity by indistinguishability</strong> paradigm, ensuring that the distance between the two distributions (the original and the private ones) should be limited by a threshold <em>t</em>.</p>
<p>You can find futher information about <em>t</em>-closeness <a class="reference internal" href="TAILOR.html#document-Privacy_and_Data_Governance/L2.t_closeness"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Testing"></span><div class="tex2jax_ignore mathjax_ignore section" id="testing">
<h3>Testing<a class="headerlink" href="#testing" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Assessment, Evaluation, Measurement.</p>
<p><strong>AI evaluation</strong> is any activity that estimates attributes as measures— of an AI system or some of its components, abstractly or in particular contexts of operation. These attributes, if well estimated, can be used to explain and predict the behaviour of the system. This can stem from an engineering perspective, trying to understand whether a particular AI system meets the specifications or the intention of their designers, known respectively as <strong>verification</strong> and <strong>validation</strong>. Under this perspective, AI measurement is close to computer systems <strong>testing</strong> (hardware and/or software) and other evaluation procedures in engineering. However, in AI there is an extremely complex adaptive behaviour, and in many cases, with a lack of a written and operational specification. What the systems has to do depends on some constraints and utility functions that have to be optimised, is specified by example (from which the system has to learn a model) or ultimately depends on feedback from the user or the environment (e.g., in the form of rewards).</p>
<p>You can find futher information about Testing <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/evaluation"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Accountability/Traceability"></span><div class="tex2jax_ignore mathjax_ignore section" id="traceability">
<h3>Traceability<a class="headerlink" href="#traceability" title="Permalink to this headline">¶</a></h3>
<p><strong>Traceability</strong> can be defined as the need to maintain a complete and clear documentation of the data, processes, artefacts and actors involved in the entire lifecycle of an AI model, starting from its design and ending with its production serving.</p>
<p>You can find futher information about Traceability <a class="reference internal" href="TAILOR.html#document-Accountability/L2.Traceability"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/Transparency"></span><div class="tex2jax_ignore mathjax_ignore section" id="transparency">
<h3>Transparency<a class="headerlink" href="#transparency" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Explanation by Design, Ante-hoc Explanation.</p>
<p><strong>Transparency</strong> means to rely, by design, on a transparent model, instead of providing explanations of an AI model.</p>
<p>You can find futher information about Transparency <a class="reference internal" href="TAILOR.html#document-Transparency/blackbox_transparent"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Technical_Robustness_and_Safety/Unintended"></span><div class="tex2jax_ignore mathjax_ignore section" id="unintended-behaviour">
<h3>Unintended Behaviour<a class="headerlink" href="#unintended-behaviour" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: (Mis)directed behaviour, Alignement</p>
<p>The goal of AI <strong>intended behaviour</strong> is to ensure that AI systems are aligned with human intentions and values. This first requires determining the normative question of what values or principles we have and what humans really want, collectively or individually, and second, the technical question of how to imbue AI systems with these values and goals.</p>
<p>You can find futher information about Unintended Behaviour <a class="reference internal" href="TAILOR.html#document-Technical_Robustness_and_Safety/alignment"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Transparency/XAI"></span><div class="tex2jax_ignore mathjax_ignore section" id="xai">
<h3>XAI<a class="headerlink" href="#xai" title="Permalink to this headline">¶</a></h3>
<p><em>Synonyms</em>: Explainable AI.</p>
<p><strong>Explainable AI</strong> (often shortened to <strong>XAI</strong>) is one of the ethical dimensions described in the General Data Protection Regulation (GDPR).
Indeed, the GDPR mentions the right to explanation, as a suitable safeguard to ensure fair and transparent processing in respect of data subjects. It is defined as the right “to obtain an explanation of the decision reached after profiling”.</p>
<p>You can find futher information about XAI term <a class="reference internal" href="TAILOR.html#document-Transparency/XAI"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Accountability/Wicked"></span><div class="tex2jax_ignore mathjax_ignore section" id="wicked-problems">
<h3>Wicked Problems<a class="headerlink" href="#wicked-problems" title="Permalink to this headline">¶</a></h3>
<p>A class of problems for which science provides insufficient or inappropriate resolution.</p>
<p>You can find futher information about Wicked Problems term <a class="reference internal" href="TAILOR.html#document-Accountability/L3.Wicked_problems"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Workload Forecast"></span><div class="tex2jax_ignore mathjax_ignore section" id="workload-forecast">
<h3>Workload Forecast<a class="headerlink" href="#workload-forecast" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms</em>: Resource Prediction, Workload Prediction.</p>
<p><strong>Workload Forecast</strong> is the estimation of the resources a customer will require in the future to complete his tasks.  This concept has a wide variety of application and it is particularly studied in the context of data centres management. When these forecasts are generated, historical and current data are utilised to predict how many resource units, which tools and operative systems and the number of requests are required to accomplish a task.</p>
<p>You can find futher information about Workload Forecast <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/resource_prediction"><span class="doc std std-doc">here</span></a></p>
</div>
<span id="document-index/Societal_and_Environmental_Wellbeing/Workload Prediction"></span><div class="tex2jax_ignore mathjax_ignore section" id="workload-prediction">
<h3>Workload Prediction<a class="headerlink" href="#workload-prediction" title="Permalink to this headline">¶</a></h3>
<!-- (Sustainability) -->
<p><em>Synonyms</em>: Resource Prediction, Workload Forecast.</p>
<p><strong>Workload prediction</strong> is the estimation of the resources a customer will require in the future to complete his tasks.  This concept has a wide variety of application and it is particularly studied in the context of data centres management. When these forecasts are generated, historical and current data are utilised to predict how many resource units, which tools and operative systems and the number of requests are required to accomplish a task.</p>
<p>You can find futher information about Workload Prediction <a class="reference internal" href="TAILOR.html#document-Societal_and_Environmental_Wellbeing/resource_prediction"><span class="doc std std-doc">here</span></a></p>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          Di varii auctores; see <a href="/handbookTAI/authors.html" target="_blank">here</a> for the complete list of contributors. This research was partially supported by TAILOR, a project funded by EU Horizon 2020 research and innovation programme under GA No 952215<br/>
        
            &copy; Diritto d'autore 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>