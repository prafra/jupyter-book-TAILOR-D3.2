
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Technical Robustness and Safety &#8212; The TAILOR Handbook of Trustworthy AI</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tailor.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="http://tailor.isti.cnr.it/handbookTAI/index.html/Technical_Robustness_and_Safety/Technical_Robustness_and_Safety.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Alignment" href="alignment.html" />
    <link rel="prev" title="Single Tree Approximation" href="../Transparency/single_tree.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/TAILOR-logo-coloured.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">The TAILOR Handbook of Trustworthy AI</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Cerca in questo libro ..." aria-label="Cerca in questo libro ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../TAILOR.html">
   The TAILOR Handbook of Trustworthy AI
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../main/Ethical_Legal_Framework/Ethical_Legal_Framework.html">
   The Ethical and Legal Framework
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../main/Ethical_Legal_Framework/HLEG.html">
     Ethics Guidelines for Trustworthy AI by High-Level Expert Group on Artificial Intelligence
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../main/Ethical_Legal_Framework/AI_ACT.html">
     The EU AI Act
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../main/Ethical_Legal_Framework/Prohibited_AI.html">
       Prohibited AI Practices
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../main/Ethical_Legal_Framework/High_Risk_AI.html">
       High Risk AI Systems
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../main/Trustworthy_AI.html">
   Trustworthy AI
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Human_Agency_and_Oversight/Human_Agency_and_Oversight.html">
     Human Agency and Oversight
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
    <label for="toctree-checkbox-4">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Human_Agency_and_Oversight/Meaningful_human_control.html">
       Meaningful human control
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Human_Agency_and_Oversight/Causal_responsibility.html">
       Causal Responsibility
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Transparency/Transparency.html">
     Transparency
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../Transparency/XAI_dimensions.html">
       Dimensions of Explanations
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
      <label for="toctree-checkbox-6">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../Transparency/blackbox_transparent.html">
         Black Box Explanation vs Explanation by Design
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Transparency/model_specific.html">
         Model-Specific vs Model-Agnostic Explainers
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Transparency/global_local.html">
         Global vs Local Explanations
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../Transparency/XAI.html">
       Explainable AI
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
      <label for="toctree-checkbox-7">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../Transparency/XAI_kinds.html">
         Kinds of Explanations
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="current reference internal" href="#">
     Technical Robustness and Safety
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
    <label for="toctree-checkbox-8">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="alignment.html">
       Alignment
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="robustness.html">
       Robustness
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="reliability.html">
       Reliability
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="evaluation.html">
       Evaluation
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="negative_side_effects.html">
       Negative side effects
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="distributional_shift.html">
       Distributional shift
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="security.html">
       Security
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="adversarial_attack.html">
       Adversarial Attack
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="data_poisoning.html">
       Data Poisoning
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="uncertainty.html">
       Uncertainty
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Diversity_Non-Discrimination_and_Fairness/Diversity_Non-Discrimination_and_Fairness.html">
     Diversity, Non-Discrimination, and Fairness
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
    <label for="toctree-checkbox-9">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Diversity_Non-Discrimination_and_Fairness/auditing.html">
       Auditing AI
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Diversity_Non-Discrimination_and_Fairness/bias.html">
       Bias
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Diversity_Non-Discrimination_and_Fairness/bias_factors.html">
       Bias Conducive Factors
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Diversity_Non-Discrimination_and_Fairness/bias_lmm.html">
       Bias and Fairness in LLMs
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Diversity_Non-Discrimination_and_Fairness/equity.html">
       Discrimination &amp; Equity
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Diversity_Non-Discrimination_and_Fairness/fairness.html">
       Fairness notions and metrics
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Diversity_Non-Discrimination_and_Fairness/fair_ML.html">
       Fair Machine Learning
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Diversity_Non-Discrimination_and_Fairness/discrimination.html">
       Grounds of Discrimination
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Diversity_Non-Discrimination_and_Fairness/intersectionality.html">
       Intersectionality
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Diversity_Non-Discrimination_and_Fairness/justice.html">
       Justice
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Diversity_Non-Discrimination_and_Fairness/segregation.html">
       Segregation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Accountability/Accountability_and_Reproducibility.html">
     Accountability
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
    <label for="toctree-checkbox-10">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../Accountability/L2.Accountability.html">
       Accountability
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
      <label for="toctree-checkbox-11">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../Accountability/L3.Wicked_problems.html">
         Wicked problems
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Accountability/L3.The_frame_problem.html">
         The Frame Problem
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Accountability/L3.Problem_of_many_hands.html">
         The Problem of Many Hands
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Accountability/L2.Reproducibility.html">
       Reproducibility
      </a>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../Accountability/L2.Traceability.html">
       Traceability
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
      <label for="toctree-checkbox-12">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../Accountability/L3.Provenance_tracking.html">
         Provenance Tracking
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Accountability/L3.Continuous_performance_monitoring.html">
         Continuous Performance Monitoring
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Privacy_and_Data_Governance/Privacy_and_Data_Governance.html">
     Privacy and Data Governance
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
    <label for="toctree-checkbox-13">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../Privacy_and_Data_Governance/L1.anonymization.html">
       Data Anonymization (and Pseudonymization)
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
      <label for="toctree-checkbox-14">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../Privacy_and_Data_Governance/L2.pseudonymization.html">
         Pseudonymization
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../Privacy_and_Data_Governance/L1.privacy_model.html">
       Privacy Models
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
      <label for="toctree-checkbox-15">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../Privacy_and_Data_Governance/L2.randomization.html">
         Randomization Methods
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Privacy_and_Data_Governance/L2.differential_privacy.html">
         Differential Privacy
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Privacy_and_Data_Governance/L2.indistinguishability.html">
         Anonymity by Indistinguishability
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Privacy_and_Data_Governance/L2.federated.html">
         Federated Learning
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../Privacy_and_Data_Governance/L1.attacks.html">
       Attacks on anonymization schemes
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
      <label for="toctree-checkbox-16">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../Privacy_and_Data_Governance/L2.reidentification.html">
         Re-identification Attack
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Societal_and_Environmental_Wellbeing/Societal_and_Environmental_Wellbeing.html">
     Societal and Environmental Wellbeing
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
    <label for="toctree-checkbox-17">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../Societal_and_Environmental_Wellbeing/sustenaible_AI.html">
       Sustainable AI
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
      <label for="toctree-checkbox-18">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../Societal_and_Environmental_Wellbeing/greenAI.html">
         Green AI
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Societal_and_Environmental_Wellbeing/cloud_computing.html">
         Cloud Computing
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Societal_and_Environmental_Wellbeing/edge_computing.html">
         Edge Computing
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Societal_and_Environmental_Wellbeing/data_centre.html">
         Data Centre
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Societal_and_Environmental_Wellbeing/cradle_to_cradle.html">
         Cradle-to-cradle Design
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Societal_and_Environmental_Wellbeing/resource_prediction.html">
         Resource Prediction
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Societal_and_Environmental_Wellbeing/resource_allocation.html">
         Resource Allocation
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../Societal_and_Environmental_Wellbeing/social_impact.html">
       Social Impact of AI Systems
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
      <label for="toctree-checkbox-19">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../Societal_and_Environmental_Wellbeing/human_interaction.html">
         AI human interaction
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Societal_and_Environmental_Wellbeing/workforce_impact.html">
         AI Impact on the Workforce
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../Societal_and_Environmental_Wellbeing/society_and_democracy.html">
       Society and Democracy
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
      <label for="toctree-checkbox-20">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../Societal_and_Environmental_Wellbeing/social_scoring.html">
         AI for social scoring
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../Societal_and_Environmental_Wellbeing/propaganda.html">
         AI for propaganda
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../TAILOR_project.html">
   The TAILOR project
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../authors.html">
   Complete List of Contributors
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../main/AnalyticalIndex.html">
   Index
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
  <label for="toctree-checkbox-21">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/2CC2.html">
     2CC2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Accountability/Accountability.html">
     Accountability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/propaganda.html">
     AI for propaganda
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/social_scoring.html">
     AI for social scoring
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/human_interaction.html">
     AI human interaction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/workforce_impact.html">
     AI Impact on the Workforce
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Alignment.html">
     Alignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Adversarial%20attack.html">
     Adversarial Attack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Adversarial%20example.html">
     Adversarial Example
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Adversarial%20input.html">
     Adversarial Input
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/Indistinguishability.html">
     Anonymity by Indistinguishability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Ante-hoc%20Explanation.html">
     Ante-hoc Explanation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Assessment.html">
     Assessment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/Attacks%20Anonym.html">
     Attacks on Anonymization Schema
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/Attacks%20on%20Pseudonymised%20Data.html">
     Attacks on Pseudonymised Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Diversity_Non-Discrimination_and_Fairness/Auditing.html">
     Auditing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Diversity_Non-Discrimination_and_Fairness/Bias.html">
     Bias
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Diversity_Non-Discrimination_and_Fairness/bias_factors.html">
     Bias Conducive Factors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Diversity_Non-Discrimination_and_Fairness/bias_lmm.html">
     Bias and Fairness in LLMs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Black-box%20Explanations.html">
     Black-box Explanation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Brittleness.html">
     Brittleness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/C2C.html">
     C2C
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Human_Agency_and_Oversight/Causal_responsibility.html">
     Causal Responsibility
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Cloud%20Computing.html">
     Cloud Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Accountability/Continuous%20monitoring.html">
     Continuous Performance Monitoring
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Counterexemplar.html">
     Counterexemplars
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Counterfactual.html">
     Counterfactuals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/cradle%202%20cradle.html">
     Cradle 2 cradle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Cradle.html">
     Cradle-to-cradle Design
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/Data%20Anonymization.html">
     Data Anonymization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Data%20Center.html">
     Data Center
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Data%20Poisoning.html">
     Data Poisoning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Dependability.html">
     Dependability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/Differential%20Privacy%20models.html">
     Differential Privacy Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/emotional_impact.html">
     Emotional Impact
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/epsilon_delta-differential_privacy.html">
     (
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
     ,
     <span class="math notranslate nohighlight">
      \(\delta\)
     </span>
     )-Differential Privacy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/Epsilon-differential_privacy.html">
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
     -Differential Privacy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/Epsilon-indist.html">
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
     -Indistinguishability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Data%20Shift.html">
     Distributional Shift
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Dimensions%20of%20Explanations.html">
     Dimensions of Explanations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Diversity_Non-Discrimination_and_Fairness/Discrimination.html">
     Grounds of Discrimination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Distributional%20Shift.html">
     Distributional Shift
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Direct.html">
     Direct Behaviour
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Edge%20Computing.html">
     Edge Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Energy%20Aware.html">
     Energy-aware Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Energy%20Efficient.html">
     Energy-efficient Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Evaluation.html">
     Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Diversity_Non-Discrimination_and_Fairness/Equity.html">
     Equity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Exemplars.html">
     Exemplars
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Explainable%20AI.html">
     Explainable AI
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Explanation%20by%20Design.html">
     Explanation by Design
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Diversity_Non-Discrimination_and_Fairness/Fair%20Machine%20Learning.html">
     Fair Machine Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Diversity_Non-Discrimination_and_Fairness/Fairness.html">
     Fairness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Features%20Importance.html">
     Feature Importance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/Federated.html">
     Federated Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Accountability/Frame.html">
     The Frame Problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Fog%20Computing.html">
     Fog Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Generalizable%20XAI.html">
     Model Agnostic
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Global%20Explanations.html">
     Global Explanations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Green%20AI.html">
     Green AI
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Green%20Computing.html">
     Green Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Green%20IT.html">
     Green IT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/ICT%20sustainability.html">
     ICT sustainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Intended.html">
     Intended Behaviour
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Diversity_Non-Discrimination_and_Fairness/intersectionality.html">
     Intersectionality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Diversity_Non-Discrimination_and_Fairness/Justice.html">
     Justice
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/K-Anonymity.html">
     K-anonymity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/L_diversity.html">
     l-diversity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/Linking%20Attack.html">
     Linking Attack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Local%20Explanations.html">
     Local Explanations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Lore.html">
     Local Rule-based Explanation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Human_Agency_and_Oversight/Meaningful_human_control.html">
     Meaningful Human Control
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Measurement.html">
     Measurement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Mesh%20Computing.html">
     Mesh Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Misdirect.html">
     Misdirect Behaviour
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Model-Agnostic.html">
     Model Agnostic
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Model-Specific.html">
     Model Specific
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Negative%20Side%20Effects.html">
     Negative Side Effects
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Not%20Generalizable%20XAI.html">
     Model Specific
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/Perturbation.html">
     Achiving Differential Privacy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Post-hoc%20Explanations.html">
     Post-hoc Explanation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Power%20Aware.html">
     Power-aware Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/Privacy%20model.html">
     Privacy models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Accountability/Problem_of_many_hands.html">
     Problem of Many Hands
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Prototypes.html">
     Prototypes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Accountability/Provenance.html">
     Provenance Tracking
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/Pseudonymised%20Data.html">
     Pseudonymization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/Randomization.html">
     Randomization Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/Re-identification%20Attack.html">
     Re-identification Attack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Regenerative%20Design.html">
     Regenerative Design
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Reliability.html">
     Reliability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Accountability/Repeatability.html">
     Repeatability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Accountability/Replicability.html">
     Replicability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Accountability/Reproducibility.html">
     Reproducibility
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Resource%20Allocation.html">
     Resource Allocation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Resource%20Prediction.html">
     Resource Prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Resource%20Scheduling.html">
     Resource Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Robustness.html">
     Robustness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Rules.html">
     Rules List and Rules Set
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Saliency%20Maps.html">
     Saliency Maps
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Security.html">
     Security
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Diversity_Non-Discrimination_and_Fairness/Segregation.html">
     Segregation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/self-identification.html">
     Self-identification of AI
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Single%20Tree%20Approximation.html">
     Single Tree Approxiamation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/social_impact.html">
     Social Impact of AI Systems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/society_and_democracy.html">
     Society and Democracy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Privacy_and_Data_Governance/T_closeness.html">
     t-closeness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Testing.html">
     Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Accountability/Traceability.html">
     Traceability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/Transparency.html">
     Transparency
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Technical_Robustness_and_Safety/Unintended.html">
     Unintended Behaviour
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Transparency/XAI.html">
     XAI
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Accountability/Wicked.html">
     Wicked Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Workload%20Forecast.html">
     Workload Forecast
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/Societal_and_Environmental_Wellbeing/Workload%20Prediction.html">
     Workload Prediction
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Attiva / disattiva la navigazione" aria-controls="site-navigation"
                title="Attiva / disattiva la navigazione" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Scarica questa pagina"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Technical_Robustness_and_Safety/Technical_Robustness_and_Safety.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Scarica il file sorgente" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Stampa in PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/prafra/jupyter-book-TAILOR-D3.2"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Repository di origine"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/prafra/jupyter-book-TAILOR-D3.2/issues/new?title=Issue%20on%20page%20%2FTechnical_Robustness_and_Safety/Technical_Robustness_and_Safety.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Apri un problema"><i class="fas fa-lightbulb"></i>questione aperta</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Modalità schermo intero"
        title="Modalità schermo intero"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contenuti
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#in-brief">
   In Brief
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#abstract">
   Abstract
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation-and-background">
   Motivation and Background
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#guidelines">
   Guidelines
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#main-keywords">
   Main Keywords
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recommended-reading">
   Recommended reading
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliography">
   Bibliography
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="technical-robustness-and-safety">
<h1>Technical Robustness and Safety<a class="headerlink" href="#technical-robustness-and-safety" title="Permalink to this headline">¶</a></h1>
<div class="section" id="in-brief">
<h2>In Brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">¶</a></h2>
<p><strong>Safety and Robustness</strong>: The safety of an AI system refers to the
extent the system meets its intended functionality without producing any
physical or psychological harm, especially to human beings, and by
extension to other material or immaterial elements that may be valuable
for humans, including the system itself. Safety must also cover the way
and conditions in which the system ceases its operation, and the
consequences of stopping. The term robustness emphasises that safety and
—conditionally to it— functionality, must be preserved under harsh
conditions, including unanticipated errors, exceptional situations,
unintended or intended damage, manipulation or catastrophic states.</p>
</div>
<div class="section" id="abstract">
<h2>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">¶</a></h2>
<!-- bold terms in this section were <span style="color: darkblue"> -->
<p>In this part we will cover the main elements that define the safety and robustness of AI systems. Some of them are common to system safety in general, to software-hardware computer systems or to critical systems engineering, such as <strong>software bugs</strong>. Some others are magnified in artificial intelligence, such as <strong>denial of service</strong>, a robustness issue that can appear by inducing an AI system to unrecoverable states or by generating inputs that collapse the system due to high computational demands. Some other issues are more specific to AI systems, such as <strong>reward hacking</strong>. These new issues appear more clearly in those systems that are specified in non-programmatic or non-explicit ways (e.g., through a utility function to be optimised, through examples, rewards or other implicit ways), as exemplified by systems that operate with solvers or machine learning models. We will pay more attention to these more AI-specific issues because they are less covered in the traditional literature about safety in computer systems. They are also more challenging because of their cognitive character, the ambiguities of human intent, several ethical issues and the relevance of long-term risks. This character and the fast development of the field has also blurred some distinctions between safety (threats without malicious intent) and <a class="reference internal" href="security.html"><span class="doc">Security</span></a> (intentional threats), especially in now popular research areas such as <a class="reference internal" href="adversarial_attack.html"><span class="doc">Adversarial Attack</span></a> and <a class="reference internal" href="data_poisoning.html"><span class="doc">Data Poisoning</span></a>, and also within <a class="reference internal" href="../Privacy_and_Data_Governance/Privacy_and_Data_Governance.html"><span class="doc std std-doc">data privacy</span></a> (e.g., <strong>information leakage</strong> by querying machine learning models or other <strong>side channel attacks</strong>). In the end, protecting the environment from the system (safety) also requires protecting the system from the environment (<a class="reference internal" href="security.html"><span class="doc">Security</span></a>). Taking into account the changing character of the field, we include a taxonomic organisation of terms in the area of AI safety and robustness and their definition.</p>
</div>
<div class="section" id="motivation-and-background">
<h2>Motivation and Background<a class="headerlink" href="#motivation-and-background" title="Permalink to this headline">¶</a></h2>
<p>Given the increasing capabilities and widespread use of artificial
intelligence, there is a growing concern about its risks, as humans are
progressively replaced or sidelined from the decision loop of
intelligent machines. The technical foundations and assumptions on which
traditional safety engineering principles are based are inadequate for
systems in which AI algorithms, and in particular Machine Learning (ML)
algorithms, are interacting with people and the environment at
increasingly higher levels of autonomy. There have been regulatory
efforts to limit the use of AI systems in safety-critical or hostile
environments, such as health, defense, energy, etc.
<span id="id1">[<a class="reference internal" href="#id17">1</a>, <a class="reference internal" href="#id18">2</a>]</span>, but the consequences can
also be devastating in areas that were not considered high risk, just by
the scaling numbers or domino effects of AI systems. On top of the
numerous safety challenges posed by present-day AI systems, a
forward-looking analysis on more capable future AI systems raises more
systemic concerns, such as highly disruptive scenarios in the workplace,
the effect on human cognition in the long term and even existential
risks.</p>
</div>
<div class="section" id="guidelines">
<h2>Guidelines<a class="headerlink" href="#guidelines" title="Permalink to this headline">¶</a></h2>
<p>Actions to ensure safety and robustness of AI systems need to take a
holistic perspective, encompassing all the elements and stages
associated with the conception, design, implementation and maintenance
of these systems. We organise<!--[^1]--> the field of AI safety and robustness
into seven groups, following similar categorisations<!--[^landscape]-->:</p>
<ul class="simple">
<li><p><strong>AI Safety Foundations</strong>: This category covers a number of foundational
concepts, characteristics and problems related to AI safety that
need special consideration from a theoretical perspective. This
includes concepts such as uncertainty, generality or value
alignment, as well as characteristics such autonomy levels, safety
criticality, types of human-machine and environment-machine
interaction. This group intends to collect any cross-category
concerns in AI Safety and Robustness.</p></li>
<li><p><strong>Specification and Modelling</strong>: The main scope of this category is on
how to describe needs, designs and actual operating AI systems from
different perspectives (technical concerns) and abstraction levels.
This includes the specification and modelling of risk management
properties (e.g., hazards, failures modes, mitigation measures), as
well as safety-related requirements, training, behaviour or quality
attributes in AI-based systems.</p></li>
<li><p><strong>Verification and Validation</strong>: This category concerns design and
implementation-time approaches to ensure that an AI-based system
meets its requirements (verification) and behaves as expected
(validation). The range of techniques covers any
formal/mathematical, model-based simulation or testing approach that
provides evidence that an AI-based system satisfies its defined
(safety) requirements and does not deviate from its intended
behaviour and causes unintended consequences, even in extreme and
unanticipated situations (robustness).</p></li>
<li><p><strong>Runtime Monitoring and Enforcement</strong>: The increasing autonomy and
learning nature of AI-based systems is particularly challenging for
their verification and validation (V&amp;V), due to our inability to
collect an epistemologically sufficient quantity of evidence to
ensure correctness. Runtime monitoring is useful to cover the gaps
of design-time V&amp;V by observing the internal states of a given
system and its interactions with external entities, with the aim of
determining system behaviour correctness or predicting potential
risks. Enforcement deals with runtime mechanisms to self-adapt,
optimise or reconfigure system behaviour with the aim of supporting
fallback to a safe system state from the (anomalous) current state.</p></li>
<li><p><strong>Human-Machine Interaction</strong>: As autonomy progressively substitutes
cognitive human tasks, some kind of human-machine interaction issues
become more critical, such as the loss of situational awareness or
overconfidence. Other issues include: collaborative missions that
need unambiguous communication to manage self-initiative to start or
transfer tasks; safety-critical situations in which earning and
maintaining trust is essential at operational phases; or cooperative
human-machine decision tasks where understanding machine decisions
are crucial to validate safe autonomous actions.</p></li>
<li><p><strong>Process Assurance and Certification</strong>: Process Assurance is the
planned and systematic activities that assure system lifecycle
processes conform to its requirements (including safety) and quality
procedures. In our context, it covers the management of the
different phases of AI-based systems, including training and
operational phases, the traceability of data and artefacts, and
people. Certification implies a (legal) recognition that a system or
process complies with industry standards and regulations to ensure
it delivers its intended functions safely. Certification is
challenged by the inscrutability of AI-based systems and the
inability to ensure functional safety under uncertain and
exceptional situations prior to its operation.</p></li>
<li><p><strong>Safety-related Ethics, Security and Privacy</strong>: While these are quite
large fields, we are interested in their intersection and
dependencies with safety and robustness. Ethics becomes increasingly
important as autonomy (with learning and adaptive abilities)
involves the transfer of safety risks, responsibility, and
liability, among others. AI-specific security and privacy issues
must be considered with regard to its impact on safety and
robustness. For example, malicious adversarial attacks can be
studied with focus on situations that compromise systems towards a
dangerous situation.</p></li>
</ul>
<p>Fig. <a class="reference internal" href="#t3-2taxonomy32"><span class="std std-numref">20</span></a> reflects the seven categories described above.
Many of the terms and concepts we will expand on correspond to one or
more of these categories.</p>
<div class="figure align-center" id="t3-2taxonomy32">
<a class="reference internal image-reference" href="../_images/taxonomy.jpg"><img alt="../_images/taxonomy.jpg" src="../_images/taxonomy.jpg" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 20 </span><span class="caption-text">Taxonomy of AI Safety. Taken from <span id="id2">[<a class="reference internal" href="#id22">3</a>]</span>-</span><a class="headerlink" href="#t3-2taxonomy32" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="main-keywords">
<h2>Main Keywords<a class="headerlink" href="#main-keywords" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="alignment.html"><span class="doc">Alignment</span></a>: The goal of AI <strong>alignment</strong> is to ensure that AI systems are aligned with human intentions and values. This first requires determining the normative question of what values or principles we have and what humans really want, collectively or individually, and second, the technical question of how to imbue AI systems with these values and goals..</p></li>
<li><p><a class="reference internal" href="robustness.html"><span class="doc">Robustness</span></a>: <strong>Robustness</strong> is the degree in which an AI system functions1 reliably and accurately under harsh conditions. These conditions may include adversarial intervention, implementer error, or skewed goal-execution by an automated learner (in reinforcement learning applications). The measure of robustness is therefore the strength of a system’s integrity and the soundness of its operation in response to difficult conditions, adversarial attacks, perturbations, data poisoning, and undesirable reinforcement learning behaviour.</p></li>
<li><p><a class="reference internal" href="reliability.html"><span class="doc">Reliability</span></a>: The objective of reliability is that an AI system behaves exactly as its designers intended and anticipated, over time. A reliable system adheres to the specifications it was programmed to carry out at any time. Reliability is therefore a measure of consistency of operation and can establish confidence in the safety of a system based upon the dependability with which it operationally conforms to its intended functionality.</p></li>
<li><p><a class="reference internal" href="evaluation.html"><span class="doc">Evaluation</span></a>: <strong>AI measurement</strong> is any activity that estimates attributes as <em>measures</em>— of an AI system or some of its components, abstractly or in particular contexts of operation. These attributes, if well estimated, can be used to explain and predict the <em>behaviour</em> of the system. This can stem from an engineering perspective, trying to understand whether a particular AI system meets the specifications or the intention of their designers, known respectively as <strong>verification</strong> and <strong>validation</strong>. Under this perspective, AI measurement is close to computer systems <strong>testing</strong> (hardware and/or software) and other evaluation procedures in engineering. However, in AI there is an extremely complex <em>adaptive</em> behaviour, and in many cases, with a lack of a written and operational specification. What the systems has to do depends on some constraints and utility functions that have to be optimised, is specified by example (from which the system has to learn a model) or ultimately depends on feedback from the user or the environment (e.g., in the form of rewards).</p></li>
<li><p><a class="reference internal" href="negative_side_effects.html"><span class="doc">Negative side effects</span></a>: <strong>Negative side effects</strong> are an important safety issue in AI system that considers all possible unintended harm that is caused as a secondary effect of the AI system’s operation. An agent can disrupt or break other systems around, or damage third parties, including humans, or can exhaust resources, or a combination of all this. This usually happens because many things the system should <em>not</em> do are not included in its specification. In the case of AI systems, this is even more poignant as written specifications are usually replaced by an optimisation or loss function, in which it is even more difficult to express these things the system should not do, as they frequently rely on ‘common sense’.</p></li>
<li><p><a class="reference internal" href="distributional_shift.html"><span class="doc">Distributional shift</span></a>: Once trained, most machine learning systems operate on static models of the world that have been built from historical data which have become fixed in the systems’ parameters. This freezing of the model before it is released ‘into the wild’ makes its accuracy and reliability especially vulnerable to changes in the underlying distribution of data. When the historical data that have crystallised into the trained model’s architecture cease to reflect the population concerned, the model’s mapping function will no longer be able to accurately and reliably transform its inputs into its target output values. These systems can quickly become prone to error in unexpected and harmful ways. In all cases, the system and the operators must remain vigilant to the potentially rapid concept drifts that may occur in the complex, dynamic, and evolving environments in which your AI project will intervene. Remaining aware of these transformations in the data is crucial for safe AI.</p></li>
<li><p><a class="reference internal" href="security.html"><span class="doc">Security</span></a>: The goal of <strong>security</strong> encompasses the protection of several operational dimensions of an AI system when confronted with possible attacks, trying to take control of the system or having access to design, operational or personal information. A secure system is capable of maintaining the integrity of the information that constitutes it. This includes protecting its architecture from the unauthorised modification or damage of any of its component parts. A secure system also keeps confidential and private information protected even under hostile or adversarial conditions.</p></li>
<li><p><a class="reference internal" href="adversarial_attack.html"><span class="doc">Adversarial Attack</span></a>: An <strong>adversarial</strong> input is any perturbation of the input features or observations of a system (sometimes imperceptible to both humans and the own system) that makes the system fail or take the system to a dangerous state. A prototypical case of an adversarial situation happens with machine learning models, when an external agent maliciously modify input data –often in imperceptible ways– to induce them into misclassification or incorrect prediction. For instance, by undetectably altering a few pixels on a picture, an adversarial attacker can mislead a model into generating an incorrect output (like identifying a panda as a gibbon or a ‘stop’ sign as a ‘speed limit’ sign) with an extremely high confidence. While a good amount of attention has been paid to the risks that adversarial attacks pose in deep learning applications like computer vision, these kinds of perturbations are also effective across a vast range of machine learning techniques and uses such as spam filtering and malware detection. A different but related type of adversarial attack is called <a class="reference internal" href="data_poisoning.html"><span class="doc">Data Poisoning</span></a>, but this involves a malicious compromise of data sources (used for training or testing) at the point of collection and pre-processing.</p></li>
<li><p><a class="reference internal" href="data_poisoning.html"><span class="doc">Data Poisoning</span></a>: <strong>Data poisoning</strong> occurs when an adversary modifies or manipulates part of the dataset upon which a model will be trained, validated, or tested. By altering a selected subset of training inputs, a poisoning attack can induce a trained AI system into curated misclassification, systemic malfunction, and poor performance. An especially concerning dimension of targeted data poisoning is that an adversary may introduce a ‘backdoor’ into the infected model whereby the trained system functions normally until it processes maliciously selected inputs that trigger error or failure. Data poisoning is possible because data collection and procurement often involves potentially unreliable or questionable sources. When data originates in uncontrollable environments like the internet, social media, or the Internet of Things, many opportunities present themselves to ill-intentioned attackers, who aim to manipulate training examples. Likewise, in third-party data curation processes (such as ‘crowdsourced’ labelling, annotation, and content identification), attackers may simply handcraft malicious inputs.</p></li>
</ul>
</div>
<div class="section" id="recommended-reading">
<h2>Recommended reading<a class="headerlink" href="#recommended-reading" title="Permalink to this headline">¶</a></h2>
<p>Some introductory sources for AI Safety and Robustnes are <span id="id3">[<a class="reference internal" href="#id22">3</a>, <a class="reference internal" href="security.html#id167">1</a>, <a class="reference internal" href="alignment.html#id160">2</a>, <a class="reference internal" href="negative_side_effects.html#id223">1</a>, <a class="reference internal" href="#id19">7</a>]</span>.</p>
</div>
<div class="section" id="bibliography">
<h2>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h2>
<p id="id4"><dl class="citation">
<dt class="label" id="id17"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Luciano Floridi. The european legislation on ai: a brief analysis of its philosophical approach. <em>Philosophy &amp; Technology</em>, 34(2):215–222, 2021.</p>
</dd>
<dt class="label" id="id18"><span class="brackets"><a class="fn-backref" href="#id1">2</a></span></dt>
<dd><p>Michael Veale and Frederik Zuiderveen Borgesius. Demystifying the draft eu artificial intelligence act—analysing the good, the bad, and the unclear elements of the proposed approach. <em>Computer Law Review International</em>, 22(4):97–112, 2021.</p>
</dd>
<dt class="label" id="id22"><span class="brackets">3</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id3">2</a>)</span></dt>
<dd><p>Huáscar Espinoza, Han Yu, Xiaowei Huang, Freddy Lecue, José Hernández-Orallo, Seán Ó hÉigeartaigh, and Richard Mallah. Towards an AI safety landscape: an overview. 2019. URL: <a class="reference external" href="https://www.ai-safety.org/">https://www.ai-safety.org/</a>.</p>
</dd>
<dt class="label" id="id20"><span class="brackets"><a class="fn-backref" href="#id3">4</a></span></dt>
<dd><p>Leslie David. Understanding artificial intelligence ethics and safety. <em>The Alan Turing Institute</em>, 2019. URL: <a class="reference external" href="https://doi.org/10.5281/zenodo.3240529">https://doi.org/10.5281/zenodo.3240529</a>.</p>
</dd>
<dt class="label" id="id21"><span class="brackets"><a class="fn-backref" href="#id3">5</a></span></dt>
<dd><p>Iason Gabriel. Artificial intelligence, values, and alignment. <em>Minds and machines</em>, 30(3):411–437, 2020.</p>
</dd>
<dt class="label" id="id79"><span class="brackets"><a class="fn-backref" href="#id3">6</a></span></dt>
<dd><p>Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in AI safety. 2016.</p>
</dd>
<dt class="label" id="id19"><span class="brackets"><a class="fn-backref" href="#id3">7</a></span></dt>
<dd><p>Stuart Russell, Daniel Dewey, and Max Tegmark. Research priorities for robust and beneficial artificial intelligence. <em>Ai Magazine</em>, 36(4):105–114, 2015.</p>
</dd>
</dl>
</p>
<blockquote>
<div><p>This entry was readapted from <em>Huáscar Espinoza, Han Yu, Xiaowei Huang, Freddy Lecue, José Hernández-Orallo, Seán Ó hÉigeartaigh, and Richard Mallah. Towards an AI safety landscape: an overview. Artificial Intelligence Safety 2019, <a class="reference external" href="https://www.ai-safety.org/">https://www.ai-safety.org/</a>.</em> by Jose Hernandez-Orallo, Fernando Martinez-Plumed, Santiago Escobar, and Pablo A. M. Casares.</p>
</div></blockquote>
<!-- http://ceur-ws.org/Vol-2419/ -->
<!--[^1]: Most of this section is taken from {cite}`espinoza2019`.-->
<!--
[^landscape]: FLI's Landscape of AI Safety and Beneficence Research for research
    contextualization and in preparation for brainstorming at the
    Beneficial AI 2017 conference
    (<https://futureoflife.org/landscape/ResearchLandscapeExtended.pdf>),
    the Assuring Autonomy International Programme (AAIP) to develop a
    Body of Knowledge (BoK) intended, in time, to become a reference
    source on assurance and regulation of Robotics and Autonomous
    Systems (RAS),
    (<https://www.york.ac.uk/assuring-autonomy/research/body-of-knowledge/>)
    and Ortega et al (DeepMind) structure of the technical AI safety
    field
    (<https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1>).
-->
</div>
<div class="toctree-wrapper compound">
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Technical_Robustness_and_Safety"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../Transparency/single_tree.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Single Tree Approximation</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="alignment.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Alignment</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          Di TAILOR WP3 members; see <a href="/handbookTAI/authors.html" target="_blank">here</a> for the complete list of contributors. This research was partially supported by TAILOR, a project funded by EU Horizon 2020 research and innovation programme under GA No 952215<br/>
        
            &copy; Diritto d'autore 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>