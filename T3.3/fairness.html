
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Fairness notions and metrics &#8212; TAILOR – D3.3 – Handbook on Trustworthy AI</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tailor.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://prafra.github.io/jupyter-book-TAILOR-D3.2/T3.3/fairness.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Fair Machine Learning" href="fair_ML.html" />
    <link rel="prev" title="Discrimination &amp; Equity" href="equity.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/TAILOR-logo-coloured.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">TAILOR – D3.3 – Handbook on Trustworthy AI</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../TAILOR.html">
                    The TAILOR Handbook of Trustworthy AI
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../authors.html">
   Complete List of Contributors
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../T3.1/T3.1.html">
   Explainable AI Systems
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../T3.1/XAI_kinds.html">
     Kinds of Explanations
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../T3.1/feature_importance.html">
       Feature Importance
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../T3.1/saliency_maps.html">
       Saliency Maps
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../T3.1/single_tree.html">
       Single Tree Approximation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../T3.1/XAI_dimensions.html">
     Dimensions of Explanations
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../T3.1/blackbox_transparent.html">
       Black Box Explanation vs Explanation by Design
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../T3.1/model_specific.html">
       Model-Specific vs Model-Agnostic Explainers
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../T3.1/global_local.html">
       Global vs Local Explanations
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../T3.2/T3.2.html">
   Safety and Robustness
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../T3.2/alignment.html">
     Alignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../T3.2/robustness.html">
     Robustness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../T3.2/reliability.html">
     Reliability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../T3.2/evaluation.html">
     Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../T3.2/negative_side_effects.html">
     Negative side effects
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../T3.2/distributional_shift.html">
     Distributional shift
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../T3.2/security.html">
     Security
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../T3.2/adversarial_attack.html">
     Adversarial Attack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../T3.2/data_poisoning.html">
     Data Poisoning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="T3.3.html">
   Fairness, Equity, and Justice by Design
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="auditing.html">
     Auditing AI
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bias.html">
     Bias
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="equity.html">
     Discrimination &amp; Equity
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Fairness notions and metrics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fair_ML.html">
     Fair Machine Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="discrimination.html">
     Grounds of Discrimination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="justice.html">
     Justice
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="segregation.html">
     Segregation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../T3.4/L1.Accountability_and_Reproducibility.html">
   Accountability and Reproducibility
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../T3.4/L2.Accountability.html">
     Accountability
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../T3.4/L3.Wicked_problems.html">
       Wicked problems
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../T3.4/L3.Meaningful_human_control.html">
       Meaningful human control
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../T3.4/L3.The_frame_problem.html">
       The Frame Problem
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../T3.4/L2.Reproducibility.html">
     Reproducibility
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../T3.4/L2.Traceability.html">
     Traceability
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
    <label for="toctree-checkbox-8">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../T3.4/L3.Provenance_tracking.html">
       Provenance Tracking
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../T3.4/L3.Continuous_performance_monitoring.html">
       Continuous Performance Monitoring
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../T3.5/T3.5.html">
   Respect for Privacy
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../T3.5/L1.anonymization.html">
     Data Anonymization (and Pseudonymization)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
    <label for="toctree-checkbox-10">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../T3.5/L2.pseudonymization.html">
       Pseudonymization
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../T3.5/L1.privacy_model.html">
     Privacy Models
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../T3.5/L2.differential_privacy.html">
       Differential Privacy
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
      <label for="toctree-checkbox-12">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../T3.5/L3.epsilon_DP.html">
         <span class="math notranslate nohighlight">
          \(\epsilon\)
         </span>
         -Differential Privacy
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../T3.5/L3.epsilon_delta_DP.html">
         (
         <span class="math notranslate nohighlight">
          \(\epsilon\)
         </span>
         ,
         <span class="math notranslate nohighlight">
          \(\delta\)
         </span>
         )-Differential Privacy
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../T3.5/L2.perturbation_mechanisms.html">
         Achieving Differential Privacy
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../T3.5/L2.k_anonymity.html">
       k-anonymity
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../T3.5/L1.attacks.html">
     Attacks on anonymization schemes
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
    <label for="toctree-checkbox-13">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../T3.5/L2.reidentification.html">
       Re-identification Attack
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../T3.6/T3.6.html">
   Sustainability
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../T3.6/greenAI.html">
     Green AI
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
    <label for="toctree-checkbox-15">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../T3.6/power_aware.html">
       Power-aware Computing
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../T3.6/cloud_computing.html">
     Cloud Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../T3.6/edge_computing.html">
     Edge Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../T3.6/data_centre.html">
     Data Centre
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../T3.6/cradle_to_cradle.html">
     Cradle-to-cradle Design
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../T3.6/resource_prediction.html">
     Resource Prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../T3.6/resource_allocation.html">
     Resource Allocation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../TAILOR_project.html">
   About TAILOR
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../main/AnalyticaIndex.html">
   Index
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/2CC2.html">
     2CC2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.4/Accountability.html">
     Accountability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Alignment.html">
     Alignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Adversarial%20attack.html">
     Adversarial Attack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Adversarial%20example.html">
     Adversarial Example
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Adversarial%20input.html">
     Adversarial Input
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.1/Ante-hoc%20Explanation.html">
     Ante-hoc Explanation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Assessment.html">
     Assessment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.5/Attacks%20Anonym.html">
     Attacks on Anonymization Schema
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.5/Attacks%20on%20Pseudonymised%20Data.html">
     Attacks on Pseudonymised Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.3/Auditing.html">
     Auditing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.3/Bias.html">
     Bias
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.1/Black-box%20Explanations.html">
     Black-box Explanation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Brittleness.html">
     Brittleness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/C2C.html">
     C2C
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Cloud%20Computing.html">
     Cloud Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.4/Continuous%20monitoring.html">
     Continuous Performance Monitoring
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/cradle%202%20cradle.html">
     Cradle 2 cradle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Cradle.html">
     Cradle-to-cradle Design
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.5/Data%20Anonymization.html">
     Data Anonymization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Data%20Center.html">
     Data Center
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Data%20Poisoning.html">
     Data Poisoning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Dependability.html">
     Dependability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.5/Differential%20Privacy%20models.html">
     Differential Privacy Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.5/epsilon_delta-differential_privacy.html">
     (
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
     ,
     <span class="math notranslate nohighlight">
      \(\delta\)
     </span>
     )-Differential Privacy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.5/Epsilon-differential_privacy.html">
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
     -Differential Privacy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.5/Epsilon-indist.html">
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
     -Indistinguishability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Data%20Shift.html">
     Distributional Shift
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.1/Dimensions%20of%20Explanations.html">
     Dimensions of Explanations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.3/Discrimination.html">
     Grounds of Discrimination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Distributional%20Shift.html">
     Distributional Shift
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Direct.html">
     Direct Behaviour
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Edge%20Computing.html">
     Edge Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Energy%20Aware.html">
     Energy-aware Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Energy%20Efficient.html">
     Energy-efficient Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Evaluation.html">
     Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.3/Equity.html">
     Equity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.1/Explanation%20by%20Design.html">
     Explanation by Design
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.3/Fair%20Machine%20Learning.html">
     Fair Machine Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.3/Fairness.html">
     Fairness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.1/Features%20Importance.html">
     Feature Importance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.4/Frame.html">
     The Frame Problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Fog%20Computing.html">
     Fog Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.1/Generalizable%20XAI.html">
     Model Agnostic
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.1/Global%20Explanations.html">
     Global Explanations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Green%20AI.html">
     Green AI
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Green%20Computing.html">
     Green Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Green%20IT.html">
     Green IT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/ICT%20sustainability.html">
     ICT sustainability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Intended.html">
     Intended Behaviour
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.3/Justice.html">
     Justice
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.5/K-Anonymity.html">
     K-anonymity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.5/Linking%20Attack.html">
     Linking Attack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.1/Local%20Explanations.html">
     Local Explanations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.4/Meaningful%20human%20control.html">
     Meaningful Human Control
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Measurement.html">
     Measurement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Mesh%20Computing.html">
     Mesh Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Misdirect.html">
     Misdirect Behaviour
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.1/Model-Agnostic.html">
     Model Agnostic
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.1/Model-Specific.html">
     Model Specific
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Negative%20Side%20Effects.html">
     Negative Side Effects
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.1/Not%20Generalizable%20XAI.html">
     Model Specific
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.5/Perturbation.html">
     Achiving Differential Privacy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.1/Post-hoc%20Explanations.html">
     Post-hoc Explanation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Power%20Aware.html">
     Power-aware Computing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.5/Privacy%20model.html">
     Privacy models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.4/Provenance.html">
     Provenance Tracking
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.5/Pseudonymised%20Data.html">
     Pseudonymization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.5/Re-identification%20Attack.html">
     Re-identification Attack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Regenerative%20Design.html">
     Regenerative Design
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Reliability.html">
     Reliability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.4/Repeatability.html">
     Repeatability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.4/Replicability.html">
     Replicability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.4/Reproducibility.html">
     Reproducibility
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Resource%20Allocation.html">
     Resource Allocation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Resource%20Prediction.html">
     Resource Prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Resource%20Scheduling.html">
     Resource Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Robustness.html">
     Robustness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.1/Saliency%20Maps.html">
     Saliency Maps
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Security.html">
     Security
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.3/Segregation.html">
     Segregation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.1/Single%20Tree%20Approximation.html">
     Single Tree Approxiamation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Testing.html">
     Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.4/Traceability.html">
     Traceability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.1/Transparency.html">
     Transparency
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.2/Unintended.html">
     Unintended Behaviour
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.4/Wicked.html">
     Wicked Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Workload%20Forecast.html">
     Workload Forecast
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../index/T3.6/Workload%20Prediction.html">
     Workload Prediction
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/prafra/jupyter-book-TAILOR-D3.2"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/prafra/jupyter-book-TAILOR-D3.2/issues/new?title=Issue%20on%20page%20%2FT3.3/fairness.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/T3.3/fairness.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#in-brief">
   In brief
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more-in-detail">
   More in Detail
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliography">
   Bibliography
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Fairness notions and metrics</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#in-brief">
   In brief
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more-in-detail">
   More in Detail
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliography">
   Bibliography
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="fairness-notions-and-metrics">
<h1>Fairness notions and metrics<a class="headerlink" href="#fairness-notions-and-metrics" title="Permalink to this headline">#</a></h1>
<section id="in-brief">
<h2>In brief<a class="headerlink" href="#in-brief" title="Permalink to this headline">#</a></h2>
<p>The term <strong>fairness</strong> is defined as the quality or state of being fair; or a lack of favoritism towards one side. The notions of fairness, and quantitative measures of
them (fairness metrics), can be distinguished based on the focus on individuals, groups and sub-groups.</p>
</section>
<section id="more-in-detail">
<h2>More in Detail<a class="headerlink" href="#more-in-detail" title="Permalink to this headline">#</a></h2>
<p>The term fairness is defined as the quality or state of being fair; or a
lack of favoritism towards one side. However, like <a class="reference internal" href="bias.html"><span class="doc">Bias</span></a>, fairness can
mean different concepts to different peoples, different contexts, and
different disciplines. The definition of fairness in various disciplines
is detailed in <span id="id1">[<a class="reference internal" href="#id2508" title="Deirdre K Mulligan, Joshua A Kroll, Nitin Kohli, and Richmond Y Wong. This thing called fairness: disciplinary confusion realizing a value in technology. Proceedings of the ACM on Human-Computer Interaction, 3(CSCW):1–36, 2019.">1</a>]</span>. An unfair model produces results
that are skewed towards particular individuals or groups. The primary
sources of this unfairness are the presence of biases. There are two
important categories of biases which play crucial role in fairness; (i)
technical bias and (ii) social bias. Technical biases can be traced back
to the sources , but social biases are very difficult to fix as these
are a matter of politics, perspectives, and shifts in prejudices and
preconceptions that can take years to change <span id="id2">[<a class="reference internal" href="#id2572" title="Sandra Wachter, Brent Mittelstadt, and Chris Russell. Bias preservation in machine learning: the legality of fairness metrics under eu non-discrimination law. W. Va. L. Rev., 123:735, 2020.">3</a>]</span>. Most of
the state-of-the-art techniques tackle technical errors, but it cannot
resolve the root causes of bias. Based on this observation, Sandra et al.
<span id="id3">[<a class="reference internal" href="#id2572" title="Sandra Wachter, Brent Mittelstadt, and Chris Russell. Bias preservation in machine learning: the legality of fairness metrics under eu non-discrimination law. W. Va. L. Rev., 123:735, 2020.">3</a>]</span> proposed three responses concerning algorithmic bias
and resulting social inequality. The first is not an active choice as it
allows the system to get worse and do nothing to fix biases. Second,
incorporate techniques to fix technical errors and maintain a status quo
to ensure that the system do not make it worse. Much works in fairness
focused on this option, called ‘bias preserving fairness’, maintains a
status quo as a baseline, aligns with the formal equality of EU
non-discrimination law. Finally, ‘bias transforming fairness’, the third
response focuses on the substantive equality of EU non-discrimination
which can only be achieved by accounting for historical (social)
inequalities. As argued in <span id="id4">[<a class="reference internal" href="#id2572" title="Sandra Wachter, Brent Mittelstadt, and Chris Russell. Bias preservation in machine learning: the legality of fairness metrics under eu non-discrimination law. W. Va. L. Rev., 123:735, 2020.">3</a>]</span>, users
(developers,deployers etc.) should give preference to ‘bias
transforming’ fairness metrics, when a fairness metric is used to make
substantive decisions about people in contexts where significant
disparity has been previously observed.</p>
<p>The notions of fairness fall under individuals, groups and sub-groups.
Individual fairness ensures that similar individuals should be treated
similarly. It accounts for the distance measures to evaluate the
similarity of individuals <span id="id5">[<a class="reference internal" href="#id2509" title="Philips George John, Deepak Vijaykeerthy, and Diptikalyan Saha. Verifying individual fairness in machine learning models. In UAI, volume 124 of Proceedings of Machine Learning Research, 749–758. AUAI Press, 2020.">4</a>, <a class="reference internal" href="#id2510" title="Asia J. Biega, Krishna P. Gummadi, and Gerhard Weikum. Equity of attention: amortizing individual fairness in rankings. In SIGIR, 405–414. ACM, 2018.">5</a>]</span>. On
the other hand, group fairness compares quantities at the group level
primarily identified by protective features such as gender, ethnicity
etc. etc. <span id="id6">[<a class="reference internal" href="#id2514" title="Yu Cheng, Zhihao Jiang, Kamesh Munagala, and Kangning Wang. Group fairness in committee selection. ACM Transactions on Economics and Computation (TEAC), 8(4):1–18, 2020.">6</a>, <a class="reference internal" href="#id2515" title="Vincent Conitzer, Rupert Freeman, Nisarg Shah, and Jennifer Wortman Vaughan. Group fairness for the allocation of indivisible goods. In AAAI, 1853–1860. AAAI Press, 2019.">7</a>]</span>. Sub-group fairness is
more rigid than group fairness as this ensures fairness concerning one
or more structured sub-groups defined by sensitive features,
interpolates between individual and group fairness notions
<span id="id7">[<a class="reference internal" href="#id2562" title="Michael J. Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: auditing and learning for subgroup fairness. In ICML, volume 80 of Proceedings of Machine Learning Research, 2569–2577. PMLR, 2018.">8</a>]</span>. According to <span id="id8">[<a class="reference internal" href="#id2575" title="Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In ITCS, volume 67 of LIPIcs, 43:1–43:23. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2017.">9</a>]</span>, it is impossible
to satisfy all of the above notions, leading to conflicts between
fairness definitions. Therefore, one suggestion could be to select
appropriate fairness criteria and use those based on the application and
deployment. Another concern has risen in <span id="id9">[<a class="reference internal" href="#id2516" title="Lydia T. Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machine learning. In IJCAI, 6196–6200. ijcai.org, 2019.">10</a>]</span>, temporal
aspects of fairness notions may harm the sensitive groups over time if
not updated.</p>
<p><strong>Some widely used fairness metrics:</strong> In order to recall some widely
used fairness metrics we need to introduce some notation. Let <span class="math notranslate nohighlight">\(V\)</span>, <span class="math notranslate nohighlight">\(A\)</span>,
and <span class="math notranslate nohighlight">\(X\)</span> be three random variables representing, respectively, the total
set of features, the sensitive features, and the remaining features
describing an individual such that <span class="math notranslate nohighlight">\(V=(X,A)\)</span> and <span class="math notranslate nohighlight">\(P(V=v_i)\)</span> represents
the probability of drawing an individual with a vector of values <span class="math notranslate nohighlight">\(v_i\)</span>
from the population. For simplicity, we focus on the case where <span class="math notranslate nohighlight">\(A\)</span> is a
binary random variable where <span class="math notranslate nohighlight">\(A=0\)</span> designates the protected group, while
<span class="math notranslate nohighlight">\(A=1\)</span> designates the non-protected group. Let <span class="math notranslate nohighlight">\(Y\)</span> represent the actual
outcome and <span class="math notranslate nohighlight">\(\hat{Y}\)</span> represent the outcome returned by the prediction
algorithm. Without loss of generality, assume that <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\hat{Y}\)</span> are
binary random variables where <span class="math notranslate nohighlight">\(Y=1\)</span> designates a positive instance,
while <span class="math notranslate nohighlight">\(Y=0\)</span> a negative one. Typically, the predicted outcome <span class="math notranslate nohighlight">\(\hat{Y}\)</span>
is derived from a score represented by a random variable <span class="math notranslate nohighlight">\(S\)</span> where
<span class="math notranslate nohighlight">\(P[S = s]\)</span> is the probability that the score value is equal to <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p><strong>Statistical parity</strong> <span id="id10">[<a class="reference internal" href="#id2569" title="Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, 214–226. 2012.">11</a>]</span> is one of the most commonly
accepted notions of fairness. It requires the prediction to be
statistically independent of the sensitive feature <span class="math notranslate nohighlight">\((\hat{Y}  \perp A)\)</span>.
In other words, the predicted acceptance rates for both protected and
unprotected groups should be equal. Statistical parity implies that<br>
<span class="math notranslate nohighlight">\(\displaystyle \frac{TP+FP}{TP+FP+FN+TN}\)</span> <a class="footnote-reference brackets" href="#statistical" id="id11">1</a> <br>
is equal for both groups. A classifier Ŷ satisfies statistical parity
if:<br>
<span class="math notranslate nohighlight">\(\label{eq:sp} P[\hat{Y} \mid A = 0] = P[\hat{Y} \mid A = 1].\)</span> <br></p>
<p><strong>Conditional statistical parity</strong> <span id="id12">[<a class="reference internal" href="#id2570" title="Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost of fairness. In KDD, 797–806. ACM, 2017.">12</a>]</span> is a
variant of statistical parity obtained by controlling on a set of
resolving features<a class="footnote-reference brackets" href="#parity" id="id13">2</a>. The resolving features (we refer to them as <span class="math notranslate nohighlight">\(R\)</span>)
among <span class="math notranslate nohighlight">\(X\)</span> are correlated with the sensitive feature <span class="math notranslate nohighlight">\(A\)</span> and give some
factual information about the label at the same time leading to a
<em>legitimate</em> discrimination. Conditional statistical parity holds if: <br>
<span class="math notranslate nohighlight">\(\label{eq:csp}
P[\hat{Y}=1 \mid R=r,A = 0] = P[\hat{Y}=1 \mid R=r,A = 1] \quad \forall r \in range(R).\)</span></p>
<p><strong>Equalized odds</strong> <span id="id14">[<a class="reference internal" href="#id2571" title="Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In NIPS, 3315–3323. 2016.">13</a>]</span> considers both the predicted and
the actual outcomes. The prediction is conditionally independent from
the protected feature, given the actual outcome
<span class="math notranslate nohighlight">\((\hat{Y} \perp A \mid Y)\)</span>. In other words, equalized odds requires that
both sub-populations to have the same true positive rate
<span class="math notranslate nohighlight">\(TPR = \frac{TP}{TP+FN}\)</span> and false positive rate
<span class="math notranslate nohighlight">\(FPR = \frac{FP}{FP+TN}\)</span>: <br>
<span class="math notranslate nohighlight">\(\label{eq:eqOdds}
P[\hat{Y} = 1 \mid Y=y,\; A=0] = P[\hat{Y}=1 \mid Y= y,\; A=1]  \quad \forall{ y \in \{0,1\}}.\)</span></p>
<p>Because equalized odds requirement is rarely satisfied in practice, two
variants can be obtained by relaxing its equation. The first one is
called <strong>equal opportunity</strong> <span id="id15">[<a class="reference internal" href="#id2571" title="Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In NIPS, 3315–3323. 2016.">13</a>]</span> and is obtained by
requiring only TPR equality among groups: <br>
<span class="math notranslate nohighlight">\(\label{eq:eqOpp}
P[\hat{Y}=1 \mid Y=1,A = 0] = P[\hat{Y}=1\mid Y=1,A = 1].\)</span> <br>
As <span class="math notranslate nohighlight">\(TPR\)</span>
does not take into consideration <span class="math notranslate nohighlight">\(FP\)</span>, equal opportunity is completely
insensitive to the number of false positives.</p>
<p>The second relaxed variant of equalized odds is called <strong>predictive
equality</strong> <span id="id16">[<a class="reference internal" href="#id2570" title="Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost of fairness. In KDD, 797–806. ACM, 2017.">12</a>]</span> which requires only the FPR to be
equal in both groups: <br>
<span class="math notranslate nohighlight">\(\label{eq:predEq}
P[\hat{Y}=1 \mid Y=0,A = 0] = P[\hat{Y}=1\mid Y=0,A = 1].\)</span> <br>
Since <span class="math notranslate nohighlight">\(FPR\)</span> is independent from <span class="math notranslate nohighlight">\(FN\)</span>, predictive equality is completely
insensitive to false negatives.</p>
<p><strong>Conditional use accuracy equality</strong> <span id="id17">[<a class="reference internal" href="#id2573" title="Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal justice risk assessments: the state of the art. Sociological Methods &amp; Research, 50:3-44, 2018.">14</a>]</span> is achieved
when all population groups have equal positive predictive value
<span class="math notranslate nohighlight">\(PPV=\frac{TP}{TP+FP}\)</span> and negative predictive value
<span class="math notranslate nohighlight">\(NPV=\frac{TN}{FN+TN}\)</span>. In other words, the probability of subjects with
positive predictive value to truly belong to the positive class and the
probability of subjects with negative predictive value to truly belong
to the negative class should be the same. By contrast to equalized odds,
one is conditioning on the algorithm’s predicted outcome not the actual
outcome. In other words, the emphasis is on the precision of prediction
rather than its recall: <br>
<span class="math notranslate nohighlight">\(\label{eq:condUseAcc}
P[Y=y\mid \hat{Y}=y ,A = 0] = P[Y=y\mid \hat{Y}=y,A = 1] \quad \forall{ y \in \{0,1\}}.\)</span></p>
<p><strong>Predictive parity</strong> <span id="id18">[<a class="reference internal" href="#id2574" title="Alexandra Chouldechova. Fair prediction with disparate impact: a study of bias in recidivism prediction instruments. Big data, 5(2):153–163, 2017.">15</a>]</span> is a relaxation of
conditional use accuracy equality requiring only equal <span class="math notranslate nohighlight">\(PPV\)</span> among
groups: $<span class="math notranslate nohighlight">\(\label{eq:predPar}
P[Y=1 \mid \hat{Y} =1,A = 0] = P[Y=1\mid \hat{Y} =1,A = 1]\)</span>$ Like
predictive equality, predictive parity is insensitive to false
negatives.</p>
<p><strong>Overall accuracy equality</strong> <span id="id19">[<a class="reference internal" href="#id2573" title="Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal justice risk assessments: the state of the art. Sociological Methods &amp; Research, 50:3-44, 2018.">14</a>]</span> is achieved when
overall accuracy for both groups is the same. This implies that</p>
<div class="math notranslate nohighlight">
\[\label{eq:accuracy}
\frac{TP+TN}{TP+FN+FP+TN}\]</div>
<p>is equal for both groups:</p>
<div class="math notranslate nohighlight">
\[\label{eq:ovAcc}
P[\hat{Y} = Y | A = 0] = P[\hat{Y} = Y | A = 1]\]</div>
<p><strong>Treatment equality</strong> <span id="id20">[<a class="reference internal" href="#id2573" title="Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal justice risk assessments: the state of the art. Sociological Methods &amp; Research, 50:3-44, 2018.">14</a>]</span> is achieved when the ratio of
FPs and FNs is the same for both protected and unprotected groups: <br>
<span class="math notranslate nohighlight">\(\label{eq:treatEq}
\frac{FN}{FP}\)</span><sub>A=0</sub> <span class="math notranslate nohighlight">\(= \frac {FN}{FP}\)</span><sub>A=1</sub></p>
<p><strong>Total fairness</strong> <span id="id21">[<a class="reference internal" href="#id2573" title="Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal justice risk assessments: the state of the art. Sociological Methods &amp; Research, 50:3-44, 2018.">14</a>]</span> holds when all aforementioned
fairness notions are satisfied simultaneously, that is, statistical
parity, equalized odds, conditional use accuracy equality (hence,
overall accuracy equality), and treatment equality. Total fairness is a
very strong notion which is very difficult to hold in practice.</p>
<p><strong>Balance</strong> <span id="id22">[<a class="reference internal" href="#id2575" title="Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In ITCS, volume 67 of LIPIcs, 43:1–43:23. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2017.">9</a>]</span> uses the score (<span class="math notranslate nohighlight">\(S\)</span>) from which the outcome
<span class="math notranslate nohighlight">\(Y\)</span> is typically derived through thresholding. <br>
<strong>Balance for positive
class</strong> focuses on the applicants who constitute positive instances and
is satisfied if the average score <span class="math notranslate nohighlight">\(S\)</span> received by those applicants is
the same for both groups: <br>
<span class="math notranslate nohighlight">\(\label{eq:balPosclass}
E[S \mid Y =1,A = 0)] = E[S \mid Y =1,A = 1].\)</span> <br>
<strong>Balance of negative
class</strong> focuses instead on the negative class: <br>
<span class="math notranslate nohighlight">\(\label{eq:balNegclass}
E[S \mid Y =0,A = 0] = E[S \mid Y =0,A = 1].\)</span></p>
<p><strong>Calibration</strong> <span id="id23">[<a class="reference internal" href="#id2574" title="Alexandra Chouldechova. Fair prediction with disparate impact: a study of bias in recidivism prediction instruments. Big data, 5(2):153–163, 2017.">15</a>]</span> holds if, for each predicted
probability score <span class="math notranslate nohighlight">\(S=s\)</span>, individuals in all groups have the same
probability to actually belong to the positive class: <br>
<span class="math notranslate nohighlight">\(\label{eq:calib}
P[Y =1 \mid S =s,A = 0] = P[Y =1 \mid S =s,A = 1] \quad \forall s \in [0,1].\)</span></p>
<p><strong>Well-calibration</strong> <span id="id24">[<a class="reference internal" href="#id2575" title="Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In ITCS, volume 67 of LIPIcs, 43:1–43:23. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2017.">9</a>]</span> is a stronger variant of
calibration. It requires that (1) calibration is satisfied, (2) the
score is interpreted as the probability to truly belong to the positive
class, and (3) for each score <span class="math notranslate nohighlight">\(S=s\)</span>, the probability to truly belong to
the positive class is equal to that particular score: <br>
<span class="math notranslate nohighlight">\(\label{eq:wellCalib}
P[Y =1 \mid S =s,A = 0] = P[Y =1 \mid S =s,A = 1] = s  \quad  \forall \; {s \in [0,1]}.\)</span></p>
<p><strong>Fairness through awareness</strong> <span id="id25">[<a class="reference internal" href="#id2569" title="Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, 214–226. 2012.">11</a>]</span> implies that similar
individuals should have similar predictions. Let <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> be two
individuals represented by their attributes values vectors <span class="math notranslate nohighlight">\(v_i\)</span> and
<span class="math notranslate nohighlight">\(v_j\)</span>. Let <span class="math notranslate nohighlight">\(d(v_i,v_j)\)</span> represent the similarity distance between
individuals <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. Let <span class="math notranslate nohighlight">\(M(v_i)\)</span> represent the probability
distribution over the outcomes of the prediction. For example, if the
outcome is binary (<span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>), <span class="math notranslate nohighlight">\(M(v_i)\)</span> might be <span class="math notranslate nohighlight">\([0.2,0.8]\)</span> which
means that for individual <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(P[\hat{Y}=0]) = 0.2\)</span> and
<span class="math notranslate nohighlight">\(P[\hat{Y}=1] = 0.8\)</span>. Let <span class="math notranslate nohighlight">\(d_M\)</span> be a distance metric between
probability distributions. Fairness through awareness is achieved iff,
for any pair of individuals <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>: <br>
<span class="math notranslate nohighlight">\(d_M(M(v_i), M(v_j))  \leq d(v_i, v_j)\)</span> <br>
In practice, fairness through
awareness assumes that the similarity metric is known for each pair of
individuals <span id="id26">[<a class="reference internal" href="#id2576" title="Michael P. Kim, Omer Reingold, and Guy N. Rothblum. Fairness through computationally-bounded awareness. In NeurIPS, 4847–4857. 2018.">16</a>]</span>. That is, a challenging aspect of this
approach is the difficulty to determine what is an appropriate metric
function to measure the similarity between two individuals. Typically,
this requires careful human intervention from professionals with domain
expertise <span id="id27">[<a class="reference internal" href="#id2611" title="Matt J. Kusner, Joshua R. Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In NIPS, 4066–4076. 2017.">17</a>]</span>.</p>
<p><strong>Process fairness</strong>  <span id="id28">[<a class="reference internal" href="#id2609" title="Nina Grgic-Hlaca, Muhammad Bilal Zafar, Krishna P. Gummadi, and Adrian Weller. Beyond distributive fairness in algorithmic decision making: feature selection for procedurally fair learning. In AAAI, 51–60. AAAI Press, 2018.">18</a>]</span> (or procedural fairness) can
be described as a set of subjective fairness notions that are centered
on the process that leads to outcomes. These notions are not focused on
the fairness of the outcomes, instead they quantify the fraction of
users that consider fair the use of a particular set of features. They
are subjective as they depend on user judgments which may be obtained by
subjective reasoning.</p>
<p>A natural approach to improve process fairness is to remove all
sensitive (protected or salient) features before training classifiers.
This simple approach connects process fairness to <em>fairness through
unawareness</em>. However, there is a trade-off to manage since dropping out
sensitive features may impact negatively classification
performance <span id="id29">[<a class="reference internal" href="#id2610" title="Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fairness beyond disparate treatment &amp; disparate impact: learning classification without disparate mistreatment. In WWW, 1171–1180. ACM, 2017.">19</a>]</span>.</p>
<p><strong>Nonstatistical fairness metrics:</strong> Recently, further metrics have been
proposed and that differ from the previous in that they do not fully
rely on statistical considerations, and take into account domain
knowledge, that is not directly observable from data, require expert
input, or reason about hypothetical situations. As they fall out of the
scope of this chapter, we will not further dwell into these and simply
mention a few to the interested reader: <em>total effect</em>
<span id="id30">[<a class="reference internal" href="#id2612" title="Judea Pearl. Causality. Cambridge university press, 2009.">20</a>]</span> (that is the “causal” version of statistical
parity and measures the effect of changing the value of an attribute,
taking into account a given causal graph), <em>effect of treatment of the
treated</em> <span id="id31">[<a class="reference internal" href="#id2612" title="Judea Pearl. Causality. Cambridge university press, 2009.">20</a>]</span> (that relies on counterfactuals with
respect to sensitive features and measures the difference between the
probabilities of instances and their counterfactuals), and
<em>counterfactual fairness</em> <span id="id32">[<a class="reference internal" href="#id2611" title="Matt J. Kusner, Joshua R. Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In NIPS, 4066–4076. 2017.">17</a>]</span> (which is a
fine-grained variant of the previous but with respect to the set all
features).</p>
<p><strong>Discussion:</strong> As the above fairness metrics often conflict, and it is
not possible to be fair according to all of these definitions, it is a
challenge to choose the relevant metric to focus on. While still very
much an open research area, some suggestions on how one can deal with
conflicts between fairness metrics can be found in
<span id="id33">[<a class="reference internal" href="#id2492" title="Michele Loi and Markus Christen. Choosing how to discriminate: navigating ethical trade-offs in fair algorithmic design for the insurance sector. Philosophy &amp; Technology, 34(4):967–992, 2021.">2</a>, <a class="reference internal" href="#id2493" title="Michelle Seng Ah Lee, Luciano Floridi, and Jatinder Singh. Formalising trade-offs beyond algorithmic fairness: lessons from ethical philosophy and welfare economics. AI Ethics, 1(4):529–544, 2021.">21</a>]</span>. Indeed, fairness metrics
frequently conflict with other metrics such as accuracy and privacy.
<span id="id34">[<a class="reference internal" href="#id2494" title="Nikita Kozodoi, Johannes Jacob, and Stefan Lessmann. Fairness in credit scoring: assessment, implementation and profit implications. European Journal of Operational Research, 297(3):1083–1094, 2022.">22</a>]</span> show that in a credit scoring case enforcing
fairness metrics can lead to significant drops in accuracy and, thus,
maximum profit. This is unavoidable: improvements on fairness often
result in lower accuracy, and research on the Pareto frontier for this
trade-off is now emerging <span id="id35">[<a class="reference internal" href="#id2496" title="Susan Wei and Marc Niethammer. The fairness-accuracy pareto front. Statistical Analysis and Data Mining: The ASA Data Science Journal, 2020.">23</a>, <a class="reference internal" href="#id2495" title="Annie Liang, Jay Lu, and Xiaosheng Mu. Algorithmic design: fairness versus accuracy. arXiv preprint arXiv:2112.09975, 2021. URL: https://arxiv.org/abs/2112.09975.">24</a>]</span>.
Similarly, there is a trade-off between fairness and privacy, as
fairness metrics typically require sensitive information in order to be
used. As a result, fairness affects privacy (and vice versa), for
example in facial recognition <span id="id36">[<a class="reference internal" href="#id2497" title="Alice Xiang. Being'seen'vs.'mis-seen': tensions between privacy and fairness in computer vision. Harvard Journal of Law &amp; Technology, Forthcoming, 2022.">25</a>]</span> and medical applications
<span id="id37">[<a class="reference internal" href="#id2498" title="Andrew Chester, Yun Sing Koh, Jörg Wicker, Quan Sun, and Junjae Lee. Balancing utility and fairness against privacy in medical data. In 2020 IEEE Symposium Series on Computational Intelligence (SSCI), 1226–1233. IEEE, 2020.">26</a>]</span>.</p>
<p>Finally, there is a connection between <em>fairness</em> and <a class="reference internal" href="justice.html"><span class="doc">Justice</span></a>, seen in
for example Rawls’ work on Justice as Fairness <span id="id38">[<a class="reference internal" href="#id2491" title="John Rawls. Justice as fairness: A restatement. Harvard University Press, 2001.">27</a>]</span>. And
indeed, a range of theories of (distributive) justice describe how
benefits and burdens should be distributed (cf. the entry on ). As such,
they can be seen as guiding the outcomes of algorithms even if they
describe what these distributions should be in society as a whole. Yet,
as <span id="id39">[<a class="reference internal" href="justice.html#id2486" title="Matthias Kuppler, Christoph Kern, Ruben L Bach, and Frauke Kreuter. Distributive justice and fairness metrics in automated decision-making: how much overlap is there? arXiv preprint arXiv:2105.01441, 2021. URL: https://arxiv.org/abs/2105.01441.">3</a>]</span> argue at length, there is little overlap
between theories of distributive justice and fairness metrics.
Non-comparative notions of justice are not captured by fairness metrics,
nor are notions such as Rawls’ difference principle, on which the right
distribution is the one where the worst off have the highest absolute
level of benefits. Fairness metrics have focused more on
<a class="reference internal" href="discrimination.html"><span class="doc std std-doc">discrimination</span></a> than on <em>justice</em>.</p>
</section>
<section id="bibliography">
<h2>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">#</a></h2>
<div class="docutils container" id="id40">
<dl class="citation">
<dt class="label" id="id2508"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Deirdre K Mulligan, Joshua A Kroll, Nitin Kohli, and Richmond Y Wong. This thing called fairness: disciplinary confusion realizing a value in technology. <em>Proceedings of the ACM on Human-Computer Interaction</em>, 3(CSCW):1–36, 2019.</p>
</dd>
<dt class="label" id="id2492"><span class="brackets"><a class="fn-backref" href="#id33">2</a></span></dt>
<dd><p>Michele Loi and Markus Christen. Choosing how to discriminate: navigating ethical trade-offs in fair algorithmic design for the insurance sector. <em>Philosophy &amp; Technology</em>, 34(4):967–992, 2021.</p>
</dd>
<dt class="label" id="id2572"><span class="brackets">3</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id3">2</a>,<a href="#id4">3</a>)</span></dt>
<dd><p>Sandra Wachter, Brent Mittelstadt, and Chris Russell. Bias preservation in machine learning: the legality of fairness metrics under eu non-discrimination law. <em>W. Va. L. Rev.</em>, 123:735, 2020.</p>
</dd>
<dt class="label" id="id2509"><span class="brackets"><a class="fn-backref" href="#id5">4</a></span></dt>
<dd><p>Philips George John, Deepak Vijaykeerthy, and Diptikalyan Saha. Verifying individual fairness in machine learning models. In <em>UAI</em>, volume 124 of Proceedings of Machine Learning Research, 749–758. AUAI Press, 2020.</p>
</dd>
<dt class="label" id="id2510"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>Asia J. Biega, Krishna P. Gummadi, and Gerhard Weikum. Equity of attention: amortizing individual fairness in rankings. In <em>SIGIR</em>, 405–414. ACM, 2018.</p>
</dd>
<dt class="label" id="id2514"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p>Yu Cheng, Zhihao Jiang, Kamesh Munagala, and Kangning Wang. Group fairness in committee selection. <em>ACM Transactions on Economics and Computation (TEAC)</em>, 8(4):1–18, 2020.</p>
</dd>
<dt class="label" id="id2515"><span class="brackets"><a class="fn-backref" href="#id6">7</a></span></dt>
<dd><p>Vincent Conitzer, Rupert Freeman, Nisarg Shah, and Jennifer Wortman Vaughan. Group fairness for the allocation of indivisible goods. In <em>AAAI</em>, 1853–1860. AAAI Press, 2019.</p>
</dd>
<dt class="label" id="id2562"><span class="brackets"><a class="fn-backref" href="#id7">8</a></span></dt>
<dd><p>Michael J. Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: auditing and learning for subgroup fairness. In <em>ICML</em>, volume 80 of Proceedings of Machine Learning Research, 2569–2577. PMLR, 2018.</p>
</dd>
<dt class="label" id="id2575"><span class="brackets">9</span><span class="fn-backref">(<a href="#id8">1</a>,<a href="#id22">2</a>,<a href="#id24">3</a>)</span></dt>
<dd><p>Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In <em>ITCS</em>, volume 67 of LIPIcs, 43:1–43:23. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2017.</p>
</dd>
<dt class="label" id="id2516"><span class="brackets"><a class="fn-backref" href="#id9">10</a></span></dt>
<dd><p>Lydia T. Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machine learning. In <em>IJCAI</em>, 6196–6200. ijcai.org, 2019.</p>
</dd>
<dt class="label" id="id2569"><span class="brackets">11</span><span class="fn-backref">(<a href="#id10">1</a>,<a href="#id25">2</a>)</span></dt>
<dd><p>Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In <em>Proceedings of the 3rd innovations in theoretical computer science conference</em>, 214–226. 2012.</p>
</dd>
<dt class="label" id="id2570"><span class="brackets">12</span><span class="fn-backref">(<a href="#id12">1</a>,<a href="#id16">2</a>)</span></dt>
<dd><p>Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost of fairness. In <em>KDD</em>, 797–806. ACM, 2017.</p>
</dd>
<dt class="label" id="id2571"><span class="brackets">13</span><span class="fn-backref">(<a href="#id14">1</a>,<a href="#id15">2</a>)</span></dt>
<dd><p>Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In <em>NIPS</em>, 3315–3323. 2016.</p>
</dd>
<dt class="label" id="id2573"><span class="brackets">14</span><span class="fn-backref">(<a href="#id17">1</a>,<a href="#id19">2</a>,<a href="#id20">3</a>,<a href="#id21">4</a>)</span></dt>
<dd><p>Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal justice risk assessments: the state of the art. <em>Sociological Methods &amp; Research</em>, 50:3–44, 2018.</p>
</dd>
<dt class="label" id="id2574"><span class="brackets">15</span><span class="fn-backref">(<a href="#id18">1</a>,<a href="#id23">2</a>)</span></dt>
<dd><p>Alexandra Chouldechova. Fair prediction with disparate impact: a study of bias in recidivism prediction instruments. <em>Big data</em>, 5(2):153–163, 2017.</p>
</dd>
<dt class="label" id="id2576"><span class="brackets"><a class="fn-backref" href="#id26">16</a></span></dt>
<dd><p>Michael P. Kim, Omer Reingold, and Guy N. Rothblum. Fairness through computationally-bounded awareness. In <em>NeurIPS</em>, 4847–4857. 2018.</p>
</dd>
<dt class="label" id="id2611"><span class="brackets">17</span><span class="fn-backref">(<a href="#id27">1</a>,<a href="#id32">2</a>)</span></dt>
<dd><p>Matt J. Kusner, Joshua R. Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In <em>NIPS</em>, 4066–4076. 2017.</p>
</dd>
<dt class="label" id="id2609"><span class="brackets"><a class="fn-backref" href="#id28">18</a></span></dt>
<dd><p>Nina Grgic-Hlaca, Muhammad Bilal Zafar, Krishna P. Gummadi, and Adrian Weller. Beyond distributive fairness in algorithmic decision making: feature selection for procedurally fair learning. In <em>AAAI</em>, 51–60. AAAI Press, 2018.</p>
</dd>
<dt class="label" id="id2610"><span class="brackets"><a class="fn-backref" href="#id29">19</a></span></dt>
<dd><p>Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fairness beyond disparate treatment &amp; disparate impact: learning classification without disparate mistreatment. In <em>WWW</em>, 1171–1180. ACM, 2017.</p>
</dd>
<dt class="label" id="id2612"><span class="brackets">20</span><span class="fn-backref">(<a href="#id30">1</a>,<a href="#id31">2</a>)</span></dt>
<dd><p>Judea Pearl. <em>Causality</em>. Cambridge university press, 2009.</p>
</dd>
<dt class="label" id="id2493"><span class="brackets"><a class="fn-backref" href="#id33">21</a></span></dt>
<dd><p>Michelle Seng Ah Lee, Luciano Floridi, and Jatinder Singh. Formalising trade-offs beyond algorithmic fairness: lessons from ethical philosophy and welfare economics. <em>AI Ethics</em>, 1(4):529–544, 2021.</p>
</dd>
<dt class="label" id="id2494"><span class="brackets"><a class="fn-backref" href="#id34">22</a></span></dt>
<dd><p>Nikita Kozodoi, Johannes Jacob, and Stefan Lessmann. Fairness in credit scoring: assessment, implementation and profit implications. <em>European Journal of Operational Research</em>, 297(3):1083–1094, 2022.</p>
</dd>
<dt class="label" id="id2496"><span class="brackets"><a class="fn-backref" href="#id35">23</a></span></dt>
<dd><p>Susan Wei and Marc Niethammer. The fairness-accuracy pareto front. <em>Statistical Analysis and Data Mining: The ASA Data Science Journal</em>, 2020.</p>
</dd>
<dt class="label" id="id2495"><span class="brackets"><a class="fn-backref" href="#id35">24</a></span></dt>
<dd><p>Annie Liang, Jay Lu, and Xiaosheng Mu. Algorithmic design: fairness versus accuracy. <em>arXiv preprint arXiv:2112.09975</em>, 2021. URL: <a class="reference external" href="https://arxiv.org/abs/2112.09975">https://arxiv.org/abs/2112.09975</a>.</p>
</dd>
<dt class="label" id="id2497"><span class="brackets"><a class="fn-backref" href="#id36">25</a></span></dt>
<dd><p>Alice Xiang. Being'seen'vs.'mis-seen': tensions between privacy and fairness in computer vision. <em>Harvard Journal of Law &amp; Technology, Forthcoming</em>, 2022.</p>
</dd>
<dt class="label" id="id2498"><span class="brackets"><a class="fn-backref" href="#id37">26</a></span></dt>
<dd><p>Andrew Chester, Yun Sing Koh, Jörg Wicker, Quan Sun, and Junjae Lee. Balancing utility and fairness against privacy in medical data. In <em>2020 IEEE Symposium Series on Computational Intelligence (SSCI)</em>, 1226–1233. IEEE, 2020.</p>
</dd>
<dt class="label" id="id2491"><span class="brackets"><a class="fn-backref" href="#id38">27</a></span></dt>
<dd><p>John Rawls. <em>Justice as fairness: A restatement</em>. Harvard University Press, 2001.</p>
</dd>
<dt class="label" id="id2486"><span class="brackets"><a class="fn-backref" href="#id39">28</a></span></dt>
<dd><p>Matthias Kuppler, Christoph Kern, Ruben L Bach, and Frauke Kreuter. Distributive justice and fairness metrics in automated decision-making: how much overlap is there? <em>arXiv preprint arXiv:2105.01441</em>, 2021. URL: <a class="reference external" href="https://arxiv.org/abs/2105.01441">https://arxiv.org/abs/2105.01441</a>.</p>
</dd>
<dt class="label" id="id2513"><span class="brackets"><a class="fn-backref" href="#id2732">29</a></span></dt>
<dd><p>Faisal Kamiran, Indre Zliobaite, and Toon Calders. Quantifying explainable discrimination and removing illegal discrimination in automated decision making. <em>Knowl. Inf. Syst.</em>, 35(3):613–644, 2013.</p>
</dd>
</dl>
</div>
<blockquote>
<div><p>This entry was written by Resmi Ramachandranpillai, Fredrik Heintz, Stefan Buijsman, Miguel Couceiro, Guilherme Alves, Karima Makhlouf, and Sami Zhioua.</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="statistical"><span class="brackets"><a class="fn-backref" href="#id11">1</a></span></dt>
<dd><p><span class="math notranslate nohighlight">\(TP,FP,FN,\)</span> and <span class="math notranslate nohighlight">\(TN\)</span> stand for: true positives, false positives, false negatives, and true negatives, respectively.</p>
</dd>
<dt class="label" id="parity"><span class="brackets"><a class="fn-backref" href="#id13">2</a></span></dt>
<dd><p>Called explanatory features in <span id="id2732">[<a class="reference internal" href="#id2513" title="Faisal Kamiran, Indre Zliobaite, and Toon Calders. Quantifying explainable discrimination and removing illegal discrimination in automated decision making. Knowl. Inf. Syst., 35(3):613–644, 2013.">29</a>]</span>.</p>
</dd>
</dl>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./T3.3"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="equity.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Discrimination &amp; Equity</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="fair_ML.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Fair Machine Learning</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By TAILOR WP3 members; see <a href="/jupyter-book-TAILOR-D3.2/authors.html" target="_blank">here</a> for the complete list of contributors.<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>